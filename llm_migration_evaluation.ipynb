{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Simple LLM Migration Tool\n",
    "\n",
    "\n",
    "This notebook implements a simple tool for testing open source Large Language Models' capability in migrating legacy PHP code (WordPress 4.3) to modern PHP 8.3 standards.\n",
    "\n",
    "## What it does:\n",
    "- ✅ Sends your prompt to any OpenRouter model\n",
    "- ✅ Saves the complete raw response to a text file\n",
    "- ✅ No complex evaluation - just pure model output\n",
    "- ✅ Perfect for examining what models actually return\n",
    "\n",
    "**Browse available models at https://openrouter.ai/models**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Environment - Multi-Provider Support\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Multi-Provider API setup\n",
    "print(\"🔬 Multi-Provider LLM Migration Tool Initialized\")\n",
    "print(f\"⚙️  Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Initialize OpenRouter client\n",
    "try:\n",
    "    OPENROUTER_API_KEY = os.getenv('OPENROUTER_API_KEY')\n",
    "    \n",
    "    if not OPENROUTER_API_KEY:\n",
    "        raise ValueError(\"OPENROUTER_API_KEY not found in environment variables.\")\n",
    "    \n",
    "    openrouter_client = openai.OpenAI(\n",
    "        base_url=\"https://openrouter.ai/api/v1\",\n",
    "        api_key=OPENROUTER_API_KEY,\n",
    "    )\n",
    "    print(\"✅ OpenRouter client initialized successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error initializing OpenRouter client: {e}\")\n",
    "    openrouter_client = None\n",
    "\n",
    "# Initialize Google AI client\n",
    "try:\n",
    "    GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
    "    \n",
    "    if not GOOGLE_API_KEY:\n",
    "        raise ValueError(\"GOOGLE_API_KEY not found in environment variables.\")\n",
    "    \n",
    "    # Import and configure Google AI with new API\n",
    "    import google.genai as genai\n",
    "    \n",
    "    # Create client with API key\n",
    "    google_client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "    print(\"✅ Google AI client initialized successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error initializing Google AI client: {e}\")\n",
    "    google_client = None\n",
    "\n",
    "# Provider configuration\n",
    "PROVIDERS = {\n",
    "    'openrouter': {\n",
    "        'client': openrouter_client,\n",
    "        'api_key': OPENROUTER_API_KEY,\n",
    "        'enabled': openrouter_client is not None\n",
    "    },\n",
    "    'google': {\n",
    "        'client': google_client,\n",
    "        'api_key': GOOGLE_API_KEY,\n",
    "        'enabled': google_client is not None\n",
    "    }\n",
    "}\n",
    "\n",
    "# Show available providers\n",
    "enabled_providers = [name for name, config in PROVIDERS.items() if config['enabled']]\n",
    "print(f\"\\n🎯 Available providers: {', '.join(enabled_providers)}\")\n",
    "\n",
    "print(\"\\n🌐 Supported models:\")\n",
    "print(\"   OpenRouter: 'anthropic/claude-3.5-sonnet', 'meta-llama/llama-3.1-8b-instruct', etc.\")\n",
    "print(\"   Google AI: 'gemini-1.5-pro', 'gemini-1.5-flash', 'gemini-1.0-pro', etc.\")\n",
    "print(\"📋 Visit https://openrouter.ai/models for OpenRouter model list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMPLIFIED MULTI-PROVIDER ROUTING SYSTEM\n",
    "import requests\n",
    "import google.genai as genai\n",
    "\n",
    "class MultiProviderClient:\n",
    "    \"\"\"Simplified multi-provider LLM client with reduced redundancy.\"\"\"\n",
    "    \n",
    "    # Provider detection patterns (data-driven approach)\n",
    "    PROVIDER_PATTERNS = {\n",
    "        'google': ['gemini', 'palm', 'bard'],\n",
    "        'openrouter': ['anthropic', 'openai', 'meta', 'mistral', 'cohere', \n",
    "                      'deepseek', 'qwen', 'dolphin', 'nous', 'microsoft']\n",
    "    }\n",
    "    \n",
    "    # Default configurations\n",
    "    DEFAULT_CONFIG = {\n",
    "        'max_tokens': {'google': 8192, 'openrouter': 80000},\n",
    "        'temperature': 0.3,\n",
    "        'timeout': 300\n",
    "    }\n",
    "    \n",
    "    def __init__(self, providers):\n",
    "        self.providers = providers\n",
    "    \n",
    "    def detect_provider(self, model_name: str) -> str:\n",
    "        \"\"\"Detect provider using pattern matching.\"\"\"\n",
    "        model_lower = model_name.lower()\n",
    "        \n",
    "        # Check for Google AI patterns\n",
    "        if any(keyword in model_lower for keyword in self.PROVIDER_PATTERNS['google']):\n",
    "            return 'google'\n",
    "        \n",
    "        # Check for OpenRouter patterns (including slash notation)\n",
    "        if ('/' in model_name or \n",
    "            any(pattern in model_lower for pattern in self.PROVIDER_PATTERNS['openrouter'])):\n",
    "            return 'openrouter'\n",
    "        \n",
    "        return 'openrouter'  # Default fallback\n",
    "    \n",
    "    def make_api_call(self, model_name: str, prompt: str, **kwargs) -> dict:\n",
    "        \"\"\"Unified API call with error handling.\"\"\"\n",
    "        provider = self.detect_provider(model_name)\n",
    "        \n",
    "        # Check provider availability\n",
    "        if not self.providers.get(provider, {}).get('enabled'):\n",
    "            return self._error_response(f'Provider {provider} is not enabled')\n",
    "        \n",
    "        print(f\"🔗 Using {provider.upper()} provider for {model_name}\")\n",
    "        \n",
    "        # Route to appropriate provider method\n",
    "        try:\n",
    "            if provider == 'google':\n",
    "                return self._call_google(model_name, prompt, **kwargs)\n",
    "            else:  # openrouter\n",
    "                return self._call_openrouter(model_name, prompt, **kwargs)\n",
    "        except Exception as e:\n",
    "            return self._error_response(str(e))\n",
    "    \n",
    "    def _call_openrouter(self, model_name: str, prompt: str, **kwargs) -> dict:\n",
    "        \"\"\"OpenRouter API call.\"\"\"\n",
    "        payload = {\n",
    "            \"model\": model_name,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"max_tokens\": kwargs.get('max_tokens', self.DEFAULT_CONFIG['max_tokens']['openrouter']),\n",
    "            \"temperature\": kwargs.get('temperature', self.DEFAULT_CONFIG['temperature'])\n",
    "        }\n",
    "        \n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.providers['openrouter']['api_key']}\",\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"HTTP-Referer\": \"https://github.com/research-project\",\n",
    "            \"X-Title\": \"LLM PHP Migration Research\"\n",
    "        }\n",
    "        \n",
    "        response = requests.post(\n",
    "            \"https://openrouter.ai/api/v1/chat/completions\",\n",
    "            headers=headers,\n",
    "            data=json.dumps(payload),\n",
    "            timeout=self.DEFAULT_CONFIG['timeout']\n",
    "        )\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            return self._error_response(f'HTTP {response.status_code}: {response.text[:500]}')\n",
    "        \n",
    "        result = response.json()\n",
    "        return self._success_response(\n",
    "            content=result['choices'][0]['message']['content'],\n",
    "            provider='openrouter',\n",
    "            model=model_name,\n",
    "            usage=result.get('usage', {})\n",
    "        )\n",
    "    \n",
    "    def _call_google(self, model_name: str, prompt: str, **kwargs) -> dict:\n",
    "        \"\"\"Google AI API call.\"\"\"\n",
    "        client = self.providers['google']['client']\n",
    "        \n",
    "        response = client.models.generate_content(\n",
    "            model=model_name,\n",
    "            contents=prompt,\n",
    "            config={\n",
    "                'temperature': kwargs.get('temperature', self.DEFAULT_CONFIG['temperature']),\n",
    "                'max_output_tokens': kwargs.get('max_tokens', self.DEFAULT_CONFIG['max_tokens']['google']),\n",
    "                'top_p': 0.95,\n",
    "                'top_k': 40\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        if not response.text:\n",
    "            return self._error_response('Empty response from Google AI')\n",
    "        \n",
    "        # Extract usage info safely\n",
    "        usage_info = {}\n",
    "        if hasattr(response, 'usage_metadata'):\n",
    "            try:\n",
    "                usage_info = {\n",
    "                    'prompt_tokens': getattr(response.usage_metadata, 'prompt_token_count', 0),\n",
    "                    'completion_tokens': getattr(response.usage_metadata, 'candidates_token_count', 0)\n",
    "                }\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return self._success_response(\n",
    "            content=response.text,\n",
    "            provider='google',\n",
    "            model=model_name,\n",
    "            usage=usage_info\n",
    "        )\n",
    "    \n",
    "    def _success_response(self, content: str, provider: str, model: str, usage: dict) -> dict:\n",
    "        \"\"\"Standardized success response.\"\"\"\n",
    "        return {\n",
    "            'success': True,\n",
    "            'content': content,\n",
    "            'provider': provider,\n",
    "            'model': model,\n",
    "            'usage': usage\n",
    "        }\n",
    "    \n",
    "    def _error_response(self, error_message: str) -> dict:\n",
    "        \"\"\"Standardized error response.\"\"\"\n",
    "        return {'success': False, 'error': error_message}\n",
    "\n",
    "# Initialize the simplified multi-provider client\n",
    "multi_client = MultiProviderClient(PROVIDERS)\n",
    "\n",
    "\n",
    "# Test provider detection\n",
    "test_models = [\n",
    "    'gemini-1.5-pro',\n",
    "    'anthropic/claude-3.5-sonnet', \n",
    "    'meta-llama/llama-3.1-8b-instruct',\n",
    "    'gemini-1.5-flash'\n",
    "]\n",
    "\n",
    "print(\"\\n🔍 Provider Detection Test:\")\n",
    "for model in test_models:\n",
    "    provider = multi_client.detect_provider(model)\n",
    "    print(f\"   {model} → {provider.upper()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD TEST FILES\n",
    "test_files = {}\n",
    "old_version_path = Path('selected_100_files\\extra_large_1000_plus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASIC PROMPTING STRATEGY\n",
    "BASIC_PROMPT_TEMPLATE = \"\"\"You are a senior PHP developer with expertise in legacy code modernization. Your task is to migrate this PHP code to PHP 8.3 standards while maintaining functional equivalence.\n",
    "\n",
    "Please migrate the following PHP code to PHP 8.3:\n",
    "\n",
    "{code}\n",
    "\n",
    "Your response should follow this EXACT format:\n",
    "\n",
    "// MIGRATION_START\n",
    "[your migrated PHP code here]\n",
    "// MIGRATION_END\n",
    "\n",
    "CRITICAL FORMATTING REQUIREMENT: \n",
    "- Place the MIGRATION_START marker BEFORE the opening <?php tag\n",
    "- Place the MIGRATION_END marker AFTER the closing PHP code\n",
    "- Do NOT place these markers inside the PHP code itself\n",
    "\n",
    "Provide only the migrated PHP code with the markers placed correctly outside the PHP code block, no additional commentary.\"\"\"\n",
    "\n",
    "print(\"✅ Basic prompting strategy configured with fixed marker placement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPREHENSIVE PROMPTING STRATEGY\n",
    "COMPREHENSIVE_PROMPT_TEMPLATE = \"\"\"You are a senior PHP developer with expertise in legacy code modernization. Your task is to migrate old PHP code up to PHP 8.3 standards while maintaining the functionality of the original code.\n",
    "\n",
    "Migration Requirements:\n",
    "1. Update deprecated syntax\n",
    "2. Replace deprecated functions\n",
    "3. Implement modern PHP features\n",
    "4. Improve security and code quality\n",
    "5. Maintain functional equivalence\n",
    "6. Enforce strict typing\n",
    "7. Adopt core PHP 8.3 constructs\n",
    "\n",
    "Please migrate the following PHP code to PHP 8.3:\n",
    "\n",
    "{code}\n",
    "\n",
    "\n",
    "\n",
    "Your response should follow this EXACT format:\n",
    "\n",
    "// MIGRATION_START\n",
    "[your migrated PHP code here]\n",
    "// MIGRATION_END\n",
    "\n",
    "CRITICAL FORMATTING REQUIREMENT: \n",
    "- Place the MIGRATION_START marker BEFORE the opening <?php tag\n",
    "- Place the MIGRATION_END marker AFTER the closing PHP code\n",
    "- Do NOT place these markers inside the PHP code itself\n",
    "\n",
    "Include the markers as comments OUTSIDE the PHP code block. Keep the original comments as they are.\n",
    "Do not add any other text, explanations, or commentary outside the markers. Make sure you give the COMPLETE migrated code.\"\"\"\n",
    "\n",
    "# CHUNKING PROMPTS FOR LARGE FILES\n",
    "CHUNK_BASIC_PROMPT_TEMPLATE = \"\"\"You are a senior PHP developer with expertise in legacy code modernization. Your task is to migrate this PARTIAL SEGMENT of a larger PHP file up to PHP 8.3 standards.\n",
    "\n",
    "CONTEXT:\n",
    "- Original file: {filename}\n",
    "- Processing lines: {start_line} to {end_line} (of {total_lines} total lines)\n",
    "- This is chunk {chunk_number} of {total_chunks}\n",
    "\n",
    "CRITICAL INSTRUCTIONS:\n",
    "1. This is only a SEGMENT of the complete file\n",
    "2. Do NOT add extra opening <?php tags if the code segment doesn't start with one\n",
    "3. Do NOT add closing ?> tags if its not already present\n",
    "4. Do NOT try to complete missing parts or add closing braces that aren't provided\n",
    "5. Preserve the exact structure - if it starts with a method, start with that method\n",
    "6. If it starts mid-class, do NOT add class opening braces\n",
    "\n",
    "Please migrate ONLY the following PHP code segment to PHP 8.3:\n",
    "\n",
    "{code}\n",
    "\n",
    "Your response should follow this EXACT format:\n",
    "\n",
    "// MIGRATION_START\n",
    "[your migrated code segment here - exactly as provided]\n",
    "// MIGRATION_END\n",
    "\n",
    "CRITICAL FORMATTING REQUIREMENT: \n",
    "- Place the MIGRATION_START marker BEFORE the code segment\n",
    "- Place the MIGRATION_END marker AFTER the code segment\n",
    "- Do NOT place these markers inside the PHP code itself\n",
    "\n",
    "Migrate only the provided code segment. Do not try to complete the file or change any logic.\"\"\"\n",
    "\n",
    "CHUNK_COMPREHENSIVE_PROMPT_TEMPLATE = \"\"\"You are a senior PHP developer with expertise in legacy code modernization. Your task is to migrate this PARTIAL SEGMENT of a larger PHP file to PHP 8.3 standards while maintaining functional equivalence.\n",
    "\n",
    "CONTEXT:\n",
    "- Original file: {filename}\n",
    "- Processing lines: {start_line} to {end_line} (of {total_lines} total lines)  \n",
    "- This is chunk {chunk_number} of {total_chunks}\n",
    "\n",
    "CRITICAL INSTRUCTIONS:\n",
    "1. This is only a SEGMENT of the complete file\n",
    "2. Do NOT add opening <?php tags if the code segment doesn't start with one\n",
    "3. Do NOT add closing ?> tags\n",
    "4. Do NOT try to complete missing parts or add code that isn't provided\n",
    "5. Preserve the exact structure - if it starts with a method, start with that method\n",
    "6. If it starts mid-class, do NOT add class opening braces\n",
    "\n",
    "Migration Requirements for this segment:\n",
    "1. Update deprecated syntax\n",
    "2. Replace deprecated functions\n",
    "3. Implement modern PHP features\n",
    "4. Improve security and code quality\n",
    "5. Maintain functional equivalence\n",
    "6. Enforce strict typing\n",
    "7. Adopt core PHP 8.3 constructs\n",
    "\n",
    "\n",
    "Please migrate ONLY the following PHP code segment to PHP 8.3:\n",
    "\n",
    "{code}\n",
    "\n",
    "Your response should follow this EXACT format:\n",
    "\n",
    "// MIGRATION_START\n",
    "[your migrated code segment here - exactly as provided, no extra <?php tags]\n",
    "// MIGRATION_END\n",
    "\n",
    "CRITICAL FORMATTING REQUIREMENT: \n",
    "- Place the MIGRATION_START marker BEFORE the code segment\n",
    "- Place the MIGRATION_END marker AFTER the code segment\n",
    "- Do NOT place these markers inside the PHP code itself\n",
    "\n",
    "Include the markers as comments OUTSIDE the code segment. Keep the original comments as they are.\n",
    "Migrate only the provided code segment. Do not add missing functions, classes, or try to complete the file.\"\"\"\n",
    "\n",
    "print(\"✅ Comprehensive and chunking prompting strategies configured with fixed marker placement\")\n",
    "print(\"🔧 Updated all prompts to prevent placing MIGRATION markers inside PHP code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import subprocess\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def chunk_code(code: str, chunk_size: int = None) -> list:\n",
    "    \"\"\"Smart PHP-aware chunking using PHP tokenizer for accurate function detection.\"\"\"\n",
    "    if chunk_size is None:\n",
    "        chunk_size = DEFAULT_CHUNK_SIZE\n",
    "    \n",
    "    lines = code.split('\\n')\n",
    "    total_lines = len(lines)\n",
    "    \n",
    "    if total_lines <= chunk_size:\n",
    "        return [{\n",
    "            'start_line': 1,\n",
    "            'end_line': total_lines,\n",
    "            'actual_size': total_lines,\n",
    "            'total_lines': total_lines,\n",
    "            'code': code\n",
    "        }]\n",
    "    \n",
    "    # Get function boundaries using smart parsing\n",
    "    function_boundaries = find_function_boundaries(code, lines)\n",
    "    \n",
    "    # Create chunks based on function boundaries\n",
    "    return create_smart_chunks(lines, function_boundaries, chunk_size, total_lines)\n",
    "\n",
    "def find_function_boundaries(code: str, lines: list) -> list:\n",
    "    \"\"\"Find function boundaries using PHP tokenizer if available, else regex.\"\"\"\n",
    "    # Try PHP tokenizer first\n",
    "    php_functions = try_php_tokenizer(code, lines)\n",
    "    if php_functions:\n",
    "        return php_functions\n",
    "    \n",
    "    # Fallback to regex-based parsing\n",
    "    return find_functions_with_regex(lines)\n",
    "\n",
    "def try_php_tokenizer(code: str, lines: list) -> list:\n",
    "    \"\"\"Try to use PHP's built-in tokenizer for accurate parsing.\"\"\"\n",
    "    try:\n",
    "        php_script = f'''<?php\n",
    "$code = <<<'EOD'\n",
    "{code}\n",
    "EOD;\n",
    "\n",
    "$tokens = token_get_all($code);\n",
    "$functions = [];\n",
    "$current_function = null;\n",
    "$brace_level = 0;\n",
    "$in_function = false;\n",
    "\n",
    "foreach ($tokens as $token) {{\n",
    "    if (is_array($token)) {{\n",
    "        if ($token[0] === T_FUNCTION) {{\n",
    "            $in_function = true;\n",
    "            $current_function = [\n",
    "                'start_line' => $token[2] - 1,\n",
    "                'end_line' => null,\n",
    "                'name' => null\n",
    "            ];\n",
    "        }}\n",
    "        \n",
    "        if ($in_function && $token[0] === T_STRING && $current_function['name'] === null) {{\n",
    "            $current_function['name'] = $token[1];\n",
    "        }}\n",
    "    }} else {{\n",
    "        if ($token === '{{' && $in_function) {{\n",
    "            $brace_level++;\n",
    "        }} elseif ($token === '}}' && $in_function) {{\n",
    "            $brace_level--;\n",
    "            if ($brace_level === 0) {{\n",
    "                $current_function['end_line'] = find_closing_brace($current_function['start_line']);\n",
    "                $functions[] = $current_function;\n",
    "                $current_function = null;\n",
    "                $in_function = false;\n",
    "            }}\n",
    "        }}\n",
    "    }}\n",
    "}}\n",
    "\n",
    "function find_closing_brace($start_line) {{\n",
    "    global $code;\n",
    "    $lines = explode(\"\\\\n\", $code);\n",
    "    $brace_count = 0;\n",
    "    $function_started = false;\n",
    "    \n",
    "    for ($i = $start_line; $i < count($lines); $i++) {{\n",
    "        $line = trim($lines[$i]);\n",
    "        if (empty($line) || strpos($line, '//') === 0 || strpos($line, '#') === 0) continue;\n",
    "        \n",
    "        for ($j = 0; $j < strlen($line); $j++) {{\n",
    "            $char = $line[$j];\n",
    "            if ($char === '{{') {{\n",
    "                $brace_count++;\n",
    "                $function_started = true;\n",
    "            }} elseif ($char === '}}' && $function_started) {{\n",
    "                $brace_count--;\n",
    "                if ($brace_count === 0) return $i;\n",
    "            }}\n",
    "        }}\n",
    "    }}\n",
    "    return $start_line;\n",
    "}}\n",
    "\n",
    "echo json_encode($functions);\n",
    "?>'''\n",
    "        \n",
    "        temp_php = Path('temp_parser.php')\n",
    "        with open(temp_php, 'w', encoding='utf-8') as f:\n",
    "            f.write(php_script)\n",
    "        \n",
    "        result = subprocess.run(['php', str(temp_php)], \n",
    "                              capture_output=True, text=True, timeout=10)\n",
    "        temp_php.unlink()\n",
    "        \n",
    "        if result.returncode == 0 and result.stdout.strip():\n",
    "            return json.loads(result.stdout.strip())\n",
    "            \n",
    "    except (subprocess.TimeoutExpired, subprocess.CalledProcessError, \n",
    "            FileNotFoundError, json.JSONDecodeError):\n",
    "        pass\n",
    "    \n",
    "    return []\n",
    "\n",
    "def find_functions_with_regex(lines: list) -> list:\n",
    "    \"\"\"Regex-based function detection with proper closing brace detection.\"\"\"\n",
    "    functions = []\n",
    "    function_patterns = [\n",
    "        r'^\\s*(?:(?:public|private|protected)\\s+)?(?:static\\s+)?function\\s+(\\w+)\\s*\\(',\n",
    "        r'^\\s*(?:abstract\\s+)?(?:final\\s+)?function\\s+(\\w+)\\s*\\(',\n",
    "        r'^\\s*function\\s+(\\w+)\\s*\\('\n",
    "    ]\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        line = lines[i].strip()\n",
    "        \n",
    "        if not line or line.startswith(('//','#')):\n",
    "            i += 1\n",
    "            continue\n",
    "        \n",
    "        # Check for function start\n",
    "        function_match = None\n",
    "        for pattern in function_patterns:\n",
    "            match = re.match(pattern, line, re.IGNORECASE)\n",
    "            if match:\n",
    "                function_match = match\n",
    "                break\n",
    "        \n",
    "        if function_match:\n",
    "            function_name = function_match.group(1)\n",
    "            closing_brace_line = find_function_closing_brace(i, lines)\n",
    "            \n",
    "            if closing_brace_line is not None:\n",
    "                functions.append({\n",
    "                    'start_line': i,\n",
    "                    'end_line': closing_brace_line,\n",
    "                    'name': function_name\n",
    "                })\n",
    "                i = closing_brace_line + 1\n",
    "            else:\n",
    "                i += 1\n",
    "        else:\n",
    "            i += 1\n",
    "    \n",
    "    return functions\n",
    "\n",
    "def find_function_closing_brace(start_line: int, lines: list) -> int:\n",
    "    \"\"\"Find the closing brace line for a function.\"\"\"\n",
    "    brace_count = 0\n",
    "    function_started = False\n",
    "    \n",
    "    for i in range(start_line, len(lines)):\n",
    "        line = lines[i].strip()\n",
    "        \n",
    "        if not line or line.startswith(('//','#')):\n",
    "            continue\n",
    "        \n",
    "        # Simple brace counting (could be enhanced to handle strings/comments)\n",
    "        for char in line:\n",
    "            if char == '{':\n",
    "                brace_count += 1\n",
    "                function_started = True\n",
    "            elif char == '}' and function_started:\n",
    "                brace_count -= 1\n",
    "                if brace_count == 0:\n",
    "                    return i\n",
    "    \n",
    "    return None\n",
    "\n",
    "def create_smart_chunks(lines: list, function_boundaries: list, target_chunk_size: int, total_lines: int) -> list:\n",
    "    \"\"\"Create chunks that respect function boundaries.\"\"\"\n",
    "    chunks = []\n",
    "    current_pos = 0\n",
    "    functions = sorted(function_boundaries, key=lambda f: f['start_line'])\n",
    "    \n",
    "    while current_pos < total_lines:\n",
    "        # Calculate chunk end position\n",
    "        initial_end_pos = min(current_pos + target_chunk_size - 1, total_lines - 1)\n",
    "        \n",
    "        # Find functions that would be split by this chunk boundary\n",
    "        relevant_functions = [f for f in functions \n",
    "                            if (current_pos <= f['start_line'] <= initial_end_pos) or\n",
    "                               (f['start_line'] < current_pos and f['end_line'] and f['end_line'] >= current_pos)]\n",
    "        \n",
    "        # Extend chunk to complete functions if reasonable\n",
    "        final_end_pos = initial_end_pos\n",
    "        if relevant_functions:\n",
    "            for func in relevant_functions:\n",
    "                if func['end_line'] and func['end_line'] <= initial_end_pos + 300:  # Max extension\n",
    "                    final_end_pos = max(final_end_pos, func['end_line'])\n",
    "        \n",
    "        # Create chunk\n",
    "        chunk = {\n",
    "            'start_line': current_pos + 1,  # Convert to 1-based\n",
    "            'end_line': final_end_pos + 1,  # Convert to 1-based\n",
    "            'actual_size': final_end_pos - current_pos + 1,\n",
    "            'total_lines': total_lines,\n",
    "            'code': '\\n'.join(lines[current_pos:final_end_pos + 1])\n",
    "        }\n",
    "        chunks.append(chunk)\n",
    "        \n",
    "        current_pos = final_end_pos + 1\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_CHUNK_SIZE = 500  # Default chunk size in lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROMPT HELPER FUNCTION\n",
    "PROMPT_TEMPLATES = {\n",
    "    'basic': BASIC_PROMPT_TEMPLATE,\n",
    "    'comprehensive': COMPREHENSIVE_PROMPT_TEMPLATE,\n",
    "    'chunk_basic': CHUNK_BASIC_PROMPT_TEMPLATE,\n",
    "    'chunk_comprehensive': CHUNK_COMPREHENSIVE_PROMPT_TEMPLATE,\n",
    "}\n",
    "\n",
    "\n",
    "def create_prompt(code: str, strategy: str = \"basic\", **kwargs) -> str:\n",
    "    \"\"\"Create migration prompts using different strategies.\"\"\"\n",
    "    if strategy not in PROMPT_TEMPLATES:\n",
    "        raise ValueError(f\"Unknown prompting strategy: {strategy}. Available: {list(PROMPT_TEMPLATES.keys())}\")\n",
    "    \n",
    "    template = PROMPT_TEMPLATES[strategy]\n",
    "    \n",
    "    # For chunking strategies, we need additional parameters\n",
    "    if strategy.startswith('chunk_'):\n",
    "        required_params = ['filename', 'start_line', 'end_line', 'total_lines', 'chunk_number', 'total_chunks']\n",
    "        missing_params = [param for param in required_params if param not in kwargs]\n",
    "        if missing_params:\n",
    "            raise ValueError(f\"Chunking strategy requires parameters: {missing_params}\")\n",
    "    \n",
    "    return template.format(code=code, **kwargs)\n",
    "\n",
    "print(\"🎯 All prompting strategies configured and ready\")\n",
    "print(f\"📋 Available strategies: {list(PROMPT_TEMPLATES.keys())}\")\n",
    "print(\"🔧 Added chunking utilities for large files\")\n",
    "print(f\"📦 Default chunk size: {DEFAULT_CHUNK_SIZE} lines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMPLIFIED MULTI-PROVIDER MIGRATION SYSTEM\n",
    "\n",
    "class MigrationManager:\n",
    "    \"\"\"Simplified migration manager with reduced redundancy.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize_model_name(model_name: str) -> str:\n",
    "        \"\"\"Convert model name to filesystem-safe format.\"\"\"\n",
    "        return model_name.replace('/', '_').replace('-', '_').replace(':', '_').replace('.', '_').lower()\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_response(response_data: dict, file_path: Path, metadata: dict = None):\n",
    "        \"\"\"Save API response with consistent metadata format.\"\"\"\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"=== RAW MODEL RESPONSE ===\\n\")\n",
    "            \n",
    "            # Write metadata\n",
    "            if metadata:\n",
    "                for key, value in metadata.items():\n",
    "                    f.write(f\"{key.capitalize()}: {value}\\n\")\n",
    "            \n",
    "            f.write(f\"Length: {len(response_data['content'])} characters\\n\")\n",
    "            f.write(f\"Usage: {response_data.get('usage', {})}\\n\")\n",
    "            f.write(f\"Timestamp: {datetime.now()}\\n\")\n",
    "            f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "            f.write(response_data['content'])\n",
    "\n",
    "def migrate_file_chunked(filename: str, original_code: str, model_name: str, strategy: str, chunk_size: int):\n",
    "    \"\"\"Migrate large file using organized chunking.\"\"\"\n",
    "    chunks = chunk_code(original_code, chunk_size)\n",
    "    total_chunks = len(chunks)\n",
    "    \n",
    "    print(f\"📦 Split into {total_chunks} chunks of ~{chunk_size} lines each\")\n",
    "    \n",
    "    # Create organized folder structure\n",
    "    model_short = MigrationManager.normalize_model_name(model_name)\n",
    "    file_base = filename.replace('.php', '')\n",
    "    \n",
    "    file_dir = Path('chunked_model_output') / model_short / file_base\n",
    "    file_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"📁 Saving chunks to: {file_dir}\")\n",
    "    \n",
    "    # Process chunks\n",
    "    chunk_strategy = f\"chunk_{strategy}\" if not strategy.startswith('chunk_') else strategy\n",
    "    all_responses = []\n",
    "    \n",
    "    for i, chunk_info in enumerate(chunks, 1):\n",
    "        print(f\"\\n[Chunk {i}/{total_chunks}] Processing lines {chunk_info['start_line']}-{chunk_info['end_line']}...\")\n",
    "        \n",
    "        # Create prompt and make API call\n",
    "        prompt = create_prompt(\n",
    "            chunk_info['code'], chunk_strategy,\n",
    "            filename=filename, start_line=chunk_info['start_line'],\n",
    "            end_line=chunk_info['end_line'], total_lines=chunk_info['total_lines'],\n",
    "            chunk_number=i, total_chunks=total_chunks\n",
    "        )\n",
    "        \n",
    "        print(f\"📏 Chunk prompt length: {len(prompt):,} characters\")\n",
    "        response = process_api_call(model_name, prompt, file_dir / f\"{i}.txt\", {\n",
    "            'file': filename, 'model': model_name, 'strategy': chunk_strategy, 'chunk': i\n",
    "        })\n",
    "        \n",
    "        all_responses.append(response)\n",
    "        status = \"✅\" if response else \"❌\"\n",
    "        print(f\"{status} Chunk {i} {'processed successfully' if response else 'failed'}\")\n",
    "    \n",
    "    # Summary\n",
    "    successful_chunks = sum(1 for r in all_responses if r is not None)\n",
    "    print(f\"\\n🎉 Chunked migration completed!\")\n",
    "    print(f\"✅ Successful chunks: {successful_chunks}/{total_chunks}\")\n",
    "    print(f\"📁 All chunks saved in: {file_dir}\")\n",
    "    \n",
    "    return all_responses\n",
    "\n",
    "def migrate_file(filename: str, model_name: str, strategy: str = \"basic\", \n",
    "                chunk_size: int = None, auto_chunk: bool = True):\n",
    "    \"\"\"Enhanced migration function with multi-provider support.\"\"\"\n",
    "    \n",
    "    chunk_size = chunk_size or DEFAULT_CHUNK_SIZE\n",
    "    \n",
    "    if filename not in test_files:\n",
    "        print(f\"❌ File '{filename}' not found\")\n",
    "        return None\n",
    "    \n",
    "    original_code = test_files[filename]\n",
    "    line_count = len(original_code.split('\\n'))\n",
    "    \n",
    "    print(f\"🚀 Migrating {filename} using {model_name} with {strategy} strategy...\")\n",
    "    print(f\"📏 Input code length: {len(original_code):,} characters ({line_count:,} lines)\")\n",
    "    \n",
    "    # Decide processing method\n",
    "    if auto_chunk and line_count > chunk_size:\n",
    "        print(f\"📦 Large file detected ({line_count} lines) - using organized chunking\")\n",
    "        return migrate_file_chunked(filename, original_code, model_name, strategy, chunk_size)\n",
    "    else:\n",
    "        print(f\"📄 Processing as single file ({line_count} lines, chunk limit: {chunk_size})\")\n",
    "        return migrate_file_single(filename, original_code, model_name, strategy)\n",
    "\n",
    "def migrate_file_single(filename: str, original_code: str, model_name: str, strategy: str):\n",
    "    \"\"\"Migrate single file using multi-provider client.\"\"\"\n",
    "    prompt = create_prompt(original_code, strategy)\n",
    "    print(f\"📏 Prompt length: {len(prompt):,} characters\")\n",
    "    \n",
    "    # Create output path\n",
    "    model_short = MigrationManager.normalize_model_name(model_name)\n",
    "    base_name = filename.replace('.php', '')\n",
    "    output_file = Path('model_output') / model_short / f\"{base_name}.txt\"\n",
    "    output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    return process_api_call(model_name, prompt, output_file, {\n",
    "        'file': filename, 'model': model_name, 'strategy': strategy\n",
    "    })\n",
    "\n",
    "def process_api_call(model_name: str, prompt: str, output_path: Path, metadata: dict):\n",
    "    \"\"\"Unified API call processing with error handling.\"\"\"\n",
    "    print(f\"🔗 Making API call via multi-provider client...\")\n",
    "    \n",
    "    # Make API call\n",
    "    result = multi_client.make_api_call(model_name, prompt)\n",
    "    print(f\"📊 Provider: {result.get('provider', 'unknown').upper()}\")\n",
    "    \n",
    "    if not result['success']:\n",
    "        print(f\"❌ API Error: {result['error']}\")\n",
    "        return None\n",
    "    \n",
    "    # Validate response\n",
    "    raw_response = result['content']\n",
    "    print(f\"📏 Response length: {len(raw_response)} characters\")\n",
    "    \n",
    "    if not raw_response or len(raw_response.strip()) < 10:\n",
    "        print(f\"❌ Model response is empty or too short\")\n",
    "        return None\n",
    "    \n",
    "    # Save response\n",
    "    metadata['provider'] = result.get('provider', 'unknown').upper()\n",
    "    MigrationManager.save_response(result, output_path, metadata)\n",
    "    print(f\"✅ Response saved to: {output_path}\")\n",
    "    \n",
    "    return raw_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if old_version_path.exists():\n",
    "    # Recursively find all PHP files in all subfolders\n",
    "    for php_file in old_version_path.rglob('*.php'):\n",
    "        try:\n",
    "            with open(php_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                content = f.read()\n",
    "                if content.strip():\n",
    "                    test_files[php_file.name] = content\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Could not load {php_file.name}: {e}\")\n",
    "    \n",
    "    print(f\"📁 Loaded {len(test_files)} PHP files from selected_100_files:\")\n",
    "    for filename in sorted(test_files.keys()):\n",
    "        size = len(test_files[filename])\n",
    "        print(f\"   📄 {filename} ({size:,} chars)\")\n",
    "else:\n",
    "    print(\"❌ selected_100_files directory not found\")\n",
    "    print(\"💡 Make sure the selected 100 files are in 'selected_100_files/' directory\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def batch_migrate(filenames: list, model: str = \"gemini-1.5-pro\", strategy: str = \"basic\", \n",
    "                 chunk_size: int = None, auto_chunk: bool = True):\n",
    "    \"\"\"Migrate multiple files with multi-provider chunking support.\"\"\"\n",
    "    chunk_size = chunk_size or DEFAULT_CHUNK_SIZE\n",
    "    provider = multi_client.detect_provider(model)\n",
    "    \n",
    "    print(f\"🔄 Batch migrating {len(filenames)} files using {provider.upper()}\")\n",
    "    if auto_chunk:\n",
    "        print(f\"📦 Auto-chunking enabled for files > {chunk_size} lines\")\n",
    "    \n",
    "    results = []\n",
    "    stats = {'files': 0, 'chunks': 0, 'success_files': 0, 'success_chunks': 0}\n",
    "    \n",
    "    for i, filename in enumerate(filenames, 1):\n",
    "        print(f\"\\n[{i}/{len(filenames)}] Processing {filename}...\")\n",
    "        result = migrate_file(filename, model, strategy, chunk_size=chunk_size, auto_chunk=auto_chunk)\n",
    "        results.append(result)\n",
    "        \n",
    "        # Update statistics\n",
    "        stats['files'] += 1\n",
    "        if result is not None:\n",
    "            if isinstance(result, list):  # Chunked file\n",
    "                stats['chunks'] += len(result)\n",
    "                stats['success_chunks'] += sum(1 for r in result if r is not None)\n",
    "                if any(r is not None for r in result):\n",
    "                    stats['success_files'] += 1\n",
    "            else:  # Single file\n",
    "                stats['chunks'] += 1\n",
    "                stats['success_chunks'] += 1\n",
    "                stats['success_files'] += 1\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n🎉 Batch migration completed!\")\n",
    "    print(f\"✅ Successful files: {stats['success_files']}/{stats['files']}\")\n",
    "    if stats['chunks'] > len(filenames):\n",
    "        print(f\"📦 Total chunks processed: {stats['success_chunks']}/{stats['chunks']}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def analyze_file_sizes(chunk_threshold: int = None):\n",
    "    \"\"\"Analyze file sizes to see chunking requirements.\"\"\"\n",
    "    chunk_threshold = chunk_threshold or DEFAULT_CHUNK_SIZE\n",
    "    \n",
    "    if not test_files:\n",
    "        print(\"❌ No test files loaded\")\n",
    "        return\n",
    "    \n",
    "    # Categorize files\n",
    "    small_files, large_files = [], []\n",
    "    for filename, content in test_files.items():\n",
    "        line_count = len(content.split('\\n'))\n",
    "        char_count = len(content)\n",
    "        file_info = (filename, line_count, char_count)\n",
    "        \n",
    "        if line_count <= chunk_threshold:\n",
    "            small_files.append(file_info)\n",
    "        else:\n",
    "            large_files.append(file_info)\n",
    "    \n",
    "    print(\"📊 File Size Analysis\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Small files summary\n",
    "    print(f\"📄 Small files (≤{chunk_threshold} lines): {len(small_files)}\")\n",
    "    for filename, lines, chars in sorted(small_files, key=lambda x: x[1], reverse=True)[:10]:\n",
    "        print(f\"   {filename}: {lines:,} lines, {chars:,} chars\")\n",
    "    if len(small_files) > 10:\n",
    "        print(f\"   ... and {len(small_files) - 10} more\")\n",
    "    \n",
    "    # Large files summary\n",
    "    if large_files:\n",
    "        print(f\"\\n📦 Large files (>{chunk_threshold} lines): {len(large_files)}\")\n",
    "        total_lines = sum(lines for _, lines, _ in large_files)\n",
    "        total_chunks = sum((lines + chunk_threshold - 1) // chunk_threshold for _, lines, _ in large_files)\n",
    "        \n",
    "        for filename, lines, chars in sorted(large_files, key=lambda x: x[1], reverse=True):\n",
    "            chunks = (lines + chunk_threshold - 1) // chunk_threshold\n",
    "            print(f\"   {filename}: {lines:,} lines, {chars:,} chars → {chunks} chunks\")\n",
    "        \n",
    "        print(f\"\\n📊 Large files summary: {total_lines:,} total lines → {total_chunks} chunks\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANALYZE YOUR FILES FOR CHUNKING\n",
    "print(\"🔍 Analyzing loaded files to see chunking requirements...\")\n",
    "analyze_file_sizes()\n",
    "\n",
    "# Test chunking on a sample file\n",
    "print(\"\\n🧪 Testing chunking logic on sample files...\")\n",
    "for filename in list(test_files.keys())[:3]:\n",
    "    code = test_files[filename]\n",
    "    line_count = len(code.split('\\n'))\n",
    "    chunks = chunk_code(code, DEFAULT_CHUNK_SIZE)\n",
    "    \n",
    "    print(f\"\\n📄 {filename}:\")\n",
    "    print(f\"   Lines: {line_count:,}\")\n",
    "    print(f\"   Chunks: {len(chunks)}\")\n",
    "    \n",
    "    if len(chunks) > 1:\n",
    "        print(f\"   Chunk breakdown:\")\n",
    "        for i, chunk_info in enumerate(chunks, 1):\n",
    "            print(f\"     Chunk {i}: lines {chunk_info['start_line']}-{chunk_info['end_line']} ({chunk_info['end_line'] - chunk_info['start_line'] + 1} lines)\")\n",
    "\n",
    "print(f\"\\n✅ File analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT THESE LINES FOR BATCH MIGRATION WITH DIFFERENT PROVIDERS:\n",
    "\n",
    "# Google AI batch migration:\n",
    "# batch_migrate(list(test_files.keys())[:3], model='gemini-1.5-pro', strategy='basic')\n",
    "\n",
    "# OpenRouter batch migration:\n",
    "# batch_migrate(list(test_files.keys())[:3], model='mistralai/mistral-small-3.2-24b-instruct:free', strategy='basic')\n",
    "\n",
    "# Mixed provider batch (you can mix and match in sequence):\n",
    "migrate_file('001_getid3.lib.php', 'mistralai/mistral-small-3.2-24b-instruct:free', 'basic')\n",
    "# migrate_file('file2.php', 'anthropic/claude-3.5-sonnet', 'comprehensive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMPLIFIED OUTPUT PARSER - CLEAN AND MINIMAL\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "class OutputParser:\n",
    "    \"\"\"Simplified parser for model responses with removed redundancy.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model_output_path = Path('model_output')\n",
    "        self.parsed_path = Path('new-version')\n",
    "        self.parsed_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    def extract_migrated_code(self, response_content: str) -> str:\n",
    "        \"\"\"Extract code between MIGRATION_START and MIGRATION_END markers.\"\"\"\n",
    "        start_match = re.search(r'//\\s*MIGRATION_START\\s*\\n', response_content, re.IGNORECASE)\n",
    "        end_match = re.search(r'\\n//\\s*MIGRATION_END', response_content, re.IGNORECASE)\n",
    "        \n",
    "        if start_match and end_match:\n",
    "            return response_content[start_match.end():end_match.start()].strip()\n",
    "        return \"\"\n",
    "    \n",
    "    def extract_metadata(self, response_content: str) -> dict:\n",
    "        \"\"\"Extract metadata from response file header.\"\"\"\n",
    "        metadata = {}\n",
    "        for line in response_content.split('\\n')[:15]:\n",
    "            if ':' in line:\n",
    "                key, value = line.split(':', 1)\n",
    "                key = key.strip().lower()\n",
    "                if key in ['file', 'model', 'strategy']:\n",
    "                    metadata[f'original_{key}' if key == 'file' else key] = value.strip()\n",
    "        return metadata\n",
    "    \n",
    "    def parse_single_file(self, response_file: Path) -> dict:\n",
    "        \"\"\"Parse a single response file.\"\"\"\n",
    "        try:\n",
    "            print(f\"Processing {response_file.name}\")\n",
    "            \n",
    "            with open(response_file, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            metadata = self.extract_metadata(content)\n",
    "            migrated_code = self.extract_migrated_code(content)\n",
    "            \n",
    "            if not metadata.get('original_file'):\n",
    "                print(f\"   ERROR: No original file found in metadata\")\n",
    "                return {'success': False}\n",
    "            \n",
    "            if not migrated_code:\n",
    "                print(f\"   ERROR: No migrated code found between markers\")\n",
    "                return {'success': False}\n",
    "            \n",
    "            print(f\"   SUCCESS: Found {len(migrated_code)} chars of migrated code\")\n",
    "            return {\n",
    "                'success': True,\n",
    "                'metadata': metadata,\n",
    "                'migrated_code': migrated_code\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"   ERROR: {e}\")\n",
    "            return {'success': False, 'error': str(e)}\n",
    "    \n",
    "    def _normalize_model_name(self, model_name: str) -> str:\n",
    "        \"\"\"Convert model name to filesystem-safe format.\"\"\"\n",
    "        return re.sub(r'[/:.-]', '_', model_name.lower())\n",
    "    \n",
    "    def _determine_output_file(self, metadata: dict, response_filename: str, model_folder: Path) -> Path:\n",
    "        \"\"\"Determine the output file path.\"\"\"\n",
    "        original_filename = metadata.get('original_file')\n",
    "        if original_filename:\n",
    "            return model_folder / original_filename\n",
    "        \n",
    "        # Fallback: derive from response filename\n",
    "        php_filename = response_filename.replace('.txt', '.php') if response_filename.endswith('.txt') else f\"{response_filename}.php\"\n",
    "        return model_folder / php_filename\n",
    "    \n",
    "    def save_parsed_file(self, result: dict, response_filename: str, model_folder_name: str = None) -> bool:\n",
    "        \"\"\"Save parsed result to organized structure.\"\"\"\n",
    "        try:\n",
    "            metadata = result['metadata']\n",
    "            migrated_code = result['migrated_code']\n",
    "            \n",
    "            # Determine model folder name\n",
    "            model_clean = model_folder_name or self._normalize_model_name(metadata.get('model', 'unknown_model'))\n",
    "            \n",
    "            # Create model folder and output file\n",
    "            model_folder = self.parsed_path / model_clean\n",
    "            model_folder.mkdir(exist_ok=True)\n",
    "            output_file = self._determine_output_file(metadata, response_filename, model_folder)\n",
    "            \n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(migrated_code)\n",
    "            \n",
    "            print(f\"   ✅ SAVED: {output_file}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ SAVE ERROR: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def _process_response_files(self, response_files: list, model_folder_name: str = None) -> tuple:\n",
    "        \"\"\"Process a list of response files and return success/failed counts.\"\"\"\n",
    "        success_count = 0\n",
    "        failed_count = 0\n",
    "        \n",
    "        for response_file in response_files:\n",
    "            result = self.parse_single_file(response_file)\n",
    "            \n",
    "            if result['success']:\n",
    "                # Update metadata with model folder name if provided\n",
    "                if model_folder_name and 'metadata' in result:\n",
    "                    result['metadata']['model_folder'] = model_folder_name\n",
    "                \n",
    "                if self.save_parsed_file(result, response_file.name, model_folder_name):\n",
    "                    success_count += 1\n",
    "                else:\n",
    "                    failed_count += 1\n",
    "            else:\n",
    "                failed_count += 1\n",
    "        \n",
    "        return success_count, failed_count\n",
    "    \n",
    "    def process_all_responses(self):\n",
    "        \"\"\"Process all response files in model_output directory.\"\"\"\n",
    "        print(\"🔄 Processing all model responses...\")\n",
    "        \n",
    "        if not self.model_output_path.exists():\n",
    "            print(f\"❌ Directory {self.model_output_path} not found\")\n",
    "            return\n",
    "        \n",
    "        # Look for model subfolders\n",
    "        model_folders = [d for d in self.model_output_path.iterdir() if d.is_dir()]\n",
    "        \n",
    "        if not model_folders:\n",
    "            # Fallback: process files directly (old structure)\n",
    "            response_files = list(self.model_output_path.glob('*.txt'))\n",
    "            if response_files:\n",
    "                print(f\"📁 Found {len(response_files)} response files in old structure\")\n",
    "                success, failed = self._process_response_files(response_files)\n",
    "                print(f\"\\n🎉 Processing completed!\")\n",
    "                print(f\"✅ Successfully processed: {success} files\")\n",
    "                print(f\"❌ Failed to process: {failed} files\")\n",
    "            else:\n",
    "                print(\"❌ No model folders or .txt files found in model_output/\")\n",
    "            return\n",
    "        \n",
    "        print(f\"📁 Found {len(model_folders)} model folders:\")\n",
    "        for folder in model_folders:\n",
    "            print(f\"   📂 {folder.name}/\")\n",
    "        \n",
    "        total_success = 0\n",
    "        total_failed = 0\n",
    "        \n",
    "        # Process each model folder\n",
    "        for model_folder in model_folders:\n",
    "            print(f\"\\n🔄 Processing model: {model_folder.name}\")\n",
    "            \n",
    "            response_files = list(model_folder.glob('*.txt'))\n",
    "            print(f\"   📄 Found {len(response_files)} response files\")\n",
    "            \n",
    "            if not response_files:\n",
    "                print(\"   ⚠️  No .txt files found in this model folder\")\n",
    "                continue\n",
    "            \n",
    "            success_count, failed_count = self._process_response_files(response_files, model_folder.name)\n",
    "            \n",
    "            print(f\"   ✅ Successfully processed: {success_count} files\")\n",
    "            print(f\"   ❌ Failed to process: {failed_count} files\")\n",
    "            \n",
    "            total_success += success_count\n",
    "            total_failed += failed_count\n",
    "        \n",
    "        print(f\"\\n🎉 Overall processing completed!\")\n",
    "        print(f\"✅ Total successfully processed: {total_success} files\")\n",
    "        print(f\"❌ Total failed to process: {total_failed} files\")\n",
    "        \n",
    "        # Show results summary\n",
    "        if total_success > 0:\n",
    "            print(f\"\\n📁 Results saved to '{self.parsed_path}':\")\n",
    "            for model_folder in sorted(self.parsed_path.iterdir()):\n",
    "                if model_folder.is_dir():\n",
    "                    php_files = list(model_folder.glob('*.php'))\n",
    "                    print(f\"   📂 {model_folder.name}/ ({len(php_files)} files)\")\n",
    "\n",
    "# Initialize simplified parser\n",
    "parser = OutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILE RECONSTRUCTOR - COMBINES PARSED CHUNKS INTO COMPLETE FILES\n",
    "class FileReconstructor:\n",
    "    \"\"\"Reconstructs complete files from parsed chunk files.\"\"\"\n",
    "    \n",
    "    def __init__(self, parser):\n",
    "        self.parser = parser  # Use the simple parser for individual chunks\n",
    "        self.chunked_output_path = Path('chunked_model_output')\n",
    "        self.final_output_path = Path('new-version')\n",
    "        self.final_output_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    def find_chunked_files(self):\n",
    "        \"\"\"Find all chunked file directories.\"\"\"\n",
    "        if not self.chunked_output_path.exists():\n",
    "            print(\"No chunked_model_output directory found\")\n",
    "            return []\n",
    "        \n",
    "        chunked_files = []\n",
    "        \n",
    "        for model_dir in self.chunked_output_path.iterdir():\n",
    "            if model_dir.is_dir():\n",
    "                for file_dir in model_dir.iterdir():\n",
    "                    if file_dir.is_dir():\n",
    "                        # Check if it has numbered chunk files\n",
    "                        chunk_files = list(file_dir.glob('*.txt'))\n",
    "                        if chunk_files:\n",
    "                            chunked_files.append({\n",
    "                                'model': model_dir.name,\n",
    "                                'filename': file_dir.name,\n",
    "                                'directory': file_dir,\n",
    "                                'chunk_count': len(chunk_files)\n",
    "                            })\n",
    "        \n",
    "        return chunked_files\n",
    "    \n",
    "    def get_chunk_files(self, directory: Path):\n",
    "        \"\"\"Get all chunk files from a directory, sorted by number.\"\"\"\n",
    "        chunk_files = []\n",
    "        \n",
    "        for file in directory.glob('*.txt'):\n",
    "            try:\n",
    "                # Extract number from filename (1.txt -> 1)\n",
    "                chunk_num = int(file.stem)\n",
    "                chunk_files.append((chunk_num, file))\n",
    "            except ValueError:\n",
    "                print(f\"WARNING: Skipping non-numeric chunk file: {file.name}\")\n",
    "        \n",
    "        # Sort by chunk number\n",
    "        chunk_files.sort(key=lambda x: x[0])\n",
    "        return chunk_files\n",
    "    \n",
    "    def reconstruct_file(self, file_info: dict):\n",
    "        \"\"\"Reconstruct a complete file from its chunks.\"\"\"\n",
    "        print(f\"\\nReconstructing {file_info['filename']}.php from {file_info['chunk_count']} chunks\")\n",
    "        print(f\"Model: {file_info['model']}\")\n",
    "        print(f\"Directory: {file_info['directory']}\")\n",
    "        \n",
    "        # Get sorted chunk files\n",
    "        chunk_files = self.get_chunk_files(file_info['directory'])\n",
    "        \n",
    "        if not chunk_files:\n",
    "            print(\"   ERROR: No valid chunk files found\")\n",
    "            return False\n",
    "        \n",
    "        # Check for missing chunks\n",
    "        expected_numbers = list(range(1, len(chunk_files) + 1))\n",
    "        actual_numbers = [num for num, _ in chunk_files]\n",
    "        missing = set(expected_numbers) - set(actual_numbers)\n",
    "        \n",
    "        if missing:\n",
    "            print(f\"   WARNING: Missing chunks: {sorted(missing)}\")\n",
    "        \n",
    "        print(f\"   Found chunks: {actual_numbers}\")\n",
    "        \n",
    "        # Parse each chunk\n",
    "        parsed_chunks = []\n",
    "        metadata = None\n",
    "        \n",
    "        for chunk_num, chunk_file in chunk_files:\n",
    "            print(f\"   Processing chunk {chunk_num}...\")\n",
    "            result = self.parser.parse_single_file(chunk_file)\n",
    "            \n",
    "            if result['success']:\n",
    "                parsed_chunks.append({\n",
    "                    'number': chunk_num,\n",
    "                    'code': result['migrated_code'],\n",
    "                    'metadata': result['metadata']\n",
    "                })\n",
    "                \n",
    "                # Use metadata from first successful chunk\n",
    "                if metadata is None:\n",
    "                    metadata = result['metadata']\n",
    "                    \n",
    "                print(f\"      SUCCESS: {len(result['migrated_code'])} chars\")\n",
    "            else:\n",
    "                print(f\"      ERROR: Failed to parse chunk {chunk_num}\")\n",
    "                parsed_chunks.append({\n",
    "                    'number': chunk_num,\n",
    "                    'code': None,\n",
    "                    'metadata': None\n",
    "                })\n",
    "        \n",
    "        if not any(chunk['code'] for chunk in parsed_chunks):\n",
    "            print(\"   ERROR: No chunks could be parsed successfully\")\n",
    "            return False\n",
    "        \n",
    "        # Combine chunks\n",
    "        combined_code = []\n",
    "        successful_chunks = 0\n",
    "        final_code = \"\"  # Initialize final_code\n",
    "        \n",
    "        for chunk in parsed_chunks:\n",
    "            if chunk['code']:\n",
    "                combined_code.append(chunk['code'])\n",
    "                successful_chunks += 1\n",
    "            else:\n",
    "                print(f\"   WARNING: Chunk {chunk['number']} failed - adding placeholder comment\")\n",
    "                combined_code.append(f\"// ERROR: Chunk {chunk['number']} failed to parse\")\n",
    "        \n",
    "        final_code = ''.join(combined_code)\n",
    "        print(f\"   Combined {successful_chunks}/{len(parsed_chunks)} chunks successfully\")\n",
    "        print(f\"   Final code length: {len(final_code)} characters\")\n",
    "        \n",
    "        # Save reconstructed file\n",
    "        return self.save_reconstructed_file(file_info, final_code, metadata)\n",
    "    \n",
    "    def save_reconstructed_file(self, file_info: dict, code: str, metadata: dict):\n",
    "        \"\"\"Save the reconstructed complete file.\"\"\"\n",
    "        try:\n",
    "            # Create model folder in final output\n",
    "            model_folder = self.final_output_path / file_info['model']\n",
    "            model_folder.mkdir(exist_ok=True)\n",
    "            \n",
    "            # Save the reconstructed file\n",
    "            output_file = model_folder / f\"{file_info['filename']}.php\"\n",
    "            \n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                # Write clean PHP code without metadata header\n",
    "                f.write(code)\n",
    "            \n",
    "            print(f\"   SAVED: {output_file}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ERROR saving file: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def reconstruct_all_files(self):\n",
    "        \"\"\"Reconstruct all chunked files found.\"\"\"\n",
    "        print(\"🔧 Starting file reconstruction...\")\n",
    "        \n",
    "        chunked_files = self.find_chunked_files()\n",
    "        \n",
    "        if not chunked_files:\n",
    "            print(\"No chunked files found to reconstruct\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Found {len(chunked_files)} chunked files to reconstruct:\")\n",
    "        for file_info in chunked_files:\n",
    "            print(f\"   {file_info['model']}/{file_info['filename']}.php ({file_info['chunk_count']} chunks)\")\n",
    "        \n",
    "        successful = 0\n",
    "        failed = 0\n",
    "        \n",
    "        for file_info in chunked_files:\n",
    "            if self.reconstruct_file(file_info):\n",
    "                successful += 1\n",
    "            else:\n",
    "                failed += 1\n",
    "        \n",
    "        print(f\"\\n🎉 Reconstruction completed!\")\n",
    "        print(f\"✅ Successfully reconstructed: {successful} files\")\n",
    "        print(f\"❌ Failed to reconstruct: {failed} files\")\n",
    "        \n",
    "        if successful > 0:\n",
    "            print(f\"\\n📁 Reconstructed files saved to: {self.final_output_path}\")\n",
    "            for model_folder in sorted(self.final_output_path.iterdir()):\n",
    "                if model_folder.is_dir():\n",
    "                    php_files = list(model_folder.glob('*.php'))\n",
    "                    print(f\"   {model_folder.name}/ ({len(php_files)} files)\")\n",
    "\n",
    "# Initialize the reconstructor with our simple parser\n",
    "reconstructor = FileReconstructor(parser)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Processing all model responses...\n",
      "❌ Directory model_output not found\n",
      "🔧 Starting file reconstruction...\n",
      "Found 1 chunked files to reconstruct:\n",
      "   mistralai_mistral_small_3_2_24b_instruct_free/001_getid3.lib.php (3 chunks)\n",
      "\n",
      "Reconstructing 001_getid3.lib.php from 3 chunks\n",
      "Model: mistralai_mistral_small_3_2_24b_instruct_free\n",
      "Directory: chunked_model_output\\mistralai_mistral_small_3_2_24b_instruct_free\\001_getid3.lib\n",
      "   Found chunks: [1, 2, 3]\n",
      "   Processing chunk 1...\n",
      "Processing 1.txt\n",
      "   SUCCESS: Found 19795 chars of migrated code\n",
      "      SUCCESS: 19795 chars\n",
      "   Processing chunk 2...\n",
      "Processing 2.txt\n",
      "   SUCCESS: Found 19548 chars of migrated code\n",
      "      SUCCESS: 19548 chars\n",
      "   Processing chunk 3...\n",
      "Processing 3.txt\n",
      "   SUCCESS: Found 8245 chars of migrated code\n",
      "      SUCCESS: 8245 chars\n",
      "   Combined 3/3 chunks successfully\n",
      "   Final code length: 47588 characters\n",
      "   SAVED: new-version\\mistralai_mistral_small_3_2_24b_instruct_free\\001_getid3.lib.php\n",
      "\n",
      "🎉 Reconstruction completed!\n",
      "✅ Successfully reconstructed: 1 files\n",
      "❌ Failed to reconstruct: 0 files\n",
      "\n",
      "📁 Reconstructed files saved to: new-version\n",
      "   mistralai_mistral_small_3_2_24b_instruct_free/ (1 files)\n"
     ]
    }
   ],
   "source": [
    "parser.process_all_responses()\n",
    "reconstructor.reconstruct_all_files()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
