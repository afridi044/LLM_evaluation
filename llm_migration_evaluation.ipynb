{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Simple LLM Migration Tool\n",
    "\n",
    "\n",
    "This notebook implements a simple tool for testing open source Large Language Models' capability in migrating legacy PHP code (WordPress 4.3) to modern PHP 8.3 standards.\n",
    "\n",
    "## What it does:\n",
    "- ‚úÖ Sends your prompt to any OpenRouter model\n",
    "- ‚úÖ Saves the complete raw response to a text file\n",
    "- ‚úÖ No complex evaluation - just pure model output\n",
    "- ‚úÖ Perfect for examining what models actually return\n",
    "\n",
    "**Browse available models at https://openrouter.ai/models**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Simple LLM Migration Tool Initialized\n",
      "‚öôÔ∏è  Timestamp: 2025-09-02 00:49:10\n",
      "‚úÖ OpenRouter client initialized successfully with environment variable\n",
      "\n",
      "üåê Visit https://openrouter.ai/models to browse available models\n",
      "üìã Use model IDs like 'anthropic/claude-3.5-sonnet', 'meta-llama/llama-3.1-8b-instruct', etc.\n"
     ]
    }
   ],
   "source": [
    "# Setup Environment\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# OpenRouter API setup\n",
    "print(\"üî¨ Simple LLM Migration Tool Initialized\")\n",
    "print(f\"‚öôÔ∏è  Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Initialize OpenRouter client\n",
    "try:\n",
    "    # Get API key from environment variable\n",
    "    OPENROUTER_API_KEY = os.getenv('OPENROUTER_API_KEY')\n",
    "    \n",
    "    if not OPENROUTER_API_KEY:\n",
    "        raise ValueError(\"OPENROUTER_API_KEY not found in environment variables. Please check your .env file.\")\n",
    "    \n",
    "    # Create the OpenRouter client\n",
    "    client = openai.OpenAI(\n",
    "        base_url=\"https://openrouter.ai/api/v1\",\n",
    "        api_key=OPENROUTER_API_KEY,\n",
    "    )\n",
    "    print(\"‚úÖ OpenRouter client initialized successfully with environment variable\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error initializing OpenRouter client: {e}\")\n",
    "    print(\"üí° Make sure you have a .env file with OPENROUTER_API_KEY=your_key_here\")\n",
    "    client = None\n",
    "\n",
    "print(\"\\nüåê Visit https://openrouter.ai/models to browse available models\")\n",
    "print(\"üìã Use model IDs like 'anthropic/claude-3.5-sonnet', 'meta-llama/llama-3.1-8b-instruct', etc.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD TEST FILES\n",
    "test_files = {}\n",
    "old_version_path = Path('selected_100_files\\extra_large_1000_plus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Basic prompting strategy configured with fixed marker placement\n"
     ]
    }
   ],
   "source": [
    "# BASIC PROMPTING STRATEGY\n",
    "BASIC_PROMPT_TEMPLATE = \"\"\"You are a senior PHP developer with expertise in legacy code modernization. Your task is to migrate this PHP code to PHP 8.3 standards while maintaining functional equivalence.\n",
    "\n",
    "Please migrate the following PHP code to PHP 8.3:\n",
    "\n",
    "{code}\n",
    "\n",
    "Your response should follow this EXACT format:\n",
    "\n",
    "// MIGRATION_START\n",
    "[your migrated PHP code here]\n",
    "// MIGRATION_END\n",
    "\n",
    "CRITICAL FORMATTING REQUIREMENT: \n",
    "- Place the MIGRATION_START marker BEFORE the opening <?php tag\n",
    "- Place the MIGRATION_END marker AFTER the closing PHP code\n",
    "- Do NOT place these markers inside the PHP code itself\n",
    "\n",
    "Provide only the migrated PHP code with the markers placed correctly outside the PHP code block, no additional commentary.\"\"\"\n",
    "\n",
    "print(\"‚úÖ Basic prompting strategy configured with fixed marker placement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Comprehensive and chunking prompting strategies configured with fixed marker placement\n",
      "üîß Updated all prompts to prevent placing MIGRATION markers inside PHP code\n"
     ]
    }
   ],
   "source": [
    "# COMPREHENSIVE PROMPTING STRATEGY\n",
    "COMPREHENSIVE_PROMPT_TEMPLATE = \"\"\"You are a senior PHP developer with expertise in legacy code modernization. Your task is to migrate old PHP code up to PHP 8.3 standards while maintaining the functionality of the original code.\n",
    "\n",
    "Migration Requirements:\n",
    "1. Update deprecated syntax\n",
    "2. Replace deprecated functions\n",
    "3. Implement modern PHP features\n",
    "4. Improve security and code quality\n",
    "5. Maintain functional equivalence\n",
    "6. Enforce strict typing\n",
    "7. Adopt core PHP 8.3 constructs\n",
    "\n",
    "Please migrate the following PHP code to PHP 8.3:\n",
    "\n",
    "{code}\n",
    "\n",
    "\n",
    "\n",
    "Your response should follow this EXACT format:\n",
    "\n",
    "// MIGRATION_START\n",
    "[your migrated PHP code here]\n",
    "// MIGRATION_END\n",
    "\n",
    "CRITICAL FORMATTING REQUIREMENT: \n",
    "- Place the MIGRATION_START marker BEFORE the opening <?php tag\n",
    "- Place the MIGRATION_END marker AFTER the closing PHP code\n",
    "- Do NOT place these markers inside the PHP code itself\n",
    "\n",
    "Include the markers as comments OUTSIDE the PHP code block. Keep the original comments as they are.\n",
    "Do not add any other text, explanations, or commentary outside the markers. Make sure you give the COMPLETE migrated code.\"\"\"\n",
    "\n",
    "# CHUNKING PROMPTS FOR LARGE FILES\n",
    "CHUNK_BASIC_PROMPT_TEMPLATE = \"\"\"You are a senior PHP developer with expertise in legacy code modernization. Your task is to migrate this PARTIAL SEGMENT of a larger PHP file up to PHP 8.3 standards.\n",
    "\n",
    "CONTEXT:\n",
    "- Original file: {filename}\n",
    "- Processing lines: {start_line} to {end_line} (of {total_lines} total lines)\n",
    "- This is chunk {chunk_number} of {total_chunks}\n",
    "\n",
    "CRITICAL INSTRUCTIONS:\n",
    "1. This is only a SEGMENT of the complete file\n",
    "2. Do NOT add opening <?php tags if the code segment doesn't start with one\n",
    "3. Do NOT add closing ?> tags \n",
    "4. Do NOT try to complete missing parts or add code that isn't provided\n",
    "5. Preserve the exact structure - if it starts with a method, start with that method\n",
    "6. If it starts mid-class, do NOT add class opening braces\n",
    "\n",
    "Please migrate ONLY the following PHP code segment to PHP 8.3:\n",
    "\n",
    "{code}\n",
    "\n",
    "Your response should follow this EXACT format:\n",
    "\n",
    "// MIGRATION_START\n",
    "[your migrated code segment here - exactly as provided, no extra <?php tags]\n",
    "// MIGRATION_END\n",
    "\n",
    "CRITICAL FORMATTING REQUIREMENT: \n",
    "- Place the MIGRATION_START marker BEFORE the code segment\n",
    "- Place the MIGRATION_END marker AFTER the code segment\n",
    "- Do NOT place these markers inside the PHP code itself\n",
    "\n",
    "Migrate only the provided code segment. Do not add missing functions, classes, or try to complete the file.\"\"\"\n",
    "\n",
    "CHUNK_COMPREHENSIVE_PROMPT_TEMPLATE = \"\"\"You are a senior PHP developer with expertise in legacy code modernization. Your task is to migrate this PARTIAL SEGMENT of a larger PHP file to PHP 8.3 standards while maintaining functional equivalence.\n",
    "\n",
    "CONTEXT:\n",
    "- Original file: {filename}\n",
    "- Processing lines: {start_line} to {end_line} (of {total_lines} total lines)  \n",
    "- This is chunk {chunk_number} of {total_chunks}\n",
    "\n",
    "CRITICAL INSTRUCTIONS:\n",
    "1. This is only a SEGMENT of the complete file\n",
    "2. Do NOT add opening <?php tags if the code segment doesn't start with one\n",
    "3. Do NOT add closing ?> tags\n",
    "4. Do NOT try to complete missing parts or add code that isn't provided\n",
    "5. Preserve the exact structure - if it starts with a method, start with that method\n",
    "6. If it starts mid-class, do NOT add class opening braces\n",
    "\n",
    "Migration Requirements for this segment:\n",
    "1. Update deprecated syntax\n",
    "2. Replace deprecated functions\n",
    "3. Implement modern PHP features\n",
    "4. Improve security and code quality\n",
    "5. Maintain functional equivalence\n",
    "6. Enforce strict typing\n",
    "7. Adopt core PHP 8.3 constructs\n",
    "\n",
    "\n",
    "Please migrate ONLY the following PHP code segment to PHP 8.3:\n",
    "\n",
    "{code}\n",
    "\n",
    "Your response should follow this EXACT format:\n",
    "\n",
    "// MIGRATION_START\n",
    "[your migrated code segment here - exactly as provided, no extra <?php tags]\n",
    "// MIGRATION_END\n",
    "\n",
    "CRITICAL FORMATTING REQUIREMENT: \n",
    "- Place the MIGRATION_START marker BEFORE the code segment\n",
    "- Place the MIGRATION_END marker AFTER the code segment\n",
    "- Do NOT place these markers inside the PHP code itself\n",
    "\n",
    "Include the markers as comments OUTSIDE the code segment. Keep the original comments as they are.\n",
    "Migrate only the provided code segment. Do not add missing functions, classes, or try to complete the file.\"\"\"\n",
    "\n",
    "print(\"‚úÖ Comprehensive and chunking prompting strategies configured with fixed marker placement\")\n",
    "print(\"üîß Updated all prompts to prevent placing MIGRATION markers inside PHP code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_CHUNK_SIZE = 500  # Set your preferred chunk size here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ All prompting strategies configured and ready\n",
      "üìã Available strategies: ['basic', 'comprehensive', 'chunk_basic', 'chunk_comprehensive']\n",
      "üîß Added chunking utilities for large files\n",
      "üì¶ Default chunk size: 500 lines\n"
     ]
    }
   ],
   "source": [
    "# PROMPT HELPER FUNCTION\n",
    "PROMPT_TEMPLATES = {\n",
    "    'basic': BASIC_PROMPT_TEMPLATE,\n",
    "    'comprehensive': COMPREHENSIVE_PROMPT_TEMPLATE,\n",
    "    'chunk_basic': CHUNK_BASIC_PROMPT_TEMPLATE,\n",
    "    'chunk_comprehensive': CHUNK_COMPREHENSIVE_PROMPT_TEMPLATE,\n",
    "}\n",
    "\n",
    "\n",
    "def create_prompt(code: str, strategy: str = \"basic\", **kwargs) -> str:\n",
    "    \"\"\"Create migration prompts using different strategies.\"\"\"\n",
    "    if strategy not in PROMPT_TEMPLATES:\n",
    "        raise ValueError(f\"Unknown prompting strategy: {strategy}. Available: {list(PROMPT_TEMPLATES.keys())}\")\n",
    "    \n",
    "    template = PROMPT_TEMPLATES[strategy]\n",
    "    \n",
    "    # For chunking strategies, we need additional parameters\n",
    "    if strategy.startswith('chunk_'):\n",
    "        required_params = ['filename', 'start_line', 'end_line', 'total_lines', 'chunk_number', 'total_chunks']\n",
    "        missing_params = [param for param in required_params if param not in kwargs]\n",
    "        if missing_params:\n",
    "            raise ValueError(f\"Chunking strategy requires parameters: {missing_params}\")\n",
    "    \n",
    "    return template.format(code=code, **kwargs)\n",
    "\n",
    "def chunk_code(code: str, chunk_size: int = None) -> list:\n",
    "    \"\"\"Split code into chunks by line count.\"\"\"\n",
    "    if chunk_size is None:\n",
    "        chunk_size = DEFAULT_CHUNK_SIZE\n",
    "    \n",
    "    lines = code.split('\\n')\n",
    "    total_lines = len(lines)\n",
    "    \n",
    "    if total_lines <= chunk_size:\n",
    "        return [code]  # No need to chunk\n",
    "    \n",
    "    chunks = []\n",
    "    for i in range(0, total_lines, chunk_size):\n",
    "        chunk_lines = lines[i:i + chunk_size]\n",
    "        chunk_code = '\\n'.join(chunk_lines)\n",
    "        \n",
    "        chunk_info = {\n",
    "            'code': chunk_code,\n",
    "            'start_line': i + 1,\n",
    "            'end_line': min(i + chunk_size, total_lines),\n",
    "            'total_lines': total_lines\n",
    "        }\n",
    "        chunks.append(chunk_info)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "print(\"üéØ All prompting strategies configured and ready\")\n",
    "print(f\"üìã Available strategies: {list(PROMPT_TEMPLATES.keys())}\")\n",
    "print(\"üîß Added chunking utilities for large files\")\n",
    "print(f\"üì¶ Default chunk size: {DEFAULT_CHUNK_SIZE} lines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ NEW Organized Chunking System Ready!\n",
      "üéØ Updated Features:\n",
      "   ‚Ä¢ Chunked files: chunked_model_output/model_name/filename/1.txt, 2.txt, etc.\n",
      "   ‚Ä¢ Single files: model_output/model_name/filename.txt (CLEAN STRUCTURE)\n",
      "   ‚Ä¢ Clean folder organization for both chunked and single files\n",
      "   ‚Ä¢ Simple filenames without strategy suffixes\n",
      "üìÅ New model_output structure: model_output/model_name/filename.txt\n"
     ]
    }
   ],
   "source": [
    "# NEW ORGANIZED CHUNKING SYSTEM\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def migrate_file_chunked(filename: str, original_code: str, model_name: str, strategy: str, api_key: str, chunk_size: int):\n",
    "    \"\"\"Migrate a large file using NEW organized chunking approach.\"\"\"\n",
    "    chunks = chunk_code(original_code, chunk_size)\n",
    "    total_chunks = len(chunks)\n",
    "    \n",
    "    print(f\" Split into {total_chunks} chunks of ~{chunk_size} lines each\")\n",
    "    \n",
    "    # Create organized folder structure\n",
    "    chunked_output_dir = Path('chunked_model_output')\n",
    "    chunked_output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Create model folder\n",
    "    model_short = model_name.split('/')[-1].replace('-', '_').replace(':', '_').replace('.', '_').lower()\n",
    "    model_dir = chunked_output_dir / model_short\n",
    "    model_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Create file folder (remove .php extension for folder name)\n",
    "    file_base = filename.replace('.php', '')\n",
    "    file_dir = model_dir / file_base\n",
    "    file_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(f\"üìÅ Saving chunks to: {file_dir}\")\n",
    "    \n",
    "    # Use chunking strategy\n",
    "    chunk_strategy = f\"chunk_{strategy}\" if not strategy.startswith('chunk_') else strategy\n",
    "    \n",
    "    all_responses = []\n",
    "    \n",
    "    for i, chunk_info in enumerate(chunks, 1):\n",
    "        print(f\"\\n[Chunk {i}/{total_chunks}] Processing lines {chunk_info['start_line']}-{chunk_info['end_line']}...\")\n",
    "        \n",
    "        # Create prompt with chunking context\n",
    "        prompt = create_prompt(\n",
    "            chunk_info['code'], \n",
    "            chunk_strategy,\n",
    "            filename=filename,\n",
    "            start_line=chunk_info['start_line'],\n",
    "            end_line=chunk_info['end_line'],\n",
    "            total_lines=chunk_info['total_lines'],\n",
    "            chunk_number=i,\n",
    "            total_chunks=total_chunks\n",
    "        )\n",
    "        \n",
    "        print(f\"üìè Chunk prompt length: {len(prompt):,} characters\")\n",
    "        \n",
    "        # Make API call for this chunk\n",
    "        chunk_filename = f\"{filename}_chunk_{i}\"\n",
    "        response = make_api_call_chunked(chunk_filename, prompt, model_name, api_key, chunk_strategy, file_dir, i)\n",
    "        \n",
    "        if response is None:\n",
    "            print(f\"‚ùå Failed to process chunk {i}\")\n",
    "            all_responses.append(None)\n",
    "        else:\n",
    "            all_responses.append(response)\n",
    "            print(f\"‚úÖ Chunk {i} processed successfully\")\n",
    "    \n",
    "    # Count successful chunks\n",
    "    successful_chunks = sum(1 for r in all_responses if r is not None)\n",
    "    print(f\"\\nüéâ Chunked migration completed!\")\n",
    "    print(f\"‚úÖ Successful chunks: {successful_chunks}/{total_chunks}\")\n",
    "    print(f\"üìÅ All chunks saved in: {file_dir}\")\n",
    "    \n",
    "    return all_responses\n",
    "\n",
    "def make_api_call_chunked(filename: str, prompt: str, model_name: str, api_key: str, strategy: str, file_dir: Path, chunk_number: int):\n",
    "    \"\"\"Make API call and save to organized chunk structure.\"\"\"\n",
    "    \n",
    "    # Direct API call to OpenRouter\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"HTTP-Referer\": \"https://github.com/research-project\",\n",
    "        \"X-Title\": \"LLM PHP Migration Research\"\n",
    "    }\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"max_tokens\": 80000,\n",
    "        \"temperature\": 0.3\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(f\"üîó Making API call to OpenRouter...\")\n",
    "        response = requests.post(\n",
    "            \"https://openrouter.ai/api/v1/chat/completions\",\n",
    "            headers=headers,\n",
    "            data=json.dumps(payload),\n",
    "            timeout=300\n",
    "        )\n",
    "        \n",
    "        print(f\"üìä HTTP Status: {response.status_code}\")\n",
    "        print(f\"üìè Response length: {len(response.text)} characters\")\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå API Error: {response.status_code}\")\n",
    "            print(f\"üìÑ Response: {response.text[:1000]}...\")\n",
    "            return None\n",
    "        \n",
    "        # Parse response\n",
    "        try:\n",
    "            result = response.json()\n",
    "            raw_response = result['choices'][0]['message']['content']\n",
    "            \n",
    "            if not raw_response or len(raw_response.strip()) < 10:\n",
    "                print(f\"‚ùå Model response is empty or too short\")\n",
    "                return None\n",
    "                \n",
    "        except (json.JSONDecodeError, KeyError, IndexError) as e:\n",
    "            print(f\"‚ùå Response parsing error: {e}\")\n",
    "            return None\n",
    "        \n",
    "        # Save to organized structure - just the chunk number as filename\n",
    "        chunk_file = file_dir / f\"{chunk_number}.txt\"\n",
    "        \n",
    "        with open(chunk_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"=== RAW MODEL RESPONSE ===\\n\")\n",
    "            f.write(f\"File: {filename}\\n\")\n",
    "            f.write(f\"Model: {model_name}\\n\")\n",
    "            f.write(f\"Strategy: {strategy}\\n\")\n",
    "            f.write(f\"Chunk: {chunk_number}\\n\")\n",
    "            f.write(f\"Length: {len(raw_response)} characters\\n\")\n",
    "            f.write(f\"Timestamp: {datetime.now()}\\n\")\n",
    "            f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "            f.write(raw_response)\n",
    "        \n",
    "        print(f\"‚úÖ Chunk saved to: {chunk_file}\")\n",
    "        return raw_response\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"‚ùå Request error: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Unexpected error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Keep the existing functions for single files\n",
    "def migrate_file(filename: str, model_name: str, strategy: str = \"basic\", api_key: str = OPENROUTER_API_KEY, \n",
    "                chunk_size: int = None, auto_chunk: bool = True):\n",
    "    \"\"\"Enhanced migration function with NEW organized chunking.\"\"\"\n",
    "    \n",
    "    if chunk_size is None:\n",
    "        chunk_size = DEFAULT_CHUNK_SIZE\n",
    "    \n",
    "    if filename not in test_files:\n",
    "        print(f\"‚ùå File '{filename}' not found\")\n",
    "        return None\n",
    "    \n",
    "    original_code = test_files[filename]\n",
    "    line_count = len(original_code.split('\\n'))\n",
    "    \n",
    "    print(f\"üöÄ Migrating {filename} using {model_name} with {strategy} strategy...\")\n",
    "    print(f\"üìè Input code length: {len(original_code):,} characters ({line_count:,} lines)\")\n",
    "    \n",
    "    # Decide whether to chunk\n",
    "    should_chunk = auto_chunk and line_count > chunk_size\n",
    "    \n",
    "    if should_chunk:\n",
    "        print(f\"üì¶ Large file detected ({line_count} lines) - using NEW organized chunking\")\n",
    "        return migrate_file_chunked(filename, original_code, model_name, strategy, api_key, chunk_size)\n",
    "    else:\n",
    "        print(f\"üìÑ Processing as single file ({line_count} lines, chunk limit: {chunk_size})\")\n",
    "        return migrate_file_single(filename, original_code, model_name, strategy, api_key)\n",
    "\n",
    "def migrate_file_single(filename: str, original_code: str, model_name: str, strategy: str, api_key: str):\n",
    "    \"\"\"Migrate a single file without chunking - saves to regular model_output.\"\"\"\n",
    "    prompt = create_prompt(original_code, strategy)\n",
    "    print(f\"üìè Prompt length: {len(prompt):,} characters\")\n",
    "    \n",
    "    return make_api_call(filename, prompt, model_name, api_key, strategy)\n",
    "\n",
    "def make_api_call(filename: str, prompt: str, model_name: str, api_key: str, strategy: str):\n",
    "    \"\"\"Make API call for single files - saves to model_output directory.\"\"\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"HTTP-Referer\": \"https://github.com/research-project\",\n",
    "        \"X-Title\": \"LLM PHP Migration Research\"\n",
    "    }\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"max_tokens\": 80000,\n",
    "        \"temperature\": 0.3\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            \"https://openrouter.ai/api/v1/chat/completions\",\n",
    "            headers=headers,\n",
    "            data=json.dumps(payload),\n",
    "            timeout=300\n",
    "        )\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå API Error: {response.status_code}\")\n",
    "            return None\n",
    "        \n",
    "        result = response.json()\n",
    "        raw_response = result['choices'][0]['message']['content']\n",
    "        \n",
    "        # Save to organized model_output structure with simple filenames\n",
    "        output_dir = Path('model_output')\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Create model subfolder\n",
    "        model_short = model_name.split('/')[-1].replace('-', '_').replace(':', '_').replace('.', '_').lower()\n",
    "        model_folder = output_dir / model_short\n",
    "        model_folder.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Simple filename - just the base name with .txt extension\n",
    "        base_name = filename.replace('.php', '')\n",
    "        output_file = model_folder / f\"{base_name}.txt\"\n",
    "        \n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"=== RAW MODEL RESPONSE ===\\n\")\n",
    "            f.write(f\"File: {filename}\\n\")\n",
    "            f.write(f\"Model: {model_name}\\n\")\n",
    "            f.write(f\"Strategy: {strategy}\\n\")\n",
    "            f.write(f\"Length: {len(raw_response)} characters\\n\")\n",
    "            f.write(f\"Timestamp: {datetime.now()}\\n\")\n",
    "            f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "            f.write(raw_response)\n",
    "        \n",
    "        print(f\"‚úÖ Response saved to: {output_file}\")\n",
    "        return raw_response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"‚úÖ NEW Organized Chunking System Ready!\")\n",
    "print(\"üéØ Updated Features:\")\n",
    "print(\"   ‚Ä¢ Chunked files: chunked_model_output/model_name/filename/1.txt, 2.txt, etc.\")\n",
    "print(\"   ‚Ä¢ Single files: model_output/model_name/filename.txt (CLEAN STRUCTURE)\")\n",
    "print(\"   ‚Ä¢ Clean folder organization for both chunked and single files\")\n",
    "print(\"   ‚Ä¢ Simple filenames without strategy suffixes\")\n",
    "print(\"üìÅ New model_output structure: model_output/model_name/filename.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Loaded 12 PHP files from selected_100_files:\n",
      "   üìÑ 001_getid3.lib.php (43,494 chars)\n",
      "   üìÑ 002_module.audio-video.asf.php (129,115 chars)\n",
      "   üìÑ 003_wp-db.php (60,679 chars)\n",
      "   üìÑ 004_class-IXR.php (32,854 chars)\n",
      "   üìÑ 005_class-snoopy.php (37,776 chars)\n",
      "   üìÑ 006_widgets.php (47,668 chars)\n",
      "   üìÑ 009_getid3.php (62,683 chars)\n",
      "   üìÑ 010_class-wp-theme.php (39,449 chars)\n",
      "   üìÑ 012_module.audio-video.riff.php (111,145 chars)\n",
      "   üìÑ 013_file.php (45,191 chars)\n",
      "   üìÑ 014_module.tag.id3v2.php (134,159 chars)\n",
      "   üìÑ 057_class-wp-customize-manager.php (32,639 chars)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if old_version_path.exists():\n",
    "    # Recursively find all PHP files in all subfolders\n",
    "    for php_file in old_version_path.rglob('*.php'):\n",
    "        try:\n",
    "            with open(php_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                content = f.read()\n",
    "                if content.strip():\n",
    "                    test_files[php_file.name] = content\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Could not load {php_file.name}: {e}\")\n",
    "    \n",
    "    print(f\"üìÅ Loaded {len(test_files)} PHP files from selected_100_files:\")\n",
    "    for filename in sorted(test_files.keys()):\n",
    "        size = len(test_files[filename])\n",
    "        print(f\"   üìÑ {filename} ({size:,} chars)\")\n",
    "else:\n",
    "    print(\"‚ùå selected_100_files directory not found\")\n",
    "    print(\"üí° Make sure the selected 100 files are in 'selected_100_files/' directory\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Enhanced helper functions with chunk browsing ready!\n",
      "üí° New chunk inspection features:\n",
      "   browse_saved_chunks() - Browse all saved chunks\n",
      "   browse_saved_chunks('model/name') - Browse chunks for specific model\n",
      "   inspect_chunk('model/name', 'filename.php') - Inspect all chunks for a file\n",
      "   inspect_chunk('model/name', 'filename.php', 2) - Inspect specific chunk number\n",
      "   Default chunk size: 500 lines (configurable)\n",
      "   auto_chunk parameter - Enable/disable automatic chunking\n",
      "\n",
      "üí° Usage examples:\n",
      "   migrate_file('large_file.php', 'qwen/qwen3-coder:free', 'basic', chunk_size=800)\n",
      "   quick_migrate('large_file.php', auto_chunk=False)  # Force single file\n",
      "   batch_migrate(['file1.php', 'file2.php'], chunk_size=1200)\n",
      "   browse_saved_chunks('deepseek/deepseek-chat-v3.1:free')  # Browse specific model chunks\n"
     ]
    }
   ],
   "source": [
    "# HELPER FUNCTIONS WITH CHUNKING SUPPORT\n",
    "def quick_migrate(filename: str, model: str = \"anthropic/claude-3.5-sonnet\", strategy: str = \"basic\", \n",
    "                 chunk_size: int = None, auto_chunk: bool = True):\n",
    "    \"\"\"Quick migration with defaults and chunking support.\"\"\"\n",
    "    if chunk_size is None:\n",
    "        chunk_size = DEFAULT_CHUNK_SIZE\n",
    "    return migrate_file(filename, model, strategy, chunk_size=chunk_size, auto_chunk=auto_chunk)\n",
    "\n",
    "def batch_migrate(filenames: list, model: str = \"anthropic/claude-3.5-sonnet\", strategy: str = \"basic\", \n",
    "                 chunk_size: int = None, auto_chunk: bool = True):\n",
    "    \"\"\"Migrate multiple files with chunking support.\"\"\"\n",
    "    if chunk_size is None:\n",
    "        chunk_size = DEFAULT_CHUNK_SIZE\n",
    "        \n",
    "    print(f\"üîÑ Batch migrating {len(filenames)} files...\")\n",
    "    if auto_chunk:\n",
    "        print(f\"üì¶ Auto-chunking enabled for files > {chunk_size} lines\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, filename in enumerate(filenames, 1):\n",
    "        print(f\"\\n[{i}/{len(filenames)}] Processing {filename}...\")\n",
    "        result = migrate_file(filename, model, strategy, chunk_size=chunk_size, auto_chunk=auto_chunk)\n",
    "        results.append(result)\n",
    "    \n",
    "    # Count results (for chunked files, result is a list)\n",
    "    total_files = len(filenames)\n",
    "    successful_files = 0\n",
    "    total_chunks = 0\n",
    "    successful_chunks = 0\n",
    "    \n",
    "    for result in results:\n",
    "        if result is not None:\n",
    "            if isinstance(result, list):  # Chunked file\n",
    "                total_chunks += len(result)\n",
    "                successful_chunks += sum(1 for r in result if r is not None)\n",
    "                if any(r is not None for r in result):  # At least one chunk succeeded\n",
    "                    successful_files += 1\n",
    "            else:  # Single file\n",
    "                successful_files += 1\n",
    "                total_chunks += 1\n",
    "                successful_chunks += 1\n",
    "    \n",
    "    print(f\"\\nüéâ Batch migration completed!\")\n",
    "    print(f\"‚úÖ Successful files: {successful_files}/{total_files}\")\n",
    "    if total_chunks > len(filenames):\n",
    "        print(f\"üì¶ Total chunks processed: {successful_chunks}/{total_chunks}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_file_sizes(chunk_threshold: int = None):\n",
    "    \"\"\"Analyze file sizes to see which ones would be chunked.\"\"\"\n",
    "    if chunk_threshold is None:\n",
    "        chunk_threshold = DEFAULT_CHUNK_SIZE\n",
    "        \n",
    "    if not test_files:\n",
    "        print(\"‚ùå No test files loaded\")\n",
    "        return\n",
    "    \n",
    "    print(\"üìä File Size Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    small_files = []\n",
    "    large_files = []\n",
    "    \n",
    "    for filename, content in test_files.items():\n",
    "        line_count = len(content.split('\\n'))\n",
    "        char_count = len(content)\n",
    "        \n",
    "        if line_count <= chunk_threshold:\n",
    "            small_files.append((filename, line_count, char_count))\n",
    "        else:\n",
    "            large_files.append((filename, line_count, char_count))\n",
    "    \n",
    "    print(f\"üìÑ Small files (‚â§{chunk_threshold} lines): {len(small_files)}\")\n",
    "    for filename, lines, chars in sorted(small_files, key=lambda x: x[1], reverse=True)[:10]:\n",
    "        print(f\"   {filename}: {lines:,} lines, {chars:,} chars\")\n",
    "    \n",
    "    if len(small_files) > 10:\n",
    "        print(f\"   ... and {len(small_files) - 10} more\")\n",
    "    \n",
    "    print(f\"\\nüì¶ Large files (>{chunk_threshold} lines): {len(large_files)}\")\n",
    "    for filename, lines, chars in sorted(large_files, key=lambda x: x[1], reverse=True):\n",
    "        chunks_needed = (lines + chunk_threshold - 1) // chunk_threshold  # Ceiling division\n",
    "        print(f\"   {filename}: {lines:,} lines, {chars:,} chars ‚Üí {chunks_needed} chunks\")\n",
    "    \n",
    "    if large_files:\n",
    "        total_large_lines = sum(lines for _, lines, _ in large_files)\n",
    "        total_chunks_needed = sum((lines + chunk_threshold - 1) // chunk_threshold for _, lines, _ in large_files)\n",
    "        print(f\"\\nüìä Summary for large files:\")\n",
    "        print(f\"   Total lines: {total_large_lines:,}\")\n",
    "        print(f\"   Total chunks needed: {total_chunks_needed}\")\n",
    "\n",
    "def browse_saved_chunks(model_name: str = None):\n",
    "    \"\"\"Browse and analyze saved chunks in the chunks_sent_to_model directory.\"\"\"\n",
    "    chunks_dir = Path('chunks_sent_to_model')\n",
    "    \n",
    "    if not chunks_dir.exists():\n",
    "        print(\"‚ùå No chunks directory found. Run some migrations first!\")\n",
    "        return\n",
    "    \n",
    "    print(\"üìÅ Browsing saved chunks...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    model_dirs = list(chunks_dir.iterdir())\n",
    "    if not model_dirs:\n",
    "        print(\"‚ùå No model directories found in chunks_sent_to_model/\")\n",
    "        return\n",
    "    \n",
    "    # Filter by model if specified\n",
    "    if model_name:\n",
    "        model_short = model_name.split('/')[-1].replace('-', '_').replace(':', '_').replace('.', '_').lower()\n",
    "        model_dirs = [d for d in model_dirs if d.name == model_short]\n",
    "        if not model_dirs:\n",
    "            print(f\"‚ùå No chunks found for model: {model_name}\")\n",
    "            print(f\"Available models: {[d.name for d in chunks_dir.iterdir() if d.is_dir()]}\")\n",
    "            return\n",
    "    \n",
    "    for model_dir in sorted(model_dirs):\n",
    "        if not model_dir.is_dir():\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nüìÇ Model: {model_dir.name}\")\n",
    "        \n",
    "        # Group files by base name\n",
    "        files_by_base = {}\n",
    "        for file in model_dir.iterdir():\n",
    "            if file.is_file():\n",
    "                # Extract base name\n",
    "                if '_chunk_' in file.stem:\n",
    "                    base_name = file.stem.split('_chunk_')[0]\n",
    "                elif '_single_file_' in file.stem:\n",
    "                    base_name = file.stem.split('_single_file_')[0]\n",
    "                else:\n",
    "                    base_name = file.stem\n",
    "                \n",
    "                if base_name not in files_by_base:\n",
    "                    files_by_base[base_name] = {'chunks': [], 'single': [], 'other': []}\n",
    "                \n",
    "                if '_chunk_' in file.stem:\n",
    "                    files_by_base[base_name]['chunks'].append(file)\n",
    "                elif '_single_file_' in file.stem:\n",
    "                    files_by_base[base_name]['single'].append(file)\n",
    "                else:\n",
    "                    files_by_base[base_name]['other'].append(file)\n",
    "        \n",
    "        # Display grouped files\n",
    "        for base_name, file_groups in sorted(files_by_base.items()):\n",
    "            chunks = file_groups['chunks']\n",
    "            single = file_groups['single']\n",
    "            \n",
    "            if chunks:\n",
    "                # Count unique chunks (divide by 3 since each chunk has 3 files: code, prompt, metadata)\n",
    "                unique_chunks = len([f for f in chunks if f.name.endswith('_metadata.json')])\n",
    "                print(f\"   üì¶ {base_name}.php - {unique_chunks} chunks\")\n",
    "                \n",
    "                # Show chunk details\n",
    "                for chunk_file in sorted([f for f in chunks if f.name.endswith('_metadata.json')]):\n",
    "                    try:\n",
    "                        with open(chunk_file, 'r') as f:\n",
    "                            metadata = json.load(f)\n",
    "                        chunk_num = metadata.get('chunk_number', 'unknown')\n",
    "                        total_chunks = metadata.get('total_chunks', 'unknown')\n",
    "                        lines = f\"{metadata.get('start_line', '?')}-{metadata.get('end_line', '?')}\"\n",
    "                        code_len = metadata.get('code_length', 0)\n",
    "                        prompt_len = metadata.get('prompt_length', 0)\n",
    "                        print(f\"      üìÑ Chunk {chunk_num}/{total_chunks}: lines {lines}, {code_len:,} chars code, {prompt_len:,} chars prompt\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"      ‚ùå Error reading {chunk_file.name}: {e}\")\n",
    "            \n",
    "            elif single:\n",
    "                print(f\"   üìÑ {base_name}.php - single file (no chunking)\")\n",
    "\n",
    "def inspect_chunk(model_name: str, filename: str, chunk_number: int = None):\n",
    "    \"\"\"Inspect a specific chunk's data in detail.\"\"\"\n",
    "    chunks_dir = Path('chunks_sent_to_model')\n",
    "    model_short = model_name.split('/')[-1].replace('-', '_').replace(':', '_').replace('.', '_').lower()\n",
    "    model_dir = chunks_dir / model_short\n",
    "    \n",
    "    if not model_dir.exists():\n",
    "        print(f\"‚ùå No chunks found for model: {model_name}\")\n",
    "        return\n",
    "    \n",
    "    base_name = filename.replace('.php', '')\n",
    "    \n",
    "    # If chunk_number specified, look for that specific chunk\n",
    "    if chunk_number is not None:\n",
    "        pattern = f\"{base_name}_chunk_{chunk_number:03d}_of_*_metadata.json\"\n",
    "        metadata_files = list(model_dir.glob(pattern))\n",
    "        \n",
    "        if not metadata_files:\n",
    "            print(f\"‚ùå Chunk {chunk_number} not found for {filename}\")\n",
    "            return\n",
    "        \n",
    "        metadata_file = metadata_files[0]\n",
    "    else:\n",
    "        # Look for single file first\n",
    "        single_pattern = f\"{base_name}_single_file_code.php\"\n",
    "        single_files = list(model_dir.glob(single_pattern))\n",
    "        \n",
    "        if single_files:\n",
    "            print(f\"üìÑ Inspecting single file: {filename}\")\n",
    "            # Show single file data\n",
    "            code_file = model_dir / f\"{base_name}_single_file_code.php\"\n",
    "            prompt_file = model_dir / f\"{base_name}_single_file_prompt.txt\"\n",
    "            \n",
    "            if code_file.exists():\n",
    "                print(f\"üìÇ Code file: {code_file}\")\n",
    "                with open(code_file, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                    print(f\"   üìè Size: {len(content):,} characters\")\n",
    "                    # Show first few lines\n",
    "                    lines = content.split('\\n')[:15]\n",
    "                    print(\"   üìÑ First 15 lines:\")\n",
    "                    for i, line in enumerate(lines, 1):\n",
    "                        print(f\"      {i:2d}: {line[:100]}{'...' if len(line) > 100 else ''}\")\n",
    "            \n",
    "            if prompt_file.exists():\n",
    "                print(f\"\\nüìù Prompt file: {prompt_file}\")\n",
    "                with open(prompt_file, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                    print(f\"   üìè Size: {len(content):,} characters\")\n",
    "            return\n",
    "        \n",
    "        # Look for chunks\n",
    "        chunk_pattern = f\"{base_name}_chunk_*_metadata.json\"\n",
    "        metadata_files = list(model_dir.glob(chunk_pattern))\n",
    "        \n",
    "        if not metadata_files:\n",
    "            print(f\"‚ùå No chunks or single file found for {filename}\")\n",
    "            return\n",
    "        \n",
    "        # Show all chunks\n",
    "        print(f\"üì¶ Found {len(metadata_files)} chunks for {filename}\")\n",
    "        for metadata_file in sorted(metadata_files):\n",
    "            with open(metadata_file, 'r') as f:\n",
    "                metadata = json.load(f)\n",
    "            chunk_num = metadata.get('chunk_number', 'unknown')\n",
    "            print(f\"   üìÑ Chunk {chunk_num}\")\n",
    "        \n",
    "        # Ask to inspect first chunk by default\n",
    "        metadata_file = sorted(metadata_files)[0]\n",
    "        with open(metadata_file, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        chunk_number = metadata.get('chunk_number', 1)\n",
    "    \n",
    "    # Load and display chunk data\n",
    "    try:\n",
    "        with open(metadata_file, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        print(f\"\\nüìä Chunk {metadata['chunk_number']} of {metadata['total_chunks']} - {filename}\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"üìè Lines: {metadata['start_line']} to {metadata['end_line']} (of {metadata['total_lines']} total)\")\n",
    "        print(f\"üìÑ Chunk lines: {metadata['chunk_lines']}\")\n",
    "        print(f\"üíª Code length: {metadata['code_length']:,} characters\")\n",
    "        print(f\"üìù Prompt length: {metadata['prompt_length']:,} characters\")\n",
    "        print(f\"‚è∞ Timestamp: {metadata['timestamp']}\")\n",
    "        \n",
    "        # Show corresponding files\n",
    "        base_pattern = metadata_file.stem.replace('_metadata', '')\n",
    "        code_file = model_dir / f\"{base_pattern}_code.php\"\n",
    "        prompt_file = model_dir / f\"{base_pattern}_prompt.txt\"\n",
    "        \n",
    "        if code_file.exists():\n",
    "            print(f\"\\nüìÇ Code file: {code_file}\")\n",
    "            with open(code_file, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "                # Show first few lines after the header comment\n",
    "                lines = content.split('\\n')\n",
    "                start_idx = 0\n",
    "                for i, line in enumerate(lines):\n",
    "                    if line.strip() == '*/' or (line.strip().startswith('*/') and len(line.strip()) == 2):\n",
    "                        start_idx = i + 1\n",
    "                        break\n",
    "                \n",
    "                relevant_lines = lines[start_idx:start_idx + 15]\n",
    "                print(\"   üìÑ First 15 lines of actual code:\")\n",
    "                for i, line in enumerate(relevant_lines, start_idx + 1):\n",
    "                    print(f\"      {i:2d}: {line[:100]}{'...' if len(line) > 100 else ''}\")\n",
    "        \n",
    "        if prompt_file.exists():\n",
    "            print(f\"\\nüìù Prompt file: {prompt_file}\")\n",
    "            print(\"   üí° Use 'with open(prompt_file) as f: print(f.read())' to see full prompt\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error inspecting chunk: {e}\")\n",
    "\n",
    "print(\"‚úÖ Enhanced helper functions with chunk browsing ready!\")\n",
    "print(\"üí° New chunk inspection features:\")\n",
    "print(\"   browse_saved_chunks() - Browse all saved chunks\")\n",
    "print(\"   browse_saved_chunks('model/name') - Browse chunks for specific model\")\n",
    "print(\"   inspect_chunk('model/name', 'filename.php') - Inspect all chunks for a file\")\n",
    "print(\"   inspect_chunk('model/name', 'filename.php', 2) - Inspect specific chunk number\")\n",
    "print(f\"   Default chunk size: {DEFAULT_CHUNK_SIZE} lines (configurable)\")\n",
    "print(\"   auto_chunk parameter - Enable/disable automatic chunking\")\n",
    "print(\"\\nüí° Usage examples:\")\n",
    "print(\"   migrate_file('large_file.php', 'qwen/qwen3-coder:free', 'basic', chunk_size=800)\")\n",
    "print(\"   quick_migrate('large_file.php', auto_chunk=False)  # Force single file\")\n",
    "print(\"   batch_migrate(['file1.php', 'file2.php'], chunk_size=1200)\")\n",
    "print(\"   browse_saved_chunks('deepseek/deepseek-chat-v3.1:free')  # Browse specific model chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Analyzing loaded files to see chunking requirements...\n",
      "üìä File Size Analysis\n",
      "==================================================\n",
      "üìÑ Small files (‚â§500 lines): 0\n",
      "\n",
      "üì¶ Large files (>500 lines): 12\n",
      "   014_module.tag.id3v2.php: 3,415 lines, 134,159 chars ‚Üí 7 chunks\n",
      "   012_module.audio-video.riff.php: 2,436 lines, 111,145 chars ‚Üí 5 chunks\n",
      "   003_wp-db.php: 2,187 lines, 60,679 chars ‚Üí 5 chunks\n",
      "   002_module.audio-video.asf.php: 2,020 lines, 129,115 chars ‚Üí 5 chunks\n",
      "   009_getid3.php: 1,776 lines, 62,683 chars ‚Üí 4 chunks\n",
      "   006_widgets.php: 1,515 lines, 47,668 chars ‚Üí 4 chunks\n",
      "   001_getid3.lib.php: 1,342 lines, 43,494 chars ‚Üí 3 chunks\n",
      "   057_class-wp-customize-manager.php: 1,273 lines, 32,639 chars ‚Üí 3 chunks\n",
      "   005_class-snoopy.php: 1,257 lines, 37,776 chars ‚Üí 3 chunks\n",
      "   010_class-wp-theme.php: 1,236 lines, 39,449 chars ‚Üí 3 chunks\n",
      "   013_file.php: 1,151 lines, 45,191 chars ‚Üí 3 chunks\n",
      "   004_class-IXR.php: 1,101 lines, 32,854 chars ‚Üí 3 chunks\n",
      "\n",
      "üìä Summary for large files:\n",
      "   Total lines: 20,709\n",
      "   Total chunks needed: 48\n",
      "\n",
      "üß™ Testing chunking logic on sample files...\n",
      "\n",
      "üìÑ 001_getid3.lib.php:\n",
      "   Lines: 1,342\n",
      "   Chunks: 3\n",
      "   Chunk breakdown:\n",
      "     Chunk 1: lines 1-500 (500 lines)\n",
      "     Chunk 2: lines 501-1000 (500 lines)\n",
      "     Chunk 3: lines 1001-1342 (342 lines)\n",
      "\n",
      "üìÑ 002_module.audio-video.asf.php:\n",
      "   Lines: 2,020\n",
      "   Chunks: 5\n",
      "   Chunk breakdown:\n",
      "     Chunk 1: lines 1-500 (500 lines)\n",
      "     Chunk 2: lines 501-1000 (500 lines)\n",
      "     Chunk 3: lines 1001-1500 (500 lines)\n",
      "     Chunk 4: lines 1501-2000 (500 lines)\n",
      "     Chunk 5: lines 2001-2020 (20 lines)\n",
      "\n",
      "üìÑ 003_wp-db.php:\n",
      "   Lines: 2,187\n",
      "   Chunks: 5\n",
      "   Chunk breakdown:\n",
      "     Chunk 1: lines 1-500 (500 lines)\n",
      "     Chunk 2: lines 501-1000 (500 lines)\n",
      "     Chunk 3: lines 1001-1500 (500 lines)\n",
      "     Chunk 4: lines 1501-2000 (500 lines)\n",
      "     Chunk 5: lines 2001-2187 (187 lines)\n",
      "\n",
      "‚úÖ File analysis complete!\n"
     ]
    }
   ],
   "source": [
    "# ANALYZE YOUR FILES FOR CHUNKING\n",
    "print(\"üîç Analyzing loaded files to see chunking requirements...\")\n",
    "analyze_file_sizes()\n",
    "\n",
    "# Test chunking on a sample file\n",
    "print(\"\\nüß™ Testing chunking logic on sample files...\")\n",
    "for filename in list(test_files.keys())[:3]:\n",
    "    code = test_files[filename]\n",
    "    line_count = len(code.split('\\n'))\n",
    "    chunks = chunk_code(code, DEFAULT_CHUNK_SIZE)\n",
    "    \n",
    "    print(f\"\\nüìÑ {filename}:\")\n",
    "    print(f\"   Lines: {line_count:,}\")\n",
    "    print(f\"   Chunks: {len(chunks)}\")\n",
    "    \n",
    "    if len(chunks) > 1:\n",
    "        print(f\"   Chunk breakdown:\")\n",
    "        for i, chunk_info in enumerate(chunks, 1):\n",
    "            print(f\"     Chunk {i}: lines {chunk_info['start_line']}-{chunk_info['end_line']} ({chunk_info['end_line'] - chunk_info['start_line'] + 1} lines)\")\n",
    "\n",
    "print(f\"\\n‚úÖ File analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ SIMPLE MIGRATION TOOL READY!\n",
      "========================================\n",
      "üìÅ Available files: 12 loaded\n",
      "üìã Sample files: ['001_getid3.lib.php', '002_module.audio-video.asf.php', '003_wp-db.php', '004_class-IXR.php', '005_class-snoopy.php']\n",
      "\n",
      "üí° Usage examples:\n",
      "   migrate_file('014_module.tag.id3v2.php', 'qwen/qwen3-coder:free', 'basic')\n",
      "   quick_migrate('014_module.tag.id3v2.php')\n",
      "   batch_migrate(['file1.php', 'file2.php'], 'meta-llama/llama-3.1-8b-instruct')\n",
      "\n",
      "üìÅ Raw responses will be saved to model_output/ folder\n",
      "üî¨ Examine the raw model responses to see what they return!\n"
     ]
    }
   ],
   "source": [
    "# READY TO USE!\n",
    "print(\"üéâ SIMPLE MIGRATION TOOL READY!\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if test_files:\n",
    "    print(f\"üìÅ Available files: {len(test_files)} loaded\")\n",
    "    print(f\"üìã Sample files: {list(test_files.keys())[:5]}\")\n",
    "    \n",
    "    print(f\"\\nüí° Usage examples:\")\n",
    "    print(\"   migrate_file('014_module.tag.id3v2.php', 'qwen/qwen3-coder:free', 'basic')\")\n",
    "    print(\"   quick_migrate('014_module.tag.id3v2.php')\")\n",
    "    print(\"   batch_migrate(['file1.php', 'file2.php'], 'meta-llama/llama-3.1-8b-instruct')\")\n",
    "    \n",
    "    print(f\"\\nüìÅ Raw responses will be saved to model_output/ folder\")\n",
    "    print(\"üî¨ Examine the raw model responses to see what they return!\")\n",
    "else:\n",
    "    print(\"‚ùå No test files loaded - check selected_100_files directory\")\n",
    "    print(\"üí° Make sure the selected 100 files are in 'selected_100_files/' directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç CHUNK SAVING DEMONSTRATION\n",
      "==================================================\n",
      "üìÅ No chunks directory found yet - will be created when you run migrations\n",
      "\n",
      "üéØ What gets saved for each chunk:\n",
      "   üìÑ *_code.php - The actual PHP code chunk sent to the model\n",
      "   üìù *_prompt.txt - The complete prompt including context and instructions\n",
      "   üìä *_metadata.json - Chunk metadata (line numbers, lengths, timestamps)\n",
      "\n",
      "üîß For single files (no chunking):\n",
      "   üìÑ *_single_file_code.php - The complete file code\n",
      "   üìù *_single_file_prompt.txt - The complete prompt\n",
      "\n",
      "üí° This allows you to:\n",
      "   ‚úÖ See exactly what code was sent to each model\n",
      "   ‚úÖ Compare prompts across different strategies\n",
      "   ‚úÖ Debug chunking behavior\n",
      "   ‚úÖ Verify chunk boundaries and context\n",
      "   ‚úÖ Analyze prompt effectiveness\n"
     ]
    }
   ],
   "source": [
    "# DEMONSTRATE CHUNK SAVING MECHANISM\n",
    "print(\"üîç CHUNK SAVING DEMONSTRATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Show what happens when chunks are saved\n",
    "chunks_dir = Path('chunks_sent_to_model')\n",
    "if chunks_dir.exists():\n",
    "    print(f\"üìÅ Chunks directory exists: {chunks_dir}\")\n",
    "    \n",
    "    # Browse existing chunks\n",
    "    print(\"\\nüìä Current saved chunks:\")\n",
    "    browse_saved_chunks()\n",
    "    \n",
    "    print(\"\\nüí° To inspect a specific chunk:\")\n",
    "    print(\"   inspect_chunk('deepseek_deepseek_chat_v3_1_free', '007_class-wp.php', 1)\")\n",
    "    print(\"   inspect_chunk('deepseek_deepseek_chat_v3_1_free', 'some_single_file.php')\")\n",
    "    \n",
    "else:\n",
    "    print(\"üìÅ No chunks directory found yet - will be created when you run migrations\")\n",
    "\n",
    "print(\"\\nüéØ What gets saved for each chunk:\")\n",
    "print(\"   üìÑ *_code.php - The actual PHP code chunk sent to the model\")\n",
    "print(\"   üìù *_prompt.txt - The complete prompt including context and instructions\")\n",
    "print(\"   üìä *_metadata.json - Chunk metadata (line numbers, lengths, timestamps)\")\n",
    "\n",
    "print(\"\\nüîß For single files (no chunking):\")\n",
    "print(\"   üìÑ *_single_file_code.php - The complete file code\")\n",
    "print(\"   üìù *_single_file_prompt.txt - The complete prompt\")\n",
    "\n",
    "print(\"\\nüí° This allows you to:\")\n",
    "print(\"   ‚úÖ See exactly what code was sent to each model\")\n",
    "print(\"   ‚úÖ Compare prompts across different strategies\")\n",
    "print(\"   ‚úÖ Debug chunking behavior\")\n",
    "print(\"   ‚úÖ Verify chunk boundaries and context\")\n",
    "print(\"   ‚úÖ Analyze prompt effectiveness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Migrating 001_getid3.lib.php using mistralai/mistral-small-3.2-24b-instruct:free with basic strategy...\n",
      "üìè Input code length: 43,494 characters (1,342 lines)\n",
      "üì¶ Large file detected (1342 lines) - using NEW organized chunking\n",
      " Split into 3 chunks of ~500 lines each\n",
      "üìÅ Saving chunks to: chunked_model_output\\mistral_small_3_2_24b_instruct_free\\001_getid3.lib\n",
      "\n",
      "[Chunk 1/3] Processing lines 1-500...\n",
      "üìè Chunk prompt length: 17,179 characters\n",
      "üîó Making API call to OpenRouter...\n",
      "üìä HTTP Status: 429\n",
      "üìè Response length: 314 characters\n",
      "‚ùå API Error: 429\n",
      "üìÑ Response: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day\",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"50\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1756771200000\"},\"provider_name\":null}},\"user_id\":\"user_2xMT55snBKFgKi83Qy4yYtb4ain\"}...\n",
      "‚ùå Failed to process chunk 1\n",
      "\n",
      "[Chunk 2/3] Processing lines 501-1000...\n",
      "üìè Chunk prompt length: 18,129 characters\n",
      "üîó Making API call to OpenRouter...\n",
      "üìä HTTP Status: 429\n",
      "üìè Response length: 314 characters\n",
      "‚ùå API Error: 429\n",
      "üìÑ Response: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day\",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"50\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1756771200000\"},\"provider_name\":null}},\"user_id\":\"user_2xMT55snBKFgKi83Qy4yYtb4ain\"}...\n",
      "‚ùå Failed to process chunk 2\n",
      "\n",
      "[Chunk 3/3] Processing lines 1001-1342...\n",
      "üìè Chunk prompt length: 11,860 characters\n",
      "üîó Making API call to OpenRouter...\n",
      "üìä HTTP Status: 429\n",
      "üìè Response length: 314 characters\n",
      "‚ùå API Error: 429\n",
      "üìÑ Response: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day\",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"50\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1756771200000\"},\"provider_name\":null}},\"user_id\":\"user_2xMT55snBKFgKi83Qy4yYtb4ain\"}...\n",
      "‚ùå Failed to process chunk 3\n",
      "\n",
      "üéâ Chunked migration completed!\n",
      "‚úÖ Successful chunks: 0/3\n",
      "üìÅ All chunks saved in: chunked_model_output\\mistral_small_3_2_24b_instruct_free\\001_getid3.lib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try with a different model that's more reliable\n",
    "migrate_file('001_getid3.lib.php', 'mistralai/mistral-small-3.2-24b-instruct:free', 'basic')\n",
    "\n",
    "# UNCOMMENT THE LINE BELOW TO START BATCH MIGRATION\n",
    "# batch_migrate(list(test_files.keys()) , model='mistralai/mistral-small-3.2-24b-instruct:free', strategy='basic')\n",
    "\n",
    "# Or use a smaller test first:\n",
    "# batch_migrate(list(test_files.keys())[:5] , model='mistralai/mistral-small-3.2-24b-instruct:free', strategy='basic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplified output parser ready!\n",
      "Features:\n",
      "   ‚Ä¢ Simple single-file processing only\n",
      "   ‚Ä¢ Reliable MIGRATION_START/END marker extraction\n",
      "   ‚Ä¢ Clear error reporting\n",
      "   ‚Ä¢ No chunking complexity\n",
      "Usage: parser.process_all_responses()\n"
     ]
    }
   ],
   "source": [
    "# SIMPLIFIED OUTPUT PARSER - NO CHUNKING FOR NOW\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "class OutputParser:\n",
    "    \"\"\"Simple parser for model responses - single files only.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model_output_path = Path('model_output')\n",
    "        self.parsed_path = Path('new-version')\n",
    "        self.parsed_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    def extract_migrated_code(self, response_content: str) -> str:\n",
    "        \"\"\"Extract code between MIGRATION_START and MIGRATION_END markers.\"\"\"\n",
    "        # Look for migration markers (most reliable method)\n",
    "        start_pattern = r'//\\s*MIGRATION_START\\s*\\n'\n",
    "        end_pattern = r'\\n//\\s*MIGRATION_END'\n",
    "        \n",
    "        start_match = re.search(start_pattern, response_content, re.IGNORECASE)\n",
    "        end_match = re.search(end_pattern, response_content, re.IGNORECASE)\n",
    "        \n",
    "        if start_match and end_match:\n",
    "            start_pos = start_match.end()\n",
    "            end_pos = end_match.start()\n",
    "            migrated_code = response_content[start_pos:end_pos].strip()\n",
    "            return migrated_code\n",
    "        \n",
    "        # No markers found\n",
    "        return \"\"\n",
    "    \n",
    "    def extract_metadata(self, response_content: str) -> dict:\n",
    "        \"\"\"Extract metadata from response file header.\"\"\"\n",
    "        lines = response_content.split('\\n')\n",
    "        metadata = {}\n",
    "        \n",
    "        for line in lines[:15]:  # Check first 15 lines\n",
    "            if line.startswith('File:'):\n",
    "                metadata['original_file'] = line.split(':', 1)[1].strip()\n",
    "            elif line.startswith('Model:'):\n",
    "                metadata['model'] = line.split(':', 1)[1].strip()\n",
    "            elif line.startswith('Strategy:'):\n",
    "                metadata['strategy'] = line.split(':', 1)[1].strip()\n",
    "        \n",
    "        return metadata\n",
    "    \n",
    "    def parse_single_file(self, response_file: Path) -> dict:\n",
    "        \"\"\"Parse a single response file.\"\"\"\n",
    "        try:\n",
    "            print(f\"Processing {response_file.name}\")\n",
    "            \n",
    "            with open(response_file, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            # Extract metadata\n",
    "            metadata = self.extract_metadata(content)\n",
    "            if not metadata.get('original_file'):\n",
    "                print(f\"   ERROR: No original file found in metadata\")\n",
    "                return {'success': False}\n",
    "            \n",
    "            # Extract migrated code\n",
    "            migrated_code = self.extract_migrated_code(content)\n",
    "            if not migrated_code:\n",
    "                print(f\"   ERROR: No migrated code found between markers\")\n",
    "                return {'success': False}\n",
    "            \n",
    "            print(f\"   SUCCESS: Found {len(migrated_code)} chars of migrated code\")\n",
    "            return {\n",
    "                'success': True,\n",
    "                'metadata': metadata,\n",
    "                'migrated_code': migrated_code\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"   ERROR: {e}\")\n",
    "            return {'success': False, 'error': str(e)}\n",
    "    \n",
    "    def save_parsed_file(self, result: dict, response_filename: str, model_folder_name: str = None) -> bool:\n",
    "        \"\"\"Save parsed result to organized structure.\"\"\"\n",
    "        try:\n",
    "            metadata = result['metadata']\n",
    "            migrated_code = result['migrated_code']\n",
    "            \n",
    "            # Determine model folder name\n",
    "            if model_folder_name:\n",
    "                # Use provided model folder name (from new structure)\n",
    "                model_clean = model_folder_name\n",
    "            else:\n",
    "                # Extract from metadata (fallback for old structure)\n",
    "                model_name = metadata.get('model', 'unknown_model')\n",
    "                model_clean = model_name.replace('/', '_').replace('-', '_').replace(':', '_').replace('.', '_').lower()\n",
    "            \n",
    "            # Create model folder in new-version\n",
    "            model_folder = self.parsed_path / model_clean\n",
    "            model_folder.mkdir(exist_ok=True)\n",
    "            \n",
    "            # Determine output filename\n",
    "            original_filename = metadata.get('original_file')\n",
    "            if original_filename:\n",
    "                output_file = model_folder / original_filename\n",
    "            else:\n",
    "                # Fallback: derive from response filename\n",
    "                if response_filename.endswith('.txt'):\n",
    "                    php_filename = response_filename[:-4] + '.php'  # Replace .txt with .php\n",
    "                else:\n",
    "                    php_filename = response_filename + '.php'\n",
    "                output_file = model_folder / php_filename\n",
    "            \n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(migrated_code)\n",
    "            \n",
    "            print(f\"   ‚úÖ SAVED: {output_file}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå SAVE ERROR: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def process_all_responses(self):\n",
    "        \"\"\"Process all response files in model_output directory.\"\"\"\n",
    "        print(\"üîÑ Processing all model responses...\")\n",
    "        \n",
    "        if not self.model_output_path.exists():\n",
    "            print(f\"‚ùå Directory {self.model_output_path} not found\")\n",
    "            return\n",
    "        \n",
    "        # Look for model subfolders in model_output\n",
    "        model_folders = [d for d in self.model_output_path.iterdir() if d.is_dir()]\n",
    "        \n",
    "        if not model_folders:\n",
    "            # Fallback: look for .txt files directly in model_output (old structure)\n",
    "            response_files = list(self.model_output_path.glob('*.txt'))\n",
    "            if response_files:\n",
    "                print(f\"üìÅ Found {len(response_files)} response files in old structure\")\n",
    "                self._process_files_directly(response_files)\n",
    "            else:\n",
    "                print(\"‚ùå No model folders or .txt files found in model_output/\")\n",
    "            return\n",
    "        \n",
    "        print(f\"üìÅ Found {len(model_folders)} model folders:\")\n",
    "        for folder in model_folders:\n",
    "            print(f\"   üìÇ {folder.name}/\")\n",
    "        \n",
    "        total_success = 0\n",
    "        total_failed = 0\n",
    "        \n",
    "        # Process each model folder\n",
    "        for model_folder in model_folders:\n",
    "            print(f\"\\nüîÑ Processing model: {model_folder.name}\")\n",
    "            \n",
    "            # Get all .txt files in this model folder\n",
    "            response_files = list(model_folder.glob('*.txt'))\n",
    "            print(f\"   üìÑ Found {len(response_files)} response files\")\n",
    "            \n",
    "            if not response_files:\n",
    "                print(\"   ‚ö†Ô∏è  No .txt files found in this model folder\")\n",
    "                continue\n",
    "            \n",
    "            success_count = 0\n",
    "            failed_count = 0\n",
    "            \n",
    "            for response_file in response_files:\n",
    "                result = self.parse_single_file(response_file)\n",
    "                \n",
    "                if result['success']:\n",
    "                    # Update metadata to include model folder name\n",
    "                    if 'metadata' not in result:\n",
    "                        result['metadata'] = {}\n",
    "                    result['metadata']['model_folder'] = model_folder.name\n",
    "                    \n",
    "                    if self.save_parsed_file(result, response_file.name, model_folder.name):\n",
    "                        success_count += 1\n",
    "                    else:\n",
    "                        failed_count += 1\n",
    "                else:\n",
    "                    failed_count += 1\n",
    "            \n",
    "            print(f\"   ‚úÖ Successfully processed: {success_count} files\")\n",
    "            print(f\"   ‚ùå Failed to process: {failed_count} files\")\n",
    "            \n",
    "            total_success += success_count\n",
    "            total_failed += failed_count\n",
    "        \n",
    "        print(f\"\\nüéâ Overall processing completed!\")\n",
    "        print(f\"‚úÖ Total successfully processed: {total_success} files\")\n",
    "        print(f\"‚ùå Total failed to process: {total_failed} files\")\n",
    "        \n",
    "        # Show what was created\n",
    "        if total_success > 0:\n",
    "            print(f\"\\nüìÅ Results saved to '{self.parsed_path}':\")\n",
    "            for model_folder in sorted(self.parsed_path.iterdir()):\n",
    "                if model_folder.is_dir():\n",
    "                    php_files = list(model_folder.glob('*.php'))\n",
    "                    print(f\"   üìÇ {model_folder.name}/ ({len(php_files)} files)\")\n",
    "    \n",
    "    def _process_files_directly(self, response_files):\n",
    "        \"\"\"Process files directly from model_output (fallback for old structure).\"\"\"\n",
    "        success_count = 0\n",
    "        failed_count = 0\n",
    "        \n",
    "        for response_file in response_files:\n",
    "            result = self.parse_single_file(response_file)\n",
    "            \n",
    "            if result['success']:\n",
    "                if self.save_parsed_file(result, response_file.name):\n",
    "                    success_count += 1\n",
    "                else:\n",
    "                    failed_count += 1\n",
    "            else:\n",
    "                failed_count += 1\n",
    "        \n",
    "        print(f\"\\nüéâ Processing completed!\")\n",
    "        print(f\"‚úÖ Successfully processed: {success_count} files\")\n",
    "        print(f\"‚ùå Failed to process: {failed_count} files\")\n",
    "\n",
    "# Initialize simplified parser\n",
    "parser = OutputParser()\n",
    "print(\"Simplified output parser ready!\")\n",
    "print(\"Features:\")\n",
    "print(\"   ‚Ä¢ Simple single-file processing only\")\n",
    "print(\"   ‚Ä¢ Reliable MIGRATION_START/END marker extraction\")\n",
    "print(\"   ‚Ä¢ Clear error reporting\")\n",
    "print(\"   ‚Ä¢ No chunking complexity\")\n",
    "print(\"Usage: parser.process_all_responses()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ File Reconstructor ready!\n",
      "üéØ Features:\n",
      "   ‚Ä¢ Finds all chunked files in chunked_model_output/\n",
      "   ‚Ä¢ Uses simple parser to parse individual chunks\n",
      "   ‚Ä¢ Combines chunks in correct order\n",
      "   ‚Ä¢ Handles missing chunks gracefully\n",
      "   ‚Ä¢ Saves complete files to new-version/\n",
      "üí° Usage: reconstructor.reconstruct_all_files()\n"
     ]
    }
   ],
   "source": [
    "# FILE RECONSTRUCTOR - COMBINES PARSED CHUNKS INTO COMPLETE FILES\n",
    "class FileReconstructor:\n",
    "    \"\"\"Reconstructs complete files from parsed chunk files.\"\"\"\n",
    "    \n",
    "    def __init__(self, parser):\n",
    "        self.parser = parser  # Use the simple parser for individual chunks\n",
    "        self.chunked_output_path = Path('chunked_model_output')\n",
    "        self.final_output_path = Path('new-version')\n",
    "        self.final_output_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    def find_chunked_files(self):\n",
    "        \"\"\"Find all chunked file directories.\"\"\"\n",
    "        if not self.chunked_output_path.exists():\n",
    "            print(\"No chunked_model_output directory found\")\n",
    "            return []\n",
    "        \n",
    "        chunked_files = []\n",
    "        \n",
    "        for model_dir in self.chunked_output_path.iterdir():\n",
    "            if model_dir.is_dir():\n",
    "                for file_dir in model_dir.iterdir():\n",
    "                    if file_dir.is_dir():\n",
    "                        # Check if it has numbered chunk files\n",
    "                        chunk_files = list(file_dir.glob('*.txt'))\n",
    "                        if chunk_files:\n",
    "                            chunked_files.append({\n",
    "                                'model': model_dir.name,\n",
    "                                'filename': file_dir.name,\n",
    "                                'directory': file_dir,\n",
    "                                'chunk_count': len(chunk_files)\n",
    "                            })\n",
    "        \n",
    "        return chunked_files\n",
    "    \n",
    "    def get_chunk_files(self, directory: Path):\n",
    "        \"\"\"Get all chunk files from a directory, sorted by number.\"\"\"\n",
    "        chunk_files = []\n",
    "        \n",
    "        for file in directory.glob('*.txt'):\n",
    "            try:\n",
    "                # Extract number from filename (1.txt -> 1)\n",
    "                chunk_num = int(file.stem)\n",
    "                chunk_files.append((chunk_num, file))\n",
    "            except ValueError:\n",
    "                print(f\"WARNING: Skipping non-numeric chunk file: {file.name}\")\n",
    "        \n",
    "        # Sort by chunk number\n",
    "        chunk_files.sort(key=lambda x: x[0])\n",
    "        return chunk_files\n",
    "    \n",
    "    def reconstruct_file(self, file_info: dict):\n",
    "        \"\"\"Reconstruct a complete file from its chunks.\"\"\"\n",
    "        print(f\"\\nReconstructing {file_info['filename']}.php from {file_info['chunk_count']} chunks\")\n",
    "        print(f\"Model: {file_info['model']}\")\n",
    "        print(f\"Directory: {file_info['directory']}\")\n",
    "        \n",
    "        # Get sorted chunk files\n",
    "        chunk_files = self.get_chunk_files(file_info['directory'])\n",
    "        \n",
    "        if not chunk_files:\n",
    "            print(\"   ERROR: No valid chunk files found\")\n",
    "            return False\n",
    "        \n",
    "        # Check for missing chunks\n",
    "        expected_numbers = list(range(1, len(chunk_files) + 1))\n",
    "        actual_numbers = [num for num, _ in chunk_files]\n",
    "        missing = set(expected_numbers) - set(actual_numbers)\n",
    "        \n",
    "        if missing:\n",
    "            print(f\"   WARNING: Missing chunks: {sorted(missing)}\")\n",
    "        \n",
    "        print(f\"   Found chunks: {actual_numbers}\")\n",
    "        \n",
    "        # Parse each chunk\n",
    "        parsed_chunks = []\n",
    "        metadata = None\n",
    "        \n",
    "        for chunk_num, chunk_file in chunk_files:\n",
    "            print(f\"   Processing chunk {chunk_num}...\")\n",
    "            result = self.parser.parse_single_file(chunk_file)\n",
    "            \n",
    "            if result['success']:\n",
    "                parsed_chunks.append({\n",
    "                    'number': chunk_num,\n",
    "                    'code': result['migrated_code'],\n",
    "                    'metadata': result['metadata']\n",
    "                })\n",
    "                \n",
    "                # Use metadata from first successful chunk\n",
    "                if metadata is None:\n",
    "                    metadata = result['metadata']\n",
    "                    \n",
    "                print(f\"      SUCCESS: {len(result['migrated_code'])} chars\")\n",
    "            else:\n",
    "                print(f\"      ERROR: Failed to parse chunk {chunk_num}\")\n",
    "                parsed_chunks.append({\n",
    "                    'number': chunk_num,\n",
    "                    'code': None,\n",
    "                    'metadata': None\n",
    "                })\n",
    "        \n",
    "        if not any(chunk['code'] for chunk in parsed_chunks):\n",
    "            print(\"   ERROR: No chunks could be parsed successfully\")\n",
    "            return False\n",
    "        \n",
    "        # Combine chunks\n",
    "        combined_code = []\n",
    "        successful_chunks = 0\n",
    "        final_code = \"\"  # Initialize final_code\n",
    "        \n",
    "        for chunk in parsed_chunks:\n",
    "            if chunk['code']:\n",
    "                combined_code.append(chunk['code'])\n",
    "                successful_chunks += 1\n",
    "            else:\n",
    "                print(f\"   WARNING: Chunk {chunk['number']} failed - adding placeholder comment\")\n",
    "                combined_code.append(f\"// ERROR: Chunk {chunk['number']} failed to parse\")\n",
    "        \n",
    "        final_code = ''.join(combined_code)\n",
    "        print(f\"   Combined {successful_chunks}/{len(parsed_chunks)} chunks successfully\")\n",
    "        print(f\"   Final code length: {len(final_code)} characters\")\n",
    "        \n",
    "        # Save reconstructed file\n",
    "        return self.save_reconstructed_file(file_info, final_code, metadata)\n",
    "    \n",
    "    def save_reconstructed_file(self, file_info: dict, code: str, metadata: dict):\n",
    "        \"\"\"Save the reconstructed complete file.\"\"\"\n",
    "        try:\n",
    "            # Create model folder in final output\n",
    "            model_folder = self.final_output_path / file_info['model']\n",
    "            model_folder.mkdir(exist_ok=True)\n",
    "            \n",
    "            # Save the reconstructed file\n",
    "            output_file = model_folder / f\"{file_info['filename']}.php\"\n",
    "            \n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                # Write clean PHP code without metadata header\n",
    "                f.write(code)\n",
    "            \n",
    "            print(f\"   SAVED: {output_file}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ERROR saving file: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def reconstruct_all_files(self):\n",
    "        \"\"\"Reconstruct all chunked files found.\"\"\"\n",
    "        print(\"üîß Starting file reconstruction...\")\n",
    "        \n",
    "        chunked_files = self.find_chunked_files()\n",
    "        \n",
    "        if not chunked_files:\n",
    "            print(\"No chunked files found to reconstruct\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Found {len(chunked_files)} chunked files to reconstruct:\")\n",
    "        for file_info in chunked_files:\n",
    "            print(f\"   {file_info['model']}/{file_info['filename']}.php ({file_info['chunk_count']} chunks)\")\n",
    "        \n",
    "        successful = 0\n",
    "        failed = 0\n",
    "        \n",
    "        for file_info in chunked_files:\n",
    "            if self.reconstruct_file(file_info):\n",
    "                successful += 1\n",
    "            else:\n",
    "                failed += 1\n",
    "        \n",
    "        print(f\"\\nüéâ Reconstruction completed!\")\n",
    "        print(f\"‚úÖ Successfully reconstructed: {successful} files\")\n",
    "        print(f\"‚ùå Failed to reconstruct: {failed} files\")\n",
    "        \n",
    "        if successful > 0:\n",
    "            print(f\"\\nüìÅ Reconstructed files saved to: {self.final_output_path}\")\n",
    "            for model_folder in sorted(self.final_output_path.iterdir()):\n",
    "                if model_folder.is_dir():\n",
    "                    php_files = list(model_folder.glob('*.php'))\n",
    "                    print(f\"   {model_folder.name}/ ({len(php_files)} files)\")\n",
    "\n",
    "# Initialize the reconstructor with our simple parser\n",
    "reconstructor = FileReconstructor(parser)\n",
    "print(\"‚úÖ File Reconstructor ready!\")\n",
    "print(\"üéØ Features:\")\n",
    "print(\"   ‚Ä¢ Finds all chunked files in chunked_model_output/\")\n",
    "print(\"   ‚Ä¢ Uses simple parser to parse individual chunks\")\n",
    "print(\"   ‚Ä¢ Combines chunks in correct order\")\n",
    "print(\"   ‚Ä¢ Handles missing chunks gracefully\")\n",
    "print(\"   ‚Ä¢ Saves complete files to new-version/\")\n",
    "print(\"üí° Usage: reconstructor.reconstruct_all_files()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Processing all model responses...\n",
      "üìÅ Found 1 model folders:\n",
      "   üìÇ mistral_small_3_2_24b_instruct_free/\n",
      "\n",
      "üîÑ Processing model: mistral_small_3_2_24b_instruct_free\n",
      "   üìÑ Found 20 response files\n",
      "Processing 007_class-wp.txt\n",
      "   SUCCESS: Found 28022 chars of migrated code\n",
      "   ‚úÖ SAVED: new-version\\mistral_small_3_2_24b_instruct_free\\007_class-wp.php\n",
      "Processing 008_wp-login.txt\n",
      "   SUCCESS: Found 35944 chars of migrated code\n",
      "   ‚úÖ SAVED: new-version\\mistral_small_3_2_24b_instruct_free\\008_wp-login.php\n",
      "Processing 017_press-this.txt\n",
      "   SUCCESS: Found 32665 chars of migrated code\n",
      "   ‚úÖ SAVED: new-version\\mistral_small_3_2_24b_instruct_free\\017_press-this.php\n",
      "Processing 020_class-ftp.txt\n",
      "   SUCCESS: Found 36555 chars of migrated code\n",
      "   ‚úÖ SAVED: new-version\\mistral_small_3_2_24b_instruct_free\\020_class-ftp.php\n",
      "Processing 021_class-pop3.txt\n",
      "   SUCCESS: Found 16721 chars of migrated code\n",
      "   ‚úÖ SAVED: new-version\\mistral_small_3_2_24b_instruct_free\\021_class-pop3.php\n",
      "Processing 022_class-wp-customize-setting.txt\n",
      "   SUCCESS: Found 14053 chars of migrated code\n",
      "   ‚úÖ SAVED: new-version\\mistral_small_3_2_24b_instruct_free\\022_class-wp-customize-setting.php\n",
      "Processing 024_edit-form-advanced.txt\n",
      "   SUCCESS: Found 27144 chars of migrated code\n",
      "   ‚úÖ SAVED: new-version\\mistral_small_3_2_24b_instruct_free\\024_edit-form-advanced.php\n",
      "Processing 026_Source.txt\n",
      "   SUCCESS: Found 22652 chars of migrated code\n",
      "   ‚úÖ SAVED: new-version\\mistral_small_3_2_24b_instruct_free\\026_Source.php\n",
      "Processing 029_module.audio.ogg.txt\n",
      "   SUCCESS: Found 29914 chars of migrated code\n",
      "   ‚úÖ SAVED: new-version\\mistral_small_3_2_24b_instruct_free\\029_module.audio.ogg.php\n",
      "Processing 030_class-json.txt\n",
      "   SUCCESS: Found 25087 chars of migrated code\n",
      "   ‚úÖ SAVED: new-version\\mistral_small_3_2_24b_instruct_free\\030_class-json.php\n",
      "Processing 031_class-wp-plugins-list-table.txt\n",
      "   SUCCESS: Found 23780 chars of migrated code\n",
      "   ‚úÖ SAVED: new-version\\mistral_small_3_2_24b_instruct_free\\031_class-wp-plugins-list-table.php\n",
      "Processing 032_class-wp-comments-list-table.txt\n",
      "   SUCCESS: Found 25158 chars of migrated code\n",
      "   ‚úÖ SAVED: new-version\\mistral_small_3_2_24b_instruct_free\\032_class-wp-comments-list-table.php\n",
      "Processing 039_class-wp-upgrader-skins.txt\n",
      "   SUCCESS: Found 32779 chars of migrated code\n",
      "   ‚úÖ SAVED: new-version\\mistral_small_3_2_24b_instruct_free\\039_class-wp-upgrader-skins.php\n",
      "Processing 040_class-wp-media-list-table.txt\n",
      "   SUCCESS: Found 18897 chars of migrated code\n",
      "   ‚úÖ SAVED: new-version\\mistral_small_3_2_24b_instruct_free\\040_class-wp-media-list-table.php\n",
      "Processing 042_image-edit.txt\n",
      "   SUCCESS: Found 26561 chars of migrated code\n",
      "   ‚úÖ SAVED: new-version\\mistral_small_3_2_24b_instruct_free\\042_image-edit.php\n",
      "Processing 043_image.txt\n",
      "   SUCCESS: Found 22533 chars of migrated code\n",
      "   ‚úÖ SAVED: new-version\\mistral_small_3_2_24b_instruct_free\\043_image.php\n",
      "Processing 046_module.audio-video.flv.txt\n",
      "   SUCCESS: Found 29653 chars of migrated code\n",
      "   ‚úÖ SAVED: new-version\\mistral_small_3_2_24b_instruct_free\\046_module.audio-video.flv.php\n",
      "Processing 047_update-core.txt\n",
      "   SUCCESS: Found 29855 chars of migrated code\n",
      "   ‚úÖ SAVED: new-version\\mistral_small_3_2_24b_instruct_free\\047_update-core.php\n",
      "Processing 050_class-oembed.txt\n",
      "   SUCCESS: Found 24947 chars of migrated code\n",
      "   ‚úÖ SAVED: new-version\\mistral_small_3_2_24b_instruct_free\\050_class-oembed.php\n",
      "Processing 054_nav-menu.txt\n",
      "   SUCCESS: Found 28623 chars of migrated code\n",
      "   ‚úÖ SAVED: new-version\\mistral_small_3_2_24b_instruct_free\\054_nav-menu.php\n",
      "   ‚úÖ Successfully processed: 20 files\n",
      "   ‚ùå Failed to process: 0 files\n",
      "\n",
      "üéâ Overall processing completed!\n",
      "‚úÖ Total successfully processed: 20 files\n",
      "‚ùå Total failed to process: 0 files\n",
      "\n",
      "üìÅ Results saved to 'new-version':\n",
      "   üìÇ mistral_small_3_2_24b_instruct_free/ (20 files)\n",
      "üîß Starting file reconstruction...\n",
      "No chunked files found to reconstruct\n"
     ]
    }
   ],
   "source": [
    "parser.process_all_responses()\n",
    "reconstructor.reconstruct_all_files()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
