{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Simple LLM Migration Tool\n",
    "\n",
    "\n",
    "This notebook implements a simple tool for testing open source Large Language Models' capability in migrating legacy PHP code (WordPress 4.3) to modern PHP 8.3 standards.\n",
    "\n",
    "## What it does:\n",
    "- ✅ Sends your prompt to any OpenRouter model\n",
    "- ✅ Saves the complete raw response to a text file\n",
    "- ✅ No complex evaluation - just pure model output\n",
    "- ✅ Perfect for examining what models actually return\n",
    "\n",
    "**Browse available models at https://openrouter.ai/models**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔬 Multi-Provider LLM Migration Tool Initialized\n",
      "⚙️  Timestamp: 2025-09-03 12:28:35\n",
      "✅ OpenRouter client initialized successfully\n",
      "✅ OpenRouter client initialized successfully\n",
      "✅ Google AI client initialized successfully\n",
      "\n",
      "🎯 Available providers: openrouter, google\n",
      "\n",
      "🌐 Supported models:\n",
      "   OpenRouter: 'anthropic/claude-3.5-sonnet', 'meta-llama/llama-3.1-8b-instruct', etc.\n",
      "   Google AI: 'gemini-1.5-pro', 'gemini-1.5-flash', 'gemini-1.0-pro', etc.\n",
      "📋 Visit https://openrouter.ai/models for OpenRouter model list\n",
      "✅ Google AI client initialized successfully\n",
      "\n",
      "🎯 Available providers: openrouter, google\n",
      "\n",
      "🌐 Supported models:\n",
      "   OpenRouter: 'anthropic/claude-3.5-sonnet', 'meta-llama/llama-3.1-8b-instruct', etc.\n",
      "   Google AI: 'gemini-1.5-pro', 'gemini-1.5-flash', 'gemini-1.0-pro', etc.\n",
      "📋 Visit https://openrouter.ai/models for OpenRouter model list\n"
     ]
    }
   ],
   "source": [
    "# Setup Environment - Multi-Provider Support\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Multi-Provider API setup\n",
    "print(\"🔬 Multi-Provider LLM Migration Tool Initialized\")\n",
    "print(f\"⚙️  Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Initialize OpenRouter client\n",
    "try:\n",
    "    OPENROUTER_API_KEY = os.getenv('OPENROUTER_API_KEY')\n",
    "    \n",
    "    if not OPENROUTER_API_KEY:\n",
    "        raise ValueError(\"OPENROUTER_API_KEY not found in environment variables.\")\n",
    "    \n",
    "    openrouter_client = openai.OpenAI(\n",
    "        base_url=\"https://openrouter.ai/api/v1\",\n",
    "        api_key=OPENROUTER_API_KEY,\n",
    "    )\n",
    "    print(\"✅ OpenRouter client initialized successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error initializing OpenRouter client: {e}\")\n",
    "    openrouter_client = None\n",
    "\n",
    "# Initialize Google AI client\n",
    "try:\n",
    "    GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
    "    \n",
    "    if not GOOGLE_API_KEY:\n",
    "        raise ValueError(\"GOOGLE_API_KEY not found in environment variables.\")\n",
    "    \n",
    "    # Import and configure Google AI with new API\n",
    "    import google.genai as genai\n",
    "    \n",
    "    # Create client with API key\n",
    "    google_client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "    print(\"✅ Google AI client initialized successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error initializing Google AI client: {e}\")\n",
    "    google_client = None\n",
    "\n",
    "# Provider configuration\n",
    "PROVIDERS = {\n",
    "    'openrouter': {\n",
    "        'client': openrouter_client,\n",
    "        'api_key': OPENROUTER_API_KEY,\n",
    "        'enabled': openrouter_client is not None\n",
    "    },\n",
    "    'google': {\n",
    "        'client': google_client,\n",
    "        'api_key': GOOGLE_API_KEY,\n",
    "        'enabled': google_client is not None\n",
    "    }\n",
    "}\n",
    "\n",
    "# Show available providers\n",
    "enabled_providers = [name for name, config in PROVIDERS.items() if config['enabled']]\n",
    "print(f\"\\n🎯 Available providers: {', '.join(enabled_providers)}\")\n",
    "\n",
    "print(\"\\n🌐 Supported models:\")\n",
    "print(\"   OpenRouter: 'anthropic/claude-3.5-sonnet', 'meta-llama/llama-3.1-8b-instruct', etc.\")\n",
    "print(\"   Google AI: 'gemini-1.5-pro', 'gemini-1.5-flash', 'gemini-1.0-pro', etc.\")\n",
    "print(\"📋 Visit https://openrouter.ai/models for OpenRouter model list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Simplified Multi-Provider Client Ready!\n",
      "✅ Improvements:\n",
      "   • Removed code duplication in error handling\n",
      "   • Data-driven provider detection\n",
      "   • Centralized configuration management\n",
      "   • Standardized response formatting\n",
      "   • Cleaner error handling\n",
      "\n",
      "🔍 Provider Detection Test:\n",
      "   gemini-1.5-pro → GOOGLE\n",
      "   anthropic/claude-3.5-sonnet → OPENROUTER\n",
      "   meta-llama/llama-3.1-8b-instruct → OPENROUTER\n",
      "   gemini-1.5-flash → GOOGLE\n"
     ]
    }
   ],
   "source": [
    "# SIMPLIFIED MULTI-PROVIDER ROUTING SYSTEM\n",
    "import requests\n",
    "import google.genai as genai\n",
    "\n",
    "class MultiProviderClient:\n",
    "    \"\"\"Simplified multi-provider LLM client with reduced redundancy.\"\"\"\n",
    "    \n",
    "    # Provider detection patterns (data-driven approach)\n",
    "    PROVIDER_PATTERNS = {\n",
    "        'google': ['gemini', 'palm', 'bard'],\n",
    "        'openrouter': ['anthropic', 'openai', 'meta', 'mistral', 'cohere', \n",
    "                      'deepseek', 'qwen', 'dolphin', 'nous', 'microsoft']\n",
    "    }\n",
    "    \n",
    "    # Default configurations\n",
    "    DEFAULT_CONFIG = {\n",
    "        'max_tokens': {'google': 8192, 'openrouter': 80000},\n",
    "        'temperature': 0.3,\n",
    "        'timeout': 300\n",
    "    }\n",
    "    \n",
    "    def __init__(self, providers):\n",
    "        self.providers = providers\n",
    "    \n",
    "    def detect_provider(self, model_name: str) -> str:\n",
    "        \"\"\"Detect provider using pattern matching.\"\"\"\n",
    "        model_lower = model_name.lower()\n",
    "        \n",
    "        # Check for Google AI patterns\n",
    "        if any(keyword in model_lower for keyword in self.PROVIDER_PATTERNS['google']):\n",
    "            return 'google'\n",
    "        \n",
    "        # Check for OpenRouter patterns (including slash notation)\n",
    "        if ('/' in model_name or \n",
    "            any(pattern in model_lower for pattern in self.PROVIDER_PATTERNS['openrouter'])):\n",
    "            return 'openrouter'\n",
    "        \n",
    "        return 'openrouter'  # Default fallback\n",
    "    \n",
    "    def make_api_call(self, model_name: str, prompt: str, **kwargs) -> dict:\n",
    "        \"\"\"Unified API call with error handling.\"\"\"\n",
    "        provider = self.detect_provider(model_name)\n",
    "        \n",
    "        # Check provider availability\n",
    "        if not self.providers.get(provider, {}).get('enabled'):\n",
    "            return self._error_response(f'Provider {provider} is not enabled')\n",
    "        \n",
    "        print(f\"🔗 Using {provider.upper()} provider for {model_name}\")\n",
    "        \n",
    "        # Route to appropriate provider method\n",
    "        try:\n",
    "            if provider == 'google':\n",
    "                return self._call_google(model_name, prompt, **kwargs)\n",
    "            else:  # openrouter\n",
    "                return self._call_openrouter(model_name, prompt, **kwargs)\n",
    "        except Exception as e:\n",
    "            return self._error_response(str(e))\n",
    "    \n",
    "    def _call_openrouter(self, model_name: str, prompt: str, **kwargs) -> dict:\n",
    "        \"\"\"OpenRouter API call.\"\"\"\n",
    "        payload = {\n",
    "            \"model\": model_name,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"max_tokens\": kwargs.get('max_tokens', self.DEFAULT_CONFIG['max_tokens']['openrouter']),\n",
    "            \"temperature\": kwargs.get('temperature', self.DEFAULT_CONFIG['temperature'])\n",
    "        }\n",
    "        \n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.providers['openrouter']['api_key']}\",\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"HTTP-Referer\": \"https://github.com/research-project\",\n",
    "            \"X-Title\": \"LLM PHP Migration Research\"\n",
    "        }\n",
    "        \n",
    "        response = requests.post(\n",
    "            \"https://openrouter.ai/api/v1/chat/completions\",\n",
    "            headers=headers,\n",
    "            data=json.dumps(payload),\n",
    "            timeout=self.DEFAULT_CONFIG['timeout']\n",
    "        )\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            return self._error_response(f'HTTP {response.status_code}: {response.text[:500]}')\n",
    "        \n",
    "        result = response.json()\n",
    "        return self._success_response(\n",
    "            content=result['choices'][0]['message']['content'],\n",
    "            provider='openrouter',\n",
    "            model=model_name,\n",
    "            usage=result.get('usage', {})\n",
    "        )\n",
    "    \n",
    "    def _call_google(self, model_name: str, prompt: str, **kwargs) -> dict:\n",
    "        \"\"\"Google AI API call.\"\"\"\n",
    "        client = self.providers['google']['client']\n",
    "        \n",
    "        response = client.models.generate_content(\n",
    "            model=model_name,\n",
    "            contents=prompt,\n",
    "            config={\n",
    "                'temperature': kwargs.get('temperature', self.DEFAULT_CONFIG['temperature']),\n",
    "                'max_output_tokens': kwargs.get('max_tokens', self.DEFAULT_CONFIG['max_tokens']['google']),\n",
    "                'top_p': 0.95,\n",
    "                'top_k': 40\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        if not response.text:\n",
    "            return self._error_response('Empty response from Google AI')\n",
    "        \n",
    "        # Extract usage info safely\n",
    "        usage_info = {}\n",
    "        if hasattr(response, 'usage_metadata'):\n",
    "            try:\n",
    "                usage_info = {\n",
    "                    'prompt_tokens': getattr(response.usage_metadata, 'prompt_token_count', 0),\n",
    "                    'completion_tokens': getattr(response.usage_metadata, 'candidates_token_count', 0)\n",
    "                }\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return self._success_response(\n",
    "            content=response.text,\n",
    "            provider='google',\n",
    "            model=model_name,\n",
    "            usage=usage_info\n",
    "        )\n",
    "    \n",
    "    def _success_response(self, content: str, provider: str, model: str, usage: dict) -> dict:\n",
    "        \"\"\"Standardized success response.\"\"\"\n",
    "        return {\n",
    "            'success': True,\n",
    "            'content': content,\n",
    "            'provider': provider,\n",
    "            'model': model,\n",
    "            'usage': usage\n",
    "        }\n",
    "    \n",
    "    def _error_response(self, error_message: str) -> dict:\n",
    "        \"\"\"Standardized error response.\"\"\"\n",
    "        return {'success': False, 'error': error_message}\n",
    "\n",
    "# Initialize the simplified multi-provider client\n",
    "multi_client = MultiProviderClient(PROVIDERS)\n",
    "\n",
    "\n",
    "# Test provider detection\n",
    "test_models = [\n",
    "    'gemini-1.5-pro',\n",
    "    'anthropic/claude-3.5-sonnet', \n",
    "    'meta-llama/llama-3.1-8b-instruct',\n",
    "    'gemini-1.5-flash'\n",
    "]\n",
    "\n",
    "print(\"\\n🔍 Provider Detection Test:\")\n",
    "for model in test_models:\n",
    "    provider = multi_client.detect_provider(model)\n",
    "    print(f\"   {model} → {provider.upper()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD TEST FILES\n",
    "test_files = {}\n",
    "old_version_path = Path('selected_100_files\\extra_large_1000_plus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Basic prompting strategy configured with fixed marker placement\n"
     ]
    }
   ],
   "source": [
    "# BASIC PROMPTING STRATEGY\n",
    "BASIC_PROMPT_TEMPLATE = \"\"\"You are a senior PHP developer with expertise in legacy code modernization. Your task is to migrate this PHP code to PHP 8.3 standards while maintaining functional equivalence.\n",
    "\n",
    "Please migrate the following PHP code to PHP 8.3:\n",
    "\n",
    "{code}\n",
    "\n",
    "Your response should follow this EXACT format:\n",
    "\n",
    "// MIGRATION_START\n",
    "[your migrated PHP code here]\n",
    "// MIGRATION_END\n",
    "\n",
    "CRITICAL FORMATTING REQUIREMENT: \n",
    "- Place the MIGRATION_START marker BEFORE the opening <?php tag\n",
    "- Place the MIGRATION_END marker AFTER the closing PHP code\n",
    "- Do NOT place these markers inside the PHP code itself\n",
    "\n",
    "Provide only the migrated PHP code with the markers placed correctly outside the PHP code block, no additional commentary.\"\"\"\n",
    "\n",
    "print(\"✅ Basic prompting strategy configured with fixed marker placement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Comprehensive and chunking prompting strategies configured with fixed marker placement\n",
      "🔧 Updated all prompts to prevent placing MIGRATION markers inside PHP code\n"
     ]
    }
   ],
   "source": [
    "# COMPREHENSIVE PROMPTING STRATEGY\n",
    "COMPREHENSIVE_PROMPT_TEMPLATE = \"\"\"You are a senior PHP developer with expertise in legacy code modernization. Your task is to migrate old PHP code up to PHP 8.3 standards while maintaining the functionality of the original code.\n",
    "\n",
    "Migration Requirements:\n",
    "1. Update deprecated syntax\n",
    "2. Replace deprecated functions\n",
    "3. Implement modern PHP features\n",
    "4. Improve security and code quality\n",
    "5. Maintain functional equivalence\n",
    "6. Enforce strict typing\n",
    "7. Adopt core PHP 8.3 constructs\n",
    "\n",
    "Please migrate the following PHP code to PHP 8.3:\n",
    "\n",
    "{code}\n",
    "\n",
    "\n",
    "\n",
    "Your response should follow this EXACT format:\n",
    "\n",
    "// MIGRATION_START\n",
    "[your migrated PHP code here]\n",
    "// MIGRATION_END\n",
    "\n",
    "CRITICAL FORMATTING REQUIREMENT: \n",
    "- Place the MIGRATION_START marker BEFORE the opening <?php tag\n",
    "- Place the MIGRATION_END marker AFTER the closing PHP code\n",
    "- Do NOT place these markers inside the PHP code itself\n",
    "\n",
    "Include the markers as comments OUTSIDE the PHP code block. Keep the original comments as they are.\n",
    "Do not add any other text, explanations, or commentary outside the markers. Make sure you give the COMPLETE migrated code.\"\"\"\n",
    "\n",
    "# CHUNKING PROMPTS FOR LARGE FILES\n",
    "CHUNK_BASIC_PROMPT_TEMPLATE = \"\"\"You are a senior PHP developer with expertise in legacy code modernization. Your task is to migrate this PARTIAL SEGMENT of a larger PHP file up to PHP 8.3 standards.\n",
    "\n",
    "CONTEXT:\n",
    "- Original file: {filename}\n",
    "- Processing lines: {start_line} to {end_line} (of {total_lines} total lines)\n",
    "- This is chunk {chunk_number} of {total_chunks}\n",
    "\n",
    "CRITICAL INSTRUCTIONS:\n",
    "1. This is only a SEGMENT of the complete file\n",
    "2. Do NOT add extra opening <?php tags if the code segment doesn't start with one\n",
    "3. Do NOT add closing ?> tags if its not already present\n",
    "4. Do NOT try to complete missing parts or add closing braces that aren't provided\n",
    "5. Preserve the exact structure - if it starts with a method, start with that method\n",
    "6. If it starts mid-class, do NOT add class opening braces\n",
    "\n",
    "Please migrate ONLY the following PHP code segment to PHP 8.3:\n",
    "\n",
    "{code}\n",
    "\n",
    "Your response should follow this EXACT format:\n",
    "\n",
    "// MIGRATION_START\n",
    "[your migrated code segment here - exactly as provided]\n",
    "// MIGRATION_END\n",
    "\n",
    "CRITICAL FORMATTING REQUIREMENT: \n",
    "- Place the MIGRATION_START marker BEFORE the code segment\n",
    "- Place the MIGRATION_END marker AFTER the code segment\n",
    "- Do NOT place these markers inside the PHP code itself\n",
    "\n",
    "Migrate only the provided code segment. Do not try to complete the file or change any logic.\"\"\"\n",
    "\n",
    "CHUNK_COMPREHENSIVE_PROMPT_TEMPLATE = \"\"\"You are a senior PHP developer with expertise in legacy code modernization. Your task is to migrate this PARTIAL SEGMENT of a larger PHP file to PHP 8.3 standards while maintaining functional equivalence.\n",
    "\n",
    "CONTEXT:\n",
    "- Original file: {filename}\n",
    "- Processing lines: {start_line} to {end_line} (of {total_lines} total lines)  \n",
    "- This is chunk {chunk_number} of {total_chunks}\n",
    "\n",
    "CRITICAL INSTRUCTIONS:\n",
    "1. This is only a SEGMENT of the complete file\n",
    "2. Do NOT add opening <?php tags if the code segment doesn't start with one\n",
    "3. Do NOT add closing ?> tags\n",
    "4. Do NOT try to complete missing parts or add code that isn't provided\n",
    "5. Preserve the exact structure - if it starts with a method, start with that method\n",
    "6. If it starts mid-class, do NOT add class opening braces\n",
    "\n",
    "Migration Requirements for this segment:\n",
    "1. Update deprecated syntax\n",
    "2. Replace deprecated functions\n",
    "3. Implement modern PHP features\n",
    "4. Improve security and code quality\n",
    "5. Maintain functional equivalence\n",
    "6. Enforce strict typing\n",
    "7. Adopt core PHP 8.3 constructs\n",
    "\n",
    "\n",
    "Please migrate ONLY the following PHP code segment to PHP 8.3:\n",
    "\n",
    "{code}\n",
    "\n",
    "Your response should follow this EXACT format:\n",
    "\n",
    "// MIGRATION_START\n",
    "[your migrated code segment here - exactly as provided, no extra <?php tags]\n",
    "// MIGRATION_END\n",
    "\n",
    "CRITICAL FORMATTING REQUIREMENT: \n",
    "- Place the MIGRATION_START marker BEFORE the code segment\n",
    "- Place the MIGRATION_END marker AFTER the code segment\n",
    "- Do NOT place these markers inside the PHP code itself\n",
    "\n",
    "Include the markers as comments OUTSIDE the code segment. Keep the original comments as they are.\n",
    "Migrate only the provided code segment. Do not add missing functions, classes, or try to complete the file.\"\"\"\n",
    "\n",
    "print(\"✅ Comprehensive and chunking prompting strategies configured with fixed marker placement\")\n",
    "print(\"🔧 Updated all prompts to prevent placing MIGRATION markers inside PHP code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import subprocess\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def chunk_code(code: str, chunk_size: int = None) -> list:\n",
    "    \"\"\"Smart PHP-aware chunking using PHP tokenizer for accurate function detection.\"\"\"\n",
    "    if chunk_size is None:\n",
    "        chunk_size = DEFAULT_CHUNK_SIZE\n",
    "    \n",
    "    lines = code.split('\\n')\n",
    "    total_lines = len(lines)\n",
    "    \n",
    "    if total_lines <= chunk_size:\n",
    "        return [{\n",
    "            'start_line': 1,\n",
    "            'end_line': total_lines,\n",
    "            'actual_size': total_lines,\n",
    "            'total_lines': total_lines,\n",
    "            'code': code\n",
    "        }]\n",
    "    \n",
    "    # Get function boundaries using smart parsing\n",
    "    function_boundaries = find_function_boundaries(code, lines)\n",
    "    \n",
    "    # Create chunks based on function boundaries\n",
    "    return create_smart_chunks(lines, function_boundaries, chunk_size, total_lines)\n",
    "\n",
    "def find_function_boundaries(code: str, lines: list) -> list:\n",
    "    \"\"\"Find function boundaries using PHP tokenizer if available, else regex.\"\"\"\n",
    "    # Try PHP tokenizer first\n",
    "    php_functions = try_php_tokenizer(code, lines)\n",
    "    if php_functions:\n",
    "        return php_functions\n",
    "    \n",
    "    # Fallback to regex-based parsing\n",
    "    return find_functions_with_regex(lines)\n",
    "\n",
    "def try_php_tokenizer(code: str, lines: list) -> list:\n",
    "    \"\"\"Try to use PHP's built-in tokenizer for accurate parsing.\"\"\"\n",
    "    try:\n",
    "        php_script = f'''<?php\n",
    "$code = <<<'EOD'\n",
    "{code}\n",
    "EOD;\n",
    "\n",
    "$tokens = token_get_all($code);\n",
    "$functions = [];\n",
    "$current_function = null;\n",
    "$brace_level = 0;\n",
    "$in_function = false;\n",
    "\n",
    "foreach ($tokens as $token) {{\n",
    "    if (is_array($token)) {{\n",
    "        if ($token[0] === T_FUNCTION) {{\n",
    "            $in_function = true;\n",
    "            $current_function = [\n",
    "                'start_line' => $token[2] - 1,\n",
    "                'end_line' => null,\n",
    "                'name' => null\n",
    "            ];\n",
    "        }}\n",
    "        \n",
    "        if ($in_function && $token[0] === T_STRING && $current_function['name'] === null) {{\n",
    "            $current_function['name'] = $token[1];\n",
    "        }}\n",
    "    }} else {{\n",
    "        if ($token === '{{' && $in_function) {{\n",
    "            $brace_level++;\n",
    "        }} elseif ($token === '}}' && $in_function) {{\n",
    "            $brace_level--;\n",
    "            if ($brace_level === 0) {{\n",
    "                $current_function['end_line'] = find_closing_brace($current_function['start_line']);\n",
    "                $functions[] = $current_function;\n",
    "                $current_function = null;\n",
    "                $in_function = false;\n",
    "            }}\n",
    "        }}\n",
    "    }}\n",
    "}}\n",
    "\n",
    "function find_closing_brace($start_line) {{\n",
    "    global $code;\n",
    "    $lines = explode(\"\\\\n\", $code);\n",
    "    $brace_count = 0;\n",
    "    $function_started = false;\n",
    "    \n",
    "    for ($i = $start_line; $i < count($lines); $i++) {{\n",
    "        $line = trim($lines[$i]);\n",
    "        if (empty($line) || strpos($line, '//') === 0 || strpos($line, '#') === 0) continue;\n",
    "        \n",
    "        for ($j = 0; $j < strlen($line); $j++) {{\n",
    "            $char = $line[$j];\n",
    "            if ($char === '{{') {{\n",
    "                $brace_count++;\n",
    "                $function_started = true;\n",
    "            }} elseif ($char === '}}' && $function_started) {{\n",
    "                $brace_count--;\n",
    "                if ($brace_count === 0) return $i;\n",
    "            }}\n",
    "        }}\n",
    "    }}\n",
    "    return $start_line;\n",
    "}}\n",
    "\n",
    "echo json_encode($functions);\n",
    "?>'''\n",
    "        \n",
    "        temp_php = Path('temp_parser.php')\n",
    "        with open(temp_php, 'w', encoding='utf-8') as f:\n",
    "            f.write(php_script)\n",
    "        \n",
    "        result = subprocess.run(['php', str(temp_php)], \n",
    "                              capture_output=True, text=True, timeout=10)\n",
    "        temp_php.unlink()\n",
    "        \n",
    "        if result.returncode == 0 and result.stdout.strip():\n",
    "            return json.loads(result.stdout.strip())\n",
    "            \n",
    "    except (subprocess.TimeoutExpired, subprocess.CalledProcessError, \n",
    "            FileNotFoundError, json.JSONDecodeError):\n",
    "        pass\n",
    "    \n",
    "    return []\n",
    "\n",
    "def find_functions_with_regex(lines: list) -> list:\n",
    "    \"\"\"Regex-based function detection with proper closing brace detection.\"\"\"\n",
    "    functions = []\n",
    "    function_patterns = [\n",
    "        r'^\\s*(?:(?:public|private|protected)\\s+)?(?:static\\s+)?function\\s+(\\w+)\\s*\\(',\n",
    "        r'^\\s*(?:abstract\\s+)?(?:final\\s+)?function\\s+(\\w+)\\s*\\(',\n",
    "        r'^\\s*function\\s+(\\w+)\\s*\\('\n",
    "    ]\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        line = lines[i].strip()\n",
    "        \n",
    "        if not line or line.startswith(('//','#')):\n",
    "            i += 1\n",
    "            continue\n",
    "        \n",
    "        # Check for function start\n",
    "        function_match = None\n",
    "        for pattern in function_patterns:\n",
    "            match = re.match(pattern, line, re.IGNORECASE)\n",
    "            if match:\n",
    "                function_match = match\n",
    "                break\n",
    "        \n",
    "        if function_match:\n",
    "            function_name = function_match.group(1)\n",
    "            closing_brace_line = find_function_closing_brace(i, lines)\n",
    "            \n",
    "            if closing_brace_line is not None:\n",
    "                functions.append({\n",
    "                    'start_line': i,\n",
    "                    'end_line': closing_brace_line,\n",
    "                    'name': function_name\n",
    "                })\n",
    "                i = closing_brace_line + 1\n",
    "            else:\n",
    "                i += 1\n",
    "        else:\n",
    "            i += 1\n",
    "    \n",
    "    return functions\n",
    "\n",
    "def find_function_closing_brace(start_line: int, lines: list) -> int:\n",
    "    \"\"\"Find the closing brace line for a function.\"\"\"\n",
    "    brace_count = 0\n",
    "    function_started = False\n",
    "    \n",
    "    for i in range(start_line, len(lines)):\n",
    "        line = lines[i].strip()\n",
    "        \n",
    "        if not line or line.startswith(('//','#')):\n",
    "            continue\n",
    "        \n",
    "        # Simple brace counting (could be enhanced to handle strings/comments)\n",
    "        for char in line:\n",
    "            if char == '{':\n",
    "                brace_count += 1\n",
    "                function_started = True\n",
    "            elif char == '}' and function_started:\n",
    "                brace_count -= 1\n",
    "                if brace_count == 0:\n",
    "                    return i\n",
    "    \n",
    "    return None\n",
    "\n",
    "def create_smart_chunks(lines: list, function_boundaries: list, target_chunk_size: int, total_lines: int) -> list:\n",
    "    \"\"\"Create chunks that respect function boundaries.\"\"\"\n",
    "    chunks = []\n",
    "    current_pos = 0\n",
    "    functions = sorted(function_boundaries, key=lambda f: f['start_line'])\n",
    "    \n",
    "    while current_pos < total_lines:\n",
    "        # Calculate chunk end position\n",
    "        initial_end_pos = min(current_pos + target_chunk_size - 1, total_lines - 1)\n",
    "        \n",
    "        # Find functions that would be split by this chunk boundary\n",
    "        relevant_functions = [f for f in functions \n",
    "                            if (current_pos <= f['start_line'] <= initial_end_pos) or\n",
    "                               (f['start_line'] < current_pos and f['end_line'] and f['end_line'] >= current_pos)]\n",
    "        \n",
    "        # Extend chunk to complete functions if reasonable\n",
    "        final_end_pos = initial_end_pos\n",
    "        if relevant_functions:\n",
    "            for func in relevant_functions:\n",
    "                if func['end_line'] and func['end_line'] <= initial_end_pos + 300:  # Max extension\n",
    "                    final_end_pos = max(final_end_pos, func['end_line'])\n",
    "        \n",
    "        # Create chunk\n",
    "        chunk = {\n",
    "            'start_line': current_pos + 1,  # Convert to 1-based\n",
    "            'end_line': final_end_pos + 1,  # Convert to 1-based\n",
    "            'actual_size': final_end_pos - current_pos + 1,\n",
    "            'total_lines': total_lines,\n",
    "            'code': '\\n'.join(lines[current_pos:final_end_pos + 1])\n",
    "        }\n",
    "        chunks.append(chunk)\n",
    "        \n",
    "        current_pos = final_end_pos + 1\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_CHUNK_SIZE = 500  # Default chunk size in lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 All prompting strategies configured and ready\n",
      "📋 Available strategies: ['basic', 'comprehensive', 'chunk_basic', 'chunk_comprehensive']\n",
      "🔧 Added chunking utilities for large files\n",
      "📦 Default chunk size: 500 lines\n"
     ]
    }
   ],
   "source": [
    "# PROMPT HELPER FUNCTION\n",
    "PROMPT_TEMPLATES = {\n",
    "    'basic': BASIC_PROMPT_TEMPLATE,\n",
    "    'comprehensive': COMPREHENSIVE_PROMPT_TEMPLATE,\n",
    "    'chunk_basic': CHUNK_BASIC_PROMPT_TEMPLATE,\n",
    "    'chunk_comprehensive': CHUNK_COMPREHENSIVE_PROMPT_TEMPLATE,\n",
    "}\n",
    "\n",
    "\n",
    "def create_prompt(code: str, strategy: str = \"basic\", **kwargs) -> str:\n",
    "    \"\"\"Create migration prompts using different strategies.\"\"\"\n",
    "    if strategy not in PROMPT_TEMPLATES:\n",
    "        raise ValueError(f\"Unknown prompting strategy: {strategy}. Available: {list(PROMPT_TEMPLATES.keys())}\")\n",
    "    \n",
    "    template = PROMPT_TEMPLATES[strategy]\n",
    "    \n",
    "    # For chunking strategies, we need additional parameters\n",
    "    if strategy.startswith('chunk_'):\n",
    "        required_params = ['filename', 'start_line', 'end_line', 'total_lines', 'chunk_number', 'total_chunks']\n",
    "        missing_params = [param for param in required_params if param not in kwargs]\n",
    "        if missing_params:\n",
    "            raise ValueError(f\"Chunking strategy requires parameters: {missing_params}\")\n",
    "    \n",
    "    return template.format(code=code, **kwargs)\n",
    "\n",
    "print(\"🎯 All prompting strategies configured and ready\")\n",
    "print(f\"📋 Available strategies: {list(PROMPT_TEMPLATES.keys())}\")\n",
    "print(\"🔧 Added chunking utilities for large files\")\n",
    "print(f\"📦 Default chunk size: {DEFAULT_CHUNK_SIZE} lines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Multi-Provider Migration System Ready!\n",
      "🎯 Updated Features:\n",
      "   • Supports both OpenRouter and Google AI models\n",
      "   • Automatic provider detection and routing\n",
      "   • Same file organization structure\n",
      "   • Provider info included in saved responses\n",
      "   • Usage tracking for both providers\n",
      "📁 File structure remains the same:\n",
      "   • Single files: model_output/model_name/filename.txt\n",
      "   • Chunked files: chunked_model_output/model_name/filename/1.txt, 2.txt, etc.\n"
     ]
    }
   ],
   "source": [
    "# UPDATED MULTI-PROVIDER MIGRATION SYSTEM\n",
    "\n",
    "def migrate_file_chunked(filename: str, original_code: str, model_name: str, strategy: str, chunk_size: int):\n",
    "    \"\"\"Migrate a large file using organized chunking with multi-provider support.\"\"\"\n",
    "    chunks = chunk_code(original_code, chunk_size)\n",
    "    total_chunks = len(chunks)\n",
    "    \n",
    "    print(f\"📦 Split into {total_chunks} chunks of ~{chunk_size} lines each\")\n",
    "    \n",
    "    # Create organized folder structure\n",
    "    chunked_output_dir = Path('chunked_model_output')\n",
    "    chunked_output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Create model folder - handle both slash and non-slash model names\n",
    "    model_short = model_name.replace('/', '_').replace('-', '_').replace(':', '_').replace('.', '_').lower()\n",
    "    model_dir = chunked_output_dir / model_short\n",
    "    model_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Create file folder (remove .php extension for folder name)\n",
    "    file_base = filename.replace('.php', '')\n",
    "    file_dir = model_dir / file_base\n",
    "    file_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(f\"📁 Saving chunks to: {file_dir}\")\n",
    "    \n",
    "    # Use chunking strategy\n",
    "    chunk_strategy = f\"chunk_{strategy}\" if not strategy.startswith('chunk_') else strategy\n",
    "    \n",
    "    all_responses = []\n",
    "    \n",
    "    for i, chunk_info in enumerate(chunks, 1):\n",
    "        print(f\"\\n[Chunk {i}/{total_chunks}] Processing lines {chunk_info['start_line']}-{chunk_info['end_line']}...\")\n",
    "        \n",
    "        # Create prompt with chunking context\n",
    "        prompt = create_prompt(\n",
    "            chunk_info['code'], \n",
    "            chunk_strategy,\n",
    "            filename=filename,\n",
    "            start_line=chunk_info['start_line'],\n",
    "            end_line=chunk_info['end_line'],\n",
    "            total_lines=chunk_info['total_lines'],\n",
    "            chunk_number=i,\n",
    "            total_chunks=total_chunks\n",
    "        )\n",
    "        \n",
    "        print(f\"📏 Chunk prompt length: {len(prompt):,} characters\")\n",
    "        \n",
    "        # Make API call for this chunk using multi-provider client\n",
    "        chunk_filename = f\"{filename}_chunk_{i}\"\n",
    "        response = make_api_call_chunked_multi(chunk_filename, prompt, model_name, chunk_strategy, file_dir, i)\n",
    "        \n",
    "        if response is None:\n",
    "            print(f\"❌ Failed to process chunk {i}\")\n",
    "            all_responses.append(None)\n",
    "        else:\n",
    "            all_responses.append(response)\n",
    "            print(f\"✅ Chunk {i} processed successfully\")\n",
    "    \n",
    "    # Count successful chunks\n",
    "    successful_chunks = sum(1 for r in all_responses if r is not None)\n",
    "    print(f\"\\n🎉 Chunked migration completed!\")\n",
    "    print(f\"✅ Successful chunks: {successful_chunks}/{total_chunks}\")\n",
    "    print(f\"📁 All chunks saved in: {file_dir}\")\n",
    "    \n",
    "    return all_responses\n",
    "\n",
    "def make_api_call_chunked_multi(filename: str, prompt: str, model_name: str, strategy: str, file_dir: Path, chunk_number: int):\n",
    "    \"\"\"Make API call using multi-provider client and save to organized chunk structure.\"\"\"\n",
    "    \n",
    "    print(f\"🔗 Making API call via multi-provider client...\")\n",
    "    \n",
    "    # Use the multi-provider client\n",
    "    result = multi_client.make_api_call(model_name, prompt)\n",
    "    \n",
    "    print(f\" Provider: {result.get('provider', 'unknown').upper()}\")\n",
    "    \n",
    "    if not result['success']:\n",
    "        print(f\"❌ API Error: {result['error']}\")\n",
    "        return None\n",
    "    \n",
    "    raw_response = result['content']\n",
    "    print(f\"📏 Response length: {len(raw_response)} characters\")\n",
    "    \n",
    "    if not raw_response or len(raw_response.strip()) < 10:\n",
    "        print(f\"❌ Model response is empty or too short\")\n",
    "        return None\n",
    "    \n",
    "    # Save to organized structure - just the chunk number as filename\n",
    "    chunk_file = file_dir / f\"{chunk_number}.txt\"\n",
    "    \n",
    "    with open(chunk_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"=== RAW MODEL RESPONSE ===\\n\")\n",
    "        f.write(f\"File: {filename}\\n\")\n",
    "        f.write(f\"Model: {model_name}\\n\")\n",
    "        f.write(f\"Provider: {result.get('provider', 'unknown').upper()}\\n\")\n",
    "        f.write(f\"Strategy: {strategy}\\n\")\n",
    "        f.write(f\"Chunk: {chunk_number}\\n\")\n",
    "        f.write(f\"Length: {len(raw_response)} characters\\n\")\n",
    "        f.write(f\"Usage: {result.get('usage', {})}\\n\")\n",
    "        f.write(f\"Timestamp: {datetime.now()}\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        f.write(raw_response)\n",
    "    \n",
    "    print(f\"✅ Chunk saved to: {chunk_file}\")\n",
    "    return raw_response\n",
    "\n",
    "# Keep the existing functions for single files but update them\n",
    "def migrate_file(filename: str, model_name: str, strategy: str = \"basic\", \n",
    "                chunk_size: int = None, auto_chunk: bool = True):\n",
    "    \"\"\"Enhanced migration function with multi-provider support and organized chunking.\"\"\"\n",
    "    \n",
    "    if chunk_size is None:\n",
    "        chunk_size = DEFAULT_CHUNK_SIZE\n",
    "    \n",
    "    if filename not in test_files:\n",
    "        print(f\"❌ File '{filename}' not found\")\n",
    "        return None\n",
    "    \n",
    "    original_code = test_files[filename]\n",
    "    line_count = len(original_code.split('\\n'))\n",
    "    \n",
    "    print(f\"🚀 Migrating {filename} using {model_name} with {strategy} strategy...\")\n",
    "    print(f\"📏 Input code length: {len(original_code):,} characters ({line_count:,} lines)\")\n",
    "    \n",
    "    # Decide whether to chunk\n",
    "    should_chunk = auto_chunk and line_count > chunk_size\n",
    "    \n",
    "    if should_chunk:\n",
    "        print(f\"📦 Large file detected ({line_count} lines) - using organized chunking\")\n",
    "        return migrate_file_chunked(filename, original_code, model_name, strategy, chunk_size)\n",
    "    else:\n",
    "        print(f\"📄 Processing as single file ({line_count} lines, chunk limit: {chunk_size})\")\n",
    "        return migrate_file_single_multi(filename, original_code, model_name, strategy)\n",
    "\n",
    "def migrate_file_single_multi(filename: str, original_code: str, model_name: str, strategy: str):\n",
    "    \"\"\"Migrate a single file using multi-provider client - saves to regular model_output.\"\"\"\n",
    "    prompt = create_prompt(original_code, strategy)\n",
    "    print(f\"📏 Prompt length: {len(prompt):,} characters\")\n",
    "    \n",
    "    return make_api_call_multi(filename, prompt, model_name, strategy)\n",
    "\n",
    "def make_api_call_multi(filename: str, prompt: str, model_name: str, strategy: str):\n",
    "    \"\"\"Make API call for single files using multi-provider client - saves to model_output directory.\"\"\"\n",
    "    \n",
    "    print(f\"🔗 Making API call via multi-provider client...\")\n",
    "    \n",
    "    # Use the multi-provider client\n",
    "    result = multi_client.make_api_call(model_name, prompt)\n",
    "    \n",
    "    print(f\"📊 Provider: {result.get('provider', 'unknown').upper()}\")\n",
    "    \n",
    "    if not result['success']:\n",
    "        print(f\"❌ API Error: {result['error']}\")\n",
    "        return None\n",
    "    \n",
    "    raw_response = result['content']\n",
    "    print(f\"📏 Response length: {len(raw_response)} characters\")\n",
    "    \n",
    "    # Save to organized model_output structure with simple filenames\n",
    "    output_dir = Path('model_output')\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Create model subfolder - handle both slash and non-slash model names\n",
    "    model_short = model_name.replace('/', '_').replace('-', '_').replace(':', '_').replace('.', '_').lower()\n",
    "    model_folder = output_dir / model_short\n",
    "    model_folder.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Simple filename - just the base name with .txt extension\n",
    "    base_name = filename.replace('.php', '')\n",
    "    output_file = model_folder / f\"{base_name}.txt\"\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"=== RAW MODEL RESPONSE ===\\n\")\n",
    "        f.write(f\"File: {filename}\\n\")\n",
    "        f.write(f\"Model: {model_name}\\n\")\n",
    "        f.write(f\"Provider: {result.get('provider', 'unknown').upper()}\\n\")\n",
    "        f.write(f\"Strategy: {strategy}\\n\")\n",
    "        f.write(f\"Length: {len(raw_response)} characters\\n\")\n",
    "        f.write(f\"Usage: {result.get('usage', {})}\\n\")\n",
    "        f.write(f\"Timestamp: {datetime.now()}\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        f.write(raw_response)\n",
    "    \n",
    "    print(f\"✅ Response saved to: {output_file}\")\n",
    "    return raw_response\n",
    "\n",
    "print(\"✅ Multi-Provider Migration System Ready!\")\n",
    "print(\"🎯 Updated Features:\")\n",
    "print(\"   • Supports both OpenRouter and Google AI models\")\n",
    "print(\"   • Automatic provider detection and routing\")\n",
    "print(\"   • Same file organization structure\")\n",
    "print(\"   • Provider info included in saved responses\")\n",
    "print(\"   • Usage tracking for both providers\")\n",
    "print(\"📁 File structure remains the same:\")\n",
    "print(\"   • Single files: model_output/model_name/filename.txt\")\n",
    "print(\"   • Chunked files: chunked_model_output/model_name/filename/1.txt, 2.txt, etc.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Loaded 12 PHP files from selected_100_files:\n",
      "   📄 001_getid3.lib.php (43,504 chars)\n",
      "   📄 002_module.audio-video.asf.php (129,115 chars)\n",
      "   📄 003_wp-db.php (60,679 chars)\n",
      "   📄 004_class-IXR.php (32,854 chars)\n",
      "   📄 005_class-snoopy.php (37,776 chars)\n",
      "   📄 006_widgets.php (47,668 chars)\n",
      "   📄 009_getid3.php (62,683 chars)\n",
      "   📄 010_class-wp-theme.php (39,449 chars)\n",
      "   📄 012_module.audio-video.riff.php (111,145 chars)\n",
      "   📄 013_file.php (45,191 chars)\n",
      "   📄 014_module.tag.id3v2.php (134,159 chars)\n",
      "   📄 057_class-wp-customize-manager.php (32,639 chars)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if old_version_path.exists():\n",
    "    # Recursively find all PHP files in all subfolders\n",
    "    for php_file in old_version_path.rglob('*.php'):\n",
    "        try:\n",
    "            with open(php_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                content = f.read()\n",
    "                if content.strip():\n",
    "                    test_files[php_file.name] = content\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Could not load {php_file.name}: {e}\")\n",
    "    \n",
    "    print(f\"📁 Loaded {len(test_files)} PHP files from selected_100_files:\")\n",
    "    for filename in sorted(test_files.keys()):\n",
    "        size = len(test_files[filename])\n",
    "        print(f\"   📄 {filename} ({size:,} chars)\")\n",
    "else:\n",
    "    print(\"❌ selected_100_files directory not found\")\n",
    "    print(\"💡 Make sure the selected 100 files are in 'selected_100_files/' directory\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Multi-Provider Helper Functions Ready!\n",
      "💡 New features:\n",
      "   test_providers() - Test all available providers\n",
      "   list_available_models() - Show example models for each provider\n",
      "   Updated quick_migrate() and batch_migrate() with multi-provider support\n",
      "   Updated browsing functions show provider information\n",
      "   Default model changed to 'gemini-1.5-pro' (Google AI)\n",
      "\n",
      "💡 Usage examples:\n",
      "   quick_migrate('file.php', 'gemini-1.5-pro', 'basic')\n",
      "   quick_migrate('file.php', 'anthropic/claude-3.5-sonnet', 'comprehensive')\n",
      "   batch_migrate(['file1.php', 'file2.php'], 'gemini-1.5-flash')\n",
      "   test_providers()  # Test both Google AI and OpenRouter\n"
     ]
    }
   ],
   "source": [
    "# UPDATED HELPER FUNCTIONS WITH MULTI-PROVIDER SUPPORT\n",
    "\n",
    "def quick_migrate(filename: str, model: str = \"gemini-1.5-pro\", strategy: str = \"basic\", \n",
    "                 chunk_size: int = None, auto_chunk: bool = True):\n",
    "    \"\"\"Quick migration with defaults and multi-provider support.\"\"\"\n",
    "    if chunk_size is None:\n",
    "        chunk_size = DEFAULT_CHUNK_SIZE\n",
    "    return migrate_file(filename, model, strategy, chunk_size=chunk_size, auto_chunk=auto_chunk)\n",
    "\n",
    "def batch_migrate(filenames: list, model: str = \"gemini-1.5-pro\", strategy: str = \"basic\", \n",
    "                 chunk_size: int = None, auto_chunk: bool = True):\n",
    "    \"\"\"Migrate multiple files with multi-provider chunking support.\"\"\"\n",
    "    if chunk_size is None:\n",
    "        chunk_size = DEFAULT_CHUNK_SIZE\n",
    "        \n",
    "    provider = multi_client.detect_provider(model)\n",
    "    print(f\"🔄 Batch migrating {len(filenames)} files...\")\n",
    "    print(f\"🎯 Using {provider.upper()} provider for {model}\")\n",
    "    if auto_chunk:\n",
    "        print(f\"📦 Auto-chunking enabled for files > {chunk_size} lines\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, filename in enumerate(filenames, 1):\n",
    "        print(f\"\\n[{i}/{len(filenames)}] Processing {filename}...\")\n",
    "        result = migrate_file(filename, model, strategy, chunk_size=chunk_size, auto_chunk=auto_chunk)\n",
    "        results.append(result)\n",
    "    \n",
    "    # Count results (for chunked files, result is a list)\n",
    "    total_files = len(filenames)\n",
    "    successful_files = 0\n",
    "    total_chunks = 0\n",
    "    successful_chunks = 0\n",
    "    \n",
    "    for result in results:\n",
    "        if result is not None:\n",
    "            if isinstance(result, list):  # Chunked file\n",
    "                total_chunks += len(result)\n",
    "                successful_chunks += sum(1 for r in result if r is not None)\n",
    "                if any(r is not None for r in result):  # At least one chunk succeeded\n",
    "                    successful_files += 1\n",
    "            else:  # Single file\n",
    "                successful_files += 1\n",
    "                total_chunks += 1\n",
    "                successful_chunks += 1\n",
    "    \n",
    "    print(f\"\\n🎉 Batch migration completed!\")\n",
    "    print(f\"✅ Successful files: {successful_files}/{total_files}\")\n",
    "    if total_chunks > len(filenames):\n",
    "        print(f\"📦 Total chunks processed: {successful_chunks}/{total_chunks}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def test_providers(test_prompt: str = \"Hello, please respond with 'Provider test successful'\"):\n",
    "    \"\"\"Test all available providers with a simple prompt.\"\"\"\n",
    "    print(\"🧪 Testing all available providers...\")\n",
    "    \n",
    "    test_models = {\n",
    "        'google': 'gemini-1.5-flash',\n",
    "        'openrouter': 'anthropic/claude-3-haiku'\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for provider_name, model_name in test_models.items():\n",
    "        if not PROVIDERS[provider_name]['enabled']:\n",
    "            print(f\"\\n❌ {provider_name.upper()}: Disabled (missing API key)\")\n",
    "            results[provider_name] = {'success': False, 'error': 'Provider disabled'}\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n🔄 Testing {provider_name.upper()} with {model_name}...\")\n",
    "        \n",
    "        result = multi_client.make_api_call(model_name, test_prompt)\n",
    "        \n",
    "        if result['success']:\n",
    "            print(f\"✅ {provider_name.upper()}: Success!\")\n",
    "            print(f\"   Response: {result['content'][:100]}...\")\n",
    "            print(f\"   Usage: {result.get('usage', {})}\")\n",
    "            results[provider_name] = {'success': True, 'response': result['content']}\n",
    "        else:\n",
    "            print(f\"❌ {provider_name.upper()}: Failed - {result['error']}\")\n",
    "            results[provider_name] = {'success': False, 'error': result['error']}\n",
    "    \n",
    "    return results\n",
    "\n",
    "def list_available_models():\n",
    "    \"\"\"List example models for each provider.\"\"\"\n",
    "    print(\"📋 Available Model Examples:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    print(\"🔵 Google AI Models:\")\n",
    "    google_models = [\n",
    "        \"gemini-1.5-pro\",\n",
    "        \"gemini-1.5-flash\", \n",
    "        \"gemini-1.0-pro\"\n",
    "    ]\n",
    "    for model in google_models:\n",
    "        status = \"✅\" if PROVIDERS['google']['enabled'] else \"❌\"\n",
    "        print(f\"   {status} {model}\")\n",
    "    \n",
    "    print(\"\\n🟠 OpenRouter Models (examples):\")\n",
    "    openrouter_examples = [\n",
    "        \"anthropic/claude-3.5-sonnet\",\n",
    "        \"anthropic/claude-3-haiku\", \n",
    "        \"meta-llama/llama-3.1-8b-instruct\",\n",
    "        \"mistralai/mistral-small-3.2-24b-instruct:free\",\n",
    "        \"qwen/qwen3-coder:free\",\n",
    "        \"deepseek/deepseek-chat-v3.1:free\"\n",
    "    ]\n",
    "    for model in openrouter_examples:\n",
    "        status = \"✅\" if PROVIDERS['openrouter']['enabled'] else \"❌\"\n",
    "        print(f\"   {status} {model}\")\n",
    "    \n",
    "    print(f\"\\n💡 Provider Status:\")\n",
    "    for name, config in PROVIDERS.items():\n",
    "        status = \"✅ Enabled\" if config['enabled'] else \"❌ Disabled\"\n",
    "        print(f\"   {name.upper()}: {status}\")\n",
    "\n",
    "def analyze_file_sizes(chunk_threshold: int = None):\n",
    "    \"\"\"Analyze file sizes to see which ones would be chunked.\"\"\"\n",
    "    if chunk_threshold is None:\n",
    "        chunk_threshold = DEFAULT_CHUNK_SIZE\n",
    "        \n",
    "    if not test_files:\n",
    "        print(\"❌ No test files loaded\")\n",
    "        return\n",
    "    \n",
    "    print(\"📊 File Size Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    small_files = []\n",
    "    large_files = []\n",
    "    \n",
    "    for filename, content in test_files.items():\n",
    "        line_count = len(content.split('\\n'))\n",
    "        char_count = len(content)\n",
    "        \n",
    "        if line_count <= chunk_threshold:\n",
    "            small_files.append((filename, line_count, char_count))\n",
    "        else:\n",
    "            large_files.append((filename, line_count, char_count))\n",
    "    \n",
    "    print(f\"📄 Small files (≤{chunk_threshold} lines): {len(small_files)}\")\n",
    "    for filename, lines, chars in sorted(small_files, key=lambda x: x[1], reverse=True)[:10]:\n",
    "        print(f\"   {filename}: {lines:,} lines, {chars:,} chars\")\n",
    "    \n",
    "    if len(small_files) > 10:\n",
    "        print(f\"   ... and {len(small_files) - 10} more\")\n",
    "    \n",
    "    print(f\"\\n📦 Large files (>{chunk_threshold} lines): {len(large_files)}\")\n",
    "    for filename, lines, chars in sorted(large_files, key=lambda x: x[1], reverse=True):\n",
    "        chunks_needed = (lines + chunk_threshold - 1) // chunk_threshold  # Ceiling division\n",
    "        print(f\"   {filename}: {lines:,} lines, {chars:,} chars → {chunks_needed} chunks\")\n",
    "    \n",
    "    if large_files:\n",
    "        total_large_lines = sum(lines for _, lines, _ in large_files)\n",
    "        total_chunks_needed = sum((lines + chunk_threshold - 1) // chunk_threshold for _, lines, _ in large_files)\n",
    "        print(f\"\\n📊 Summary for large files:\")\n",
    "        print(f\"   Total lines: {total_large_lines:,}\")\n",
    "        print(f\"   Total chunks needed: {total_chunks_needed}\")\n",
    "\n",
    "# Keep existing browsing functions but update provider detection\n",
    "def browse_saved_chunks(model_name: str = None):\n",
    "    \"\"\"Browse and analyze saved chunks with multi-provider support.\"\"\"\n",
    "    chunks_dir = Path('chunks_sent_to_model')\n",
    "    \n",
    "    if not chunks_dir.exists():\n",
    "        print(\"❌ No chunks directory found. Run some migrations first!\")\n",
    "        return\n",
    "    \n",
    "    print(\"📁 Browsing saved chunks...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    model_dirs = list(chunks_dir.iterdir())\n",
    "    if not model_dirs:\n",
    "        print(\"❌ No model directories found in chunks_sent_to_model/\")\n",
    "        return\n",
    "    \n",
    "    # Filter by model if specified\n",
    "    if model_name:\n",
    "        model_short = model_name.replace('/', '_').replace('-', '_').replace(':', '_').replace('.', '_').lower()\n",
    "        model_dirs = [d for d in model_dirs if d.name == model_short]\n",
    "        if not model_dirs:\n",
    "            print(f\"❌ No chunks found for model: {model_name}\")\n",
    "            print(f\"Available models: {[d.name for d in chunks_dir.iterdir() if d.is_dir()]}\")\n",
    "            return\n",
    "    \n",
    "    for model_dir in sorted(model_dirs):\n",
    "        if not model_dir.is_dir():\n",
    "            continue\n",
    "        \n",
    "        # Detect provider from saved files (check for provider info)\n",
    "        provider_info = \"Unknown\"\n",
    "        sample_files = list(model_dir.glob('**/*.txt'))\n",
    "        if sample_files:\n",
    "            try:\n",
    "                with open(sample_files[0], 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                    for line in content.split('\\n')[:10]:\n",
    "                        if line.startswith('Provider:'):\n",
    "                            provider_info = line.split(':', 1)[1].strip()\n",
    "                            break\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        print(f\"\\n Model: {model_dir.name} ({provider_info})\")\n",
    "        \n",
    "        # Group files by base name\n",
    "        files_by_base = {}\n",
    "        for file in model_dir.iterdir():\n",
    "            if file.is_file():\n",
    "                # Extract base name\n",
    "                if '_chunk_' in file.stem:\n",
    "                    base_name = file.stem.split('_chunk_')[0]\n",
    "                elif '_single_file_' in file.stem:\n",
    "                    base_name = file.stem.split('_single_file_')[0]\n",
    "                else:\n",
    "                    base_name = file.stem\n",
    "                \n",
    "                if base_name not in files_by_base:\n",
    "                    files_by_base[base_name] = {'chunks': [], 'single': [], 'other': []}\n",
    "                \n",
    "                if '_chunk_' in file.stem:\n",
    "                    files_by_base[base_name]['chunks'].append(file)\n",
    "                elif '_single_file_' in file.stem:\n",
    "                    files_by_base[base_name]['single'].append(file)\n",
    "                else:\n",
    "                    files_by_base[base_name]['other'].append(file)\n",
    "        \n",
    "        # Display grouped files\n",
    "        for base_name, file_groups in sorted(files_by_base.items()):\n",
    "            chunks = file_groups['chunks']\n",
    "            single = file_groups['single']\n",
    "            \n",
    "            if chunks:\n",
    "                print(f\"   📦 {base_name}.php - {len(chunks)} chunks\")\n",
    "            elif single:\n",
    "                print(f\"   {base_name}.php - single file\")\n",
    "\n",
    "print(\"✅ Multi-Provider Helper Functions Ready!\")\n",
    "print(\"💡 New features:\")\n",
    "print(\"   test_providers() - Test all available providers\")\n",
    "print(\"   list_available_models() - Show example models for each provider\")\n",
    "print(\"   Updated quick_migrate() and batch_migrate() with multi-provider support\")\n",
    "print(\"   Updated browsing functions show provider information\")\n",
    "print(f\"   Default model changed to 'gemini-1.5-pro' (Google AI)\")\n",
    "print(\"\\n💡 Usage examples:\")\n",
    "print(\"   quick_migrate('file.php', 'gemini-1.5-pro', 'basic')\")\n",
    "print(\"   quick_migrate('file.php', 'anthropic/claude-3.5-sonnet', 'comprehensive')\")\n",
    "print(\"   batch_migrate(['file1.php', 'file2.php'], 'gemini-1.5-flash')\")\n",
    "print(\"   test_providers()  # Test both Google AI and OpenRouter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing providers...\n",
      "\n",
      "🔵 Testing Google AI...\n",
      "🔗 Using GOOGLE provider for gemini-1.5-flash\n",
      "✅ Google AI Success: Hello from Google AI!\n",
      "...\n",
      "\n",
      "🟠 Testing OpenRouter...\n",
      "🔗 Using OPENROUTER provider for anthropic/claude-3-haiku\n",
      "✅ Google AI Success: Hello from Google AI!\n",
      "...\n",
      "\n",
      "🟠 Testing OpenRouter...\n",
      "🔗 Using OPENROUTER provider for anthropic/claude-3-haiku\n",
      "✅ OpenRouter Success: Hello from OpenRouter!...\n",
      "✅ OpenRouter Success: Hello from OpenRouter!...\n"
     ]
    }
   ],
   "source": [
    "# SIMPLE PROVIDER TEST\n",
    "def simple_test_providers():\n",
    "    \"\"\"Simple test for both providers.\"\"\"\n",
    "    print(\"🧪 Testing providers...\")\n",
    "    \n",
    "    # Test Google AI\n",
    "    if PROVIDERS['google']['enabled']:\n",
    "        print(\"\\n🔵 Testing Google AI...\")\n",
    "        result = multi_client.make_api_call('gemini-1.5-flash', \"Say 'Hello from Google AI!'\")\n",
    "        if result['success']:\n",
    "            print(f\"✅ Google AI Success: {result['content'][:100]}...\")\n",
    "        else:\n",
    "            print(f\"❌ Google AI Failed: {result['error']}\")\n",
    "    else:\n",
    "        print(\"❌ Google AI disabled\")\n",
    "    \n",
    "    # Test OpenRouter\n",
    "    if PROVIDERS['openrouter']['enabled']:\n",
    "        print(\"\\n🟠 Testing OpenRouter...\")\n",
    "        result = multi_client.make_api_call('anthropic/claude-3-haiku', \"Say 'Hello from OpenRouter!'\")\n",
    "        if result['success']:\n",
    "            print(f\"✅ OpenRouter Success: {result['content'][:100]}...\")\n",
    "        else:\n",
    "            print(f\"❌ OpenRouter Failed: {result['error']}\")\n",
    "    else:\n",
    "        print(\"❌ OpenRouter disabled\")\n",
    "\n",
    "# Run the test\n",
    "simple_test_providers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Analyzing loaded files to see chunking requirements...\n",
      "📊 File Size Analysis\n",
      "==================================================\n",
      "📄 Small files (≤500 lines): 0\n",
      "\n",
      "📦 Large files (>500 lines): 12\n",
      "   014_module.tag.id3v2.php: 3,415 lines, 134,159 chars → 7 chunks\n",
      "   012_module.audio-video.riff.php: 2,436 lines, 111,145 chars → 5 chunks\n",
      "   003_wp-db.php: 2,187 lines, 60,679 chars → 5 chunks\n",
      "   002_module.audio-video.asf.php: 2,020 lines, 129,115 chars → 5 chunks\n",
      "   009_getid3.php: 1,776 lines, 62,683 chars → 4 chunks\n",
      "   006_widgets.php: 1,515 lines, 47,668 chars → 4 chunks\n",
      "   001_getid3.lib.php: 1,347 lines, 43,504 chars → 3 chunks\n",
      "   057_class-wp-customize-manager.php: 1,273 lines, 32,639 chars → 3 chunks\n",
      "   005_class-snoopy.php: 1,257 lines, 37,776 chars → 3 chunks\n",
      "   010_class-wp-theme.php: 1,236 lines, 39,449 chars → 3 chunks\n",
      "   013_file.php: 1,151 lines, 45,191 chars → 3 chunks\n",
      "   004_class-IXR.php: 1,101 lines, 32,854 chars → 3 chunks\n",
      "\n",
      "📊 Summary for large files:\n",
      "   Total lines: 20,714\n",
      "   Total chunks needed: 48\n",
      "\n",
      "🧪 Testing chunking logic on sample files...\n",
      "\n",
      "📄 001_getid3.lib.php:\n",
      "   Lines: 1,347\n",
      "   Chunks: 3\n",
      "   Chunk breakdown:\n",
      "     Chunk 1: lines 1-508 (508 lines)\n",
      "     Chunk 2: lines 509-1085 (577 lines)\n",
      "     Chunk 3: lines 1086-1347 (262 lines)\n",
      "\n",
      "📄 002_module.audio-video.asf.php:\n",
      "   Lines: 2,020\n",
      "   Chunks: 4\n",
      "   Chunk breakdown:\n",
      "     Chunk 1: lines 1-500 (500 lines)\n",
      "     Chunk 2: lines 501-1000 (500 lines)\n",
      "     Chunk 3: lines 1001-1577 (577 lines)\n",
      "     Chunk 4: lines 1578-2020 (443 lines)\n",
      "\n",
      "📄 003_wp-db.php:\n",
      "   Lines: 2,187\n",
      "   Chunks: 5\n",
      "   Chunk breakdown:\n",
      "     Chunk 1: lines 1-500 (500 lines)\n",
      "     Chunk 2: lines 501-1002 (502 lines)\n",
      "     Chunk 3: lines 1003-1523 (521 lines)\n",
      "     Chunk 4: lines 1524-2036 (513 lines)\n",
      "     Chunk 5: lines 2037-2187 (151 lines)\n",
      "\n",
      "✅ File analysis complete!\n",
      "\n",
      "📄 002_module.audio-video.asf.php:\n",
      "   Lines: 2,020\n",
      "   Chunks: 4\n",
      "   Chunk breakdown:\n",
      "     Chunk 1: lines 1-500 (500 lines)\n",
      "     Chunk 2: lines 501-1000 (500 lines)\n",
      "     Chunk 3: lines 1001-1577 (577 lines)\n",
      "     Chunk 4: lines 1578-2020 (443 lines)\n",
      "\n",
      "📄 003_wp-db.php:\n",
      "   Lines: 2,187\n",
      "   Chunks: 5\n",
      "   Chunk breakdown:\n",
      "     Chunk 1: lines 1-500 (500 lines)\n",
      "     Chunk 2: lines 501-1002 (502 lines)\n",
      "     Chunk 3: lines 1003-1523 (521 lines)\n",
      "     Chunk 4: lines 1524-2036 (513 lines)\n",
      "     Chunk 5: lines 2037-2187 (151 lines)\n",
      "\n",
      "✅ File analysis complete!\n"
     ]
    }
   ],
   "source": [
    "# ANALYZE YOUR FILES FOR CHUNKING\n",
    "print(\"🔍 Analyzing loaded files to see chunking requirements...\")\n",
    "analyze_file_sizes()\n",
    "\n",
    "# Test chunking on a sample file\n",
    "print(\"\\n🧪 Testing chunking logic on sample files...\")\n",
    "for filename in list(test_files.keys())[:3]:\n",
    "    code = test_files[filename]\n",
    "    line_count = len(code.split('\\n'))\n",
    "    chunks = chunk_code(code, DEFAULT_CHUNK_SIZE)\n",
    "    \n",
    "    print(f\"\\n📄 {filename}:\")\n",
    "    print(f\"   Lines: {line_count:,}\")\n",
    "    print(f\"   Chunks: {len(chunks)}\")\n",
    "    \n",
    "    if len(chunks) > 1:\n",
    "        print(f\"   Chunk breakdown:\")\n",
    "        for i, chunk_info in enumerate(chunks, 1):\n",
    "            print(f\"     Chunk {i}: lines {chunk_info['start_line']}-{chunk_info['end_line']} ({chunk_info['end_line'] - chunk_info['start_line'] + 1} lines)\")\n",
    "\n",
    "print(f\"\\n✅ File analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Migrating 001_getid3.lib.php using mistralai/mistral-small-3.2-24b-instruct:free with basic strategy...\n",
      "📏 Input code length: 43,504 characters (1,347 lines)\n",
      "📦 Large file detected (1347 lines) - using organized chunking\n",
      "📦 Split into 3 chunks of ~500 lines each\n",
      "📁 Saving chunks to: chunked_model_output\\mistralai_mistral_small_3_2_24b_instruct_free\\001_getid3.lib\n",
      "\n",
      "[Chunk 1/3] Processing lines 1-508...\n",
      "📏 Chunk prompt length: 17,255 characters\n",
      "🔗 Making API call via multi-provider client...\n",
      "🔗 Using OPENROUTER provider for mistralai/mistral-small-3.2-24b-instruct:free\n",
      "📦 Split into 3 chunks of ~500 lines each\n",
      "📁 Saving chunks to: chunked_model_output\\mistralai_mistral_small_3_2_24b_instruct_free\\001_getid3.lib\n",
      "\n",
      "[Chunk 1/3] Processing lines 1-508...\n",
      "📏 Chunk prompt length: 17,255 characters\n",
      "🔗 Making API call via multi-provider client...\n",
      "🔗 Using OPENROUTER provider for mistralai/mistral-small-3.2-24b-instruct:free\n",
      " Provider: UNKNOWN\n",
      "❌ API Error: HTTP 429: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day\",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"50\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1756944000000\"},\"provider_name\":null}},\"user_id\":\"user_2xMT55snBKFgKi83Qy4yYtb4ain\"}\n",
      "❌ Failed to process chunk 1\n",
      "\n",
      "[Chunk 2/3] Processing lines 509-1085...\n",
      "📏 Chunk prompt length: 20,388 characters\n",
      "🔗 Making API call via multi-provider client...\n",
      "🔗 Using OPENROUTER provider for mistralai/mistral-small-3.2-24b-instruct:free\n",
      " Provider: UNKNOWN\n",
      "❌ API Error: HTTP 429: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day\",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"50\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1756944000000\"},\"provider_name\":null}},\"user_id\":\"user_2xMT55snBKFgKi83Qy4yYtb4ain\"}\n",
      "❌ Failed to process chunk 1\n",
      "\n",
      "[Chunk 2/3] Processing lines 509-1085...\n",
      "📏 Chunk prompt length: 20,388 characters\n",
      "🔗 Making API call via multi-provider client...\n",
      "🔗 Using OPENROUTER provider for mistralai/mistral-small-3.2-24b-instruct:free\n",
      " Provider: UNKNOWN\n",
      "❌ API Error: HTTP 429: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day\",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"50\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1756944000000\"},\"provider_name\":null}},\"user_id\":\"user_2xMT55snBKFgKi83Qy4yYtb4ain\"}\n",
      "❌ Failed to process chunk 2\n",
      "\n",
      "[Chunk 3/3] Processing lines 1086-1347...\n",
      "📏 Chunk prompt length: 9,556 characters\n",
      "🔗 Making API call via multi-provider client...\n",
      "🔗 Using OPENROUTER provider for mistralai/mistral-small-3.2-24b-instruct:free\n",
      " Provider: UNKNOWN\n",
      "❌ API Error: HTTP 429: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day\",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"50\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1756944000000\"},\"provider_name\":null}},\"user_id\":\"user_2xMT55snBKFgKi83Qy4yYtb4ain\"}\n",
      "❌ Failed to process chunk 2\n",
      "\n",
      "[Chunk 3/3] Processing lines 1086-1347...\n",
      "📏 Chunk prompt length: 9,556 characters\n",
      "🔗 Making API call via multi-provider client...\n",
      "🔗 Using OPENROUTER provider for mistralai/mistral-small-3.2-24b-instruct:free\n",
      " Provider: UNKNOWN\n",
      "❌ API Error: HTTP 429: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day\",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"50\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1756944000000\"},\"provider_name\":null}},\"user_id\":\"user_2xMT55snBKFgKi83Qy4yYtb4ain\"}\n",
      "❌ Failed to process chunk 3\n",
      "\n",
      "🎉 Chunked migration completed!\n",
      "✅ Successful chunks: 0/3\n",
      "📁 All chunks saved in: chunked_model_output\\mistralai_mistral_small_3_2_24b_instruct_free\\001_getid3.lib\n",
      " Provider: UNKNOWN\n",
      "❌ API Error: HTTP 429: {\"error\":{\"message\":\"Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day\",\"code\":429,\"metadata\":{\"headers\":{\"X-RateLimit-Limit\":\"50\",\"X-RateLimit-Remaining\":\"0\",\"X-RateLimit-Reset\":\"1756944000000\"},\"provider_name\":null}},\"user_id\":\"user_2xMT55snBKFgKi83Qy4yYtb4ain\"}\n",
      "❌ Failed to process chunk 3\n",
      "\n",
      "🎉 Chunked migration completed!\n",
      "✅ Successful chunks: 0/3\n",
      "📁 All chunks saved in: chunked_model_output\\mistralai_mistral_small_3_2_24b_instruct_free\\001_getid3.lib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# UNCOMMENT THESE LINES FOR BATCH MIGRATION WITH DIFFERENT PROVIDERS:\n",
    "\n",
    "# Google AI batch migration:\n",
    "# batch_migrate(list(test_files.keys())[:3], model='gemini-1.5-pro', strategy='basic')\n",
    "\n",
    "# OpenRouter batch migration:\n",
    "# batch_migrate(list(test_files.keys())[:3], model='mistralai/mistral-small-3.2-24b-instruct:free', strategy='basic')\n",
    "\n",
    "# Mixed provider batch (you can mix and match in sequence):\n",
    "migrate_file('001_getid3.lib.php', 'mistralai/mistral-small-3.2-24b-instruct:free', 'basic')\n",
    "# migrate_file('file2.php', 'anthropic/claude-3.5-sonnet', 'comprehensive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplified output parser ready!\n",
      "Features:\n",
      "   • Simple single-file processing only\n",
      "   • Reliable MIGRATION_START/END marker extraction\n",
      "   • Clear error reporting\n",
      "   • No chunking complexity\n",
      "Usage: parser.process_all_responses()\n"
     ]
    }
   ],
   "source": [
    "# SIMPLIFIED OUTPUT PARSER - NO CHUNKING FOR NOW\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "class OutputParser:\n",
    "    \"\"\"Simple parser for model responses - single files only.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model_output_path = Path('model_output')\n",
    "        self.parsed_path = Path('new-version')\n",
    "        self.parsed_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    def extract_migrated_code(self, response_content: str) -> str:\n",
    "        \"\"\"Extract code between MIGRATION_START and MIGRATION_END markers.\"\"\"\n",
    "        # Look for migration markers (most reliable method)\n",
    "        start_pattern = r'//\\s*MIGRATION_START\\s*\\n'\n",
    "        end_pattern = r'\\n//\\s*MIGRATION_END'\n",
    "        \n",
    "        start_match = re.search(start_pattern, response_content, re.IGNORECASE)\n",
    "        end_match = re.search(end_pattern, response_content, re.IGNORECASE)\n",
    "        \n",
    "        if start_match and end_match:\n",
    "            start_pos = start_match.end()\n",
    "            end_pos = end_match.start()\n",
    "            migrated_code = response_content[start_pos:end_pos].strip()\n",
    "            return migrated_code\n",
    "        \n",
    "        # No markers found\n",
    "        return \"\"\n",
    "    \n",
    "    def extract_metadata(self, response_content: str) -> dict:\n",
    "        \"\"\"Extract metadata from response file header.\"\"\"\n",
    "        lines = response_content.split('\\n')\n",
    "        metadata = {}\n",
    "        \n",
    "        for line in lines[:15]:  # Check first 15 lines\n",
    "            if line.startswith('File:'):\n",
    "                metadata['original_file'] = line.split(':', 1)[1].strip()\n",
    "            elif line.startswith('Model:'):\n",
    "                metadata['model'] = line.split(':', 1)[1].strip()\n",
    "            elif line.startswith('Strategy:'):\n",
    "                metadata['strategy'] = line.split(':', 1)[1].strip()\n",
    "        \n",
    "        return metadata\n",
    "    \n",
    "    def parse_single_file(self, response_file: Path) -> dict:\n",
    "        \"\"\"Parse a single response file.\"\"\"\n",
    "        try:\n",
    "            print(f\"Processing {response_file.name}\")\n",
    "            \n",
    "            with open(response_file, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            # Extract metadata\n",
    "            metadata = self.extract_metadata(content)\n",
    "            if not metadata.get('original_file'):\n",
    "                print(f\"   ERROR: No original file found in metadata\")\n",
    "                return {'success': False}\n",
    "            \n",
    "            # Extract migrated code\n",
    "            migrated_code = self.extract_migrated_code(content)\n",
    "            if not migrated_code:\n",
    "                print(f\"   ERROR: No migrated code found between markers\")\n",
    "                return {'success': False}\n",
    "            \n",
    "            print(f\"   SUCCESS: Found {len(migrated_code)} chars of migrated code\")\n",
    "            return {\n",
    "                'success': True,\n",
    "                'metadata': metadata,\n",
    "                'migrated_code': migrated_code\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"   ERROR: {e}\")\n",
    "            return {'success': False, 'error': str(e)}\n",
    "    \n",
    "    def save_parsed_file(self, result: dict, response_filename: str, model_folder_name: str = None) -> bool:\n",
    "        \"\"\"Save parsed result to organized structure.\"\"\"\n",
    "        try:\n",
    "            metadata = result['metadata']\n",
    "            migrated_code = result['migrated_code']\n",
    "            \n",
    "            # Determine model folder name\n",
    "            if model_folder_name:\n",
    "                # Use provided model folder name (from new structure)\n",
    "                model_clean = model_folder_name\n",
    "            else:\n",
    "                # Extract from metadata (fallback for old structure)\n",
    "                model_name = metadata.get('model', 'unknown_model')\n",
    "                model_clean = model_name.replace('/', '_').replace('-', '_').replace(':', '_').replace('.', '_').lower()\n",
    "            \n",
    "            # Create model folder in new-version\n",
    "            model_folder = self.parsed_path / model_clean\n",
    "            model_folder.mkdir(exist_ok=True)\n",
    "            \n",
    "            # Determine output filename\n",
    "            original_filename = metadata.get('original_file')\n",
    "            if original_filename:\n",
    "                output_file = model_folder / original_filename\n",
    "            else:\n",
    "                # Fallback: derive from response filename\n",
    "                if response_filename.endswith('.txt'):\n",
    "                    php_filename = response_filename[:-4] + '.php'  # Replace .txt with .php\n",
    "                else:\n",
    "                    php_filename = response_filename + '.php'\n",
    "                output_file = model_folder / php_filename\n",
    "            \n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(migrated_code)\n",
    "            \n",
    "            print(f\"   ✅ SAVED: {output_file}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ SAVE ERROR: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def process_all_responses(self):\n",
    "        \"\"\"Process all response files in model_output directory.\"\"\"\n",
    "        print(\"🔄 Processing all model responses...\")\n",
    "        \n",
    "        if not self.model_output_path.exists():\n",
    "            print(f\"❌ Directory {self.model_output_path} not found\")\n",
    "            return\n",
    "        \n",
    "        # Look for model subfolders in model_output\n",
    "        model_folders = [d for d in self.model_output_path.iterdir() if d.is_dir()]\n",
    "        \n",
    "        if not model_folders:\n",
    "            # Fallback: look for .txt files directly in model_output (old structure)\n",
    "            response_files = list(self.model_output_path.glob('*.txt'))\n",
    "            if response_files:\n",
    "                print(f\"📁 Found {len(response_files)} response files in old structure\")\n",
    "                self._process_files_directly(response_files)\n",
    "            else:\n",
    "                print(\"❌ No model folders or .txt files found in model_output/\")\n",
    "            return\n",
    "        \n",
    "        print(f\"📁 Found {len(model_folders)} model folders:\")\n",
    "        for folder in model_folders:\n",
    "            print(f\"   📂 {folder.name}/\")\n",
    "        \n",
    "        total_success = 0\n",
    "        total_failed = 0\n",
    "        \n",
    "        # Process each model folder\n",
    "        for model_folder in model_folders:\n",
    "            print(f\"\\n🔄 Processing model: {model_folder.name}\")\n",
    "            \n",
    "            # Get all .txt files in this model folder\n",
    "            response_files = list(model_folder.glob('*.txt'))\n",
    "            print(f\"   📄 Found {len(response_files)} response files\")\n",
    "            \n",
    "            if not response_files:\n",
    "                print(\"   ⚠️  No .txt files found in this model folder\")\n",
    "                continue\n",
    "            \n",
    "            success_count = 0\n",
    "            failed_count = 0\n",
    "            \n",
    "            for response_file in response_files:\n",
    "                result = self.parse_single_file(response_file)\n",
    "                \n",
    "                if result['success']:\n",
    "                    # Update metadata to include model folder name\n",
    "                    if 'metadata' not in result:\n",
    "                        result['metadata'] = {}\n",
    "                    result['metadata']['model_folder'] = model_folder.name\n",
    "                    \n",
    "                    if self.save_parsed_file(result, response_file.name, model_folder.name):\n",
    "                        success_count += 1\n",
    "                    else:\n",
    "                        failed_count += 1\n",
    "                else:\n",
    "                    failed_count += 1\n",
    "            \n",
    "            print(f\"   ✅ Successfully processed: {success_count} files\")\n",
    "            print(f\"   ❌ Failed to process: {failed_count} files\")\n",
    "            \n",
    "            total_success += success_count\n",
    "            total_failed += failed_count\n",
    "        \n",
    "        print(f\"\\n🎉 Overall processing completed!\")\n",
    "        print(f\"✅ Total successfully processed: {total_success} files\")\n",
    "        print(f\"❌ Total failed to process: {total_failed} files\")\n",
    "        \n",
    "        # Show what was created\n",
    "        if total_success > 0:\n",
    "            print(f\"\\n📁 Results saved to '{self.parsed_path}':\")\n",
    "            for model_folder in sorted(self.parsed_path.iterdir()):\n",
    "                if model_folder.is_dir():\n",
    "                    php_files = list(model_folder.glob('*.php'))\n",
    "                    print(f\"   📂 {model_folder.name}/ ({len(php_files)} files)\")\n",
    "    \n",
    "    def _process_files_directly(self, response_files):\n",
    "        \"\"\"Process files directly from model_output (fallback for old structure).\"\"\"\n",
    "        success_count = 0\n",
    "        failed_count = 0\n",
    "        \n",
    "        for response_file in response_files:\n",
    "            result = self.parse_single_file(response_file)\n",
    "            \n",
    "            if result['success']:\n",
    "                if self.save_parsed_file(result, response_file.name):\n",
    "                    success_count += 1\n",
    "                else:\n",
    "                    failed_count += 1\n",
    "            else:\n",
    "                failed_count += 1\n",
    "        \n",
    "        print(f\"\\n🎉 Processing completed!\")\n",
    "        print(f\"✅ Successfully processed: {success_count} files\")\n",
    "        print(f\"❌ Failed to process: {failed_count} files\")\n",
    "\n",
    "# Initialize simplified parser\n",
    "parser = OutputParser()\n",
    "print(\"Simplified output parser ready!\")\n",
    "print(\"Features:\")\n",
    "print(\"   • Simple single-file processing only\")\n",
    "print(\"   • Reliable MIGRATION_START/END marker extraction\")\n",
    "print(\"   • Clear error reporting\")\n",
    "print(\"   • No chunking complexity\")\n",
    "print(\"Usage: parser.process_all_responses()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File Reconstructor ready!\n",
      "🎯 Features:\n",
      "   • Finds all chunked files in chunked_model_output/\n",
      "   • Uses simple parser to parse individual chunks\n",
      "   • Combines chunks in correct order\n",
      "   • Handles missing chunks gracefully\n",
      "   • Saves complete files to new-version/\n",
      "💡 Usage: reconstructor.reconstruct_all_files()\n"
     ]
    }
   ],
   "source": [
    "# FILE RECONSTRUCTOR - COMBINES PARSED CHUNKS INTO COMPLETE FILES\n",
    "class FileReconstructor:\n",
    "    \"\"\"Reconstructs complete files from parsed chunk files.\"\"\"\n",
    "    \n",
    "    def __init__(self, parser):\n",
    "        self.parser = parser  # Use the simple parser for individual chunks\n",
    "        self.chunked_output_path = Path('chunked_model_output')\n",
    "        self.final_output_path = Path('new-version')\n",
    "        self.final_output_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    def find_chunked_files(self):\n",
    "        \"\"\"Find all chunked file directories.\"\"\"\n",
    "        if not self.chunked_output_path.exists():\n",
    "            print(\"No chunked_model_output directory found\")\n",
    "            return []\n",
    "        \n",
    "        chunked_files = []\n",
    "        \n",
    "        for model_dir in self.chunked_output_path.iterdir():\n",
    "            if model_dir.is_dir():\n",
    "                for file_dir in model_dir.iterdir():\n",
    "                    if file_dir.is_dir():\n",
    "                        # Check if it has numbered chunk files\n",
    "                        chunk_files = list(file_dir.glob('*.txt'))\n",
    "                        if chunk_files:\n",
    "                            chunked_files.append({\n",
    "                                'model': model_dir.name,\n",
    "                                'filename': file_dir.name,\n",
    "                                'directory': file_dir,\n",
    "                                'chunk_count': len(chunk_files)\n",
    "                            })\n",
    "        \n",
    "        return chunked_files\n",
    "    \n",
    "    def get_chunk_files(self, directory: Path):\n",
    "        \"\"\"Get all chunk files from a directory, sorted by number.\"\"\"\n",
    "        chunk_files = []\n",
    "        \n",
    "        for file in directory.glob('*.txt'):\n",
    "            try:\n",
    "                # Extract number from filename (1.txt -> 1)\n",
    "                chunk_num = int(file.stem)\n",
    "                chunk_files.append((chunk_num, file))\n",
    "            except ValueError:\n",
    "                print(f\"WARNING: Skipping non-numeric chunk file: {file.name}\")\n",
    "        \n",
    "        # Sort by chunk number\n",
    "        chunk_files.sort(key=lambda x: x[0])\n",
    "        return chunk_files\n",
    "    \n",
    "    def reconstruct_file(self, file_info: dict):\n",
    "        \"\"\"Reconstruct a complete file from its chunks.\"\"\"\n",
    "        print(f\"\\nReconstructing {file_info['filename']}.php from {file_info['chunk_count']} chunks\")\n",
    "        print(f\"Model: {file_info['model']}\")\n",
    "        print(f\"Directory: {file_info['directory']}\")\n",
    "        \n",
    "        # Get sorted chunk files\n",
    "        chunk_files = self.get_chunk_files(file_info['directory'])\n",
    "        \n",
    "        if not chunk_files:\n",
    "            print(\"   ERROR: No valid chunk files found\")\n",
    "            return False\n",
    "        \n",
    "        # Check for missing chunks\n",
    "        expected_numbers = list(range(1, len(chunk_files) + 1))\n",
    "        actual_numbers = [num for num, _ in chunk_files]\n",
    "        missing = set(expected_numbers) - set(actual_numbers)\n",
    "        \n",
    "        if missing:\n",
    "            print(f\"   WARNING: Missing chunks: {sorted(missing)}\")\n",
    "        \n",
    "        print(f\"   Found chunks: {actual_numbers}\")\n",
    "        \n",
    "        # Parse each chunk\n",
    "        parsed_chunks = []\n",
    "        metadata = None\n",
    "        \n",
    "        for chunk_num, chunk_file in chunk_files:\n",
    "            print(f\"   Processing chunk {chunk_num}...\")\n",
    "            result = self.parser.parse_single_file(chunk_file)\n",
    "            \n",
    "            if result['success']:\n",
    "                parsed_chunks.append({\n",
    "                    'number': chunk_num,\n",
    "                    'code': result['migrated_code'],\n",
    "                    'metadata': result['metadata']\n",
    "                })\n",
    "                \n",
    "                # Use metadata from first successful chunk\n",
    "                if metadata is None:\n",
    "                    metadata = result['metadata']\n",
    "                    \n",
    "                print(f\"      SUCCESS: {len(result['migrated_code'])} chars\")\n",
    "            else:\n",
    "                print(f\"      ERROR: Failed to parse chunk {chunk_num}\")\n",
    "                parsed_chunks.append({\n",
    "                    'number': chunk_num,\n",
    "                    'code': None,\n",
    "                    'metadata': None\n",
    "                })\n",
    "        \n",
    "        if not any(chunk['code'] for chunk in parsed_chunks):\n",
    "            print(\"   ERROR: No chunks could be parsed successfully\")\n",
    "            return False\n",
    "        \n",
    "        # Combine chunks\n",
    "        combined_code = []\n",
    "        successful_chunks = 0\n",
    "        final_code = \"\"  # Initialize final_code\n",
    "        \n",
    "        for chunk in parsed_chunks:\n",
    "            if chunk['code']:\n",
    "                combined_code.append(chunk['code'])\n",
    "                successful_chunks += 1\n",
    "            else:\n",
    "                print(f\"   WARNING: Chunk {chunk['number']} failed - adding placeholder comment\")\n",
    "                combined_code.append(f\"// ERROR: Chunk {chunk['number']} failed to parse\")\n",
    "        \n",
    "        final_code = ''.join(combined_code)\n",
    "        print(f\"   Combined {successful_chunks}/{len(parsed_chunks)} chunks successfully\")\n",
    "        print(f\"   Final code length: {len(final_code)} characters\")\n",
    "        \n",
    "        # Save reconstructed file\n",
    "        return self.save_reconstructed_file(file_info, final_code, metadata)\n",
    "    \n",
    "    def save_reconstructed_file(self, file_info: dict, code: str, metadata: dict):\n",
    "        \"\"\"Save the reconstructed complete file.\"\"\"\n",
    "        try:\n",
    "            # Create model folder in final output\n",
    "            model_folder = self.final_output_path / file_info['model']\n",
    "            model_folder.mkdir(exist_ok=True)\n",
    "            \n",
    "            # Save the reconstructed file\n",
    "            output_file = model_folder / f\"{file_info['filename']}.php\"\n",
    "            \n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                # Write clean PHP code without metadata header\n",
    "                f.write(code)\n",
    "            \n",
    "            print(f\"   SAVED: {output_file}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ERROR saving file: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def reconstruct_all_files(self):\n",
    "        \"\"\"Reconstruct all chunked files found.\"\"\"\n",
    "        print(\"🔧 Starting file reconstruction...\")\n",
    "        \n",
    "        chunked_files = self.find_chunked_files()\n",
    "        \n",
    "        if not chunked_files:\n",
    "            print(\"No chunked files found to reconstruct\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Found {len(chunked_files)} chunked files to reconstruct:\")\n",
    "        for file_info in chunked_files:\n",
    "            print(f\"   {file_info['model']}/{file_info['filename']}.php ({file_info['chunk_count']} chunks)\")\n",
    "        \n",
    "        successful = 0\n",
    "        failed = 0\n",
    "        \n",
    "        for file_info in chunked_files:\n",
    "            if self.reconstruct_file(file_info):\n",
    "                successful += 1\n",
    "            else:\n",
    "                failed += 1\n",
    "        \n",
    "        print(f\"\\n🎉 Reconstruction completed!\")\n",
    "        print(f\"✅ Successfully reconstructed: {successful} files\")\n",
    "        print(f\"❌ Failed to reconstruct: {failed} files\")\n",
    "        \n",
    "        if successful > 0:\n",
    "            print(f\"\\n📁 Reconstructed files saved to: {self.final_output_path}\")\n",
    "            for model_folder in sorted(self.final_output_path.iterdir()):\n",
    "                if model_folder.is_dir():\n",
    "                    php_files = list(model_folder.glob('*.php'))\n",
    "                    print(f\"   {model_folder.name}/ ({len(php_files)} files)\")\n",
    "\n",
    "# Initialize the reconstructor with our simple parser\n",
    "reconstructor = FileReconstructor(parser)\n",
    "print(\"✅ File Reconstructor ready!\")\n",
    "print(\"🎯 Features:\")\n",
    "print(\"   • Finds all chunked files in chunked_model_output/\")\n",
    "print(\"   • Uses simple parser to parse individual chunks\")\n",
    "print(\"   • Combines chunks in correct order\")\n",
    "print(\"   • Handles missing chunks gracefully\")\n",
    "print(\"   • Saves complete files to new-version/\")\n",
    "print(\"💡 Usage: reconstructor.reconstruct_all_files()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Processing all model responses...\n",
      "❌ Directory model_output not found\n",
      "🔧 Starting file reconstruction...\n",
      "Found 1 chunked files to reconstruct:\n",
      "   mistralai_mistral_small_3_2_24b_instruct_free/001_getid3.lib.php (3 chunks)\n",
      "\n",
      "Reconstructing 001_getid3.lib.php from 3 chunks\n",
      "Model: mistralai_mistral_small_3_2_24b_instruct_free\n",
      "Directory: chunked_model_output\\mistralai_mistral_small_3_2_24b_instruct_free\\001_getid3.lib\n",
      "   Found chunks: [1, 2, 3]\n",
      "   Processing chunk 1...\n",
      "Processing 1.txt\n",
      "   SUCCESS: Found 19795 chars of migrated code\n",
      "      SUCCESS: 19795 chars\n",
      "   Processing chunk 2...\n",
      "Processing 2.txt\n",
      "   SUCCESS: Found 19548 chars of migrated code\n",
      "      SUCCESS: 19548 chars\n",
      "   Processing chunk 3...\n",
      "Processing 3.txt\n",
      "   SUCCESS: Found 8245 chars of migrated code\n",
      "      SUCCESS: 8245 chars\n",
      "   Combined 3/3 chunks successfully\n",
      "   Final code length: 47588 characters\n",
      "   SAVED: new-version\\mistralai_mistral_small_3_2_24b_instruct_free\\001_getid3.lib.php\n",
      "\n",
      "🎉 Reconstruction completed!\n",
      "✅ Successfully reconstructed: 1 files\n",
      "❌ Failed to reconstruct: 0 files\n",
      "\n",
      "📁 Reconstructed files saved to: new-version\n",
      "   mistralai_mistral_small_3_2_24b_instruct_free/ (1 files)\n"
     ]
    }
   ],
   "source": [
    "parser.process_all_responses()\n",
    "reconstructor.reconstruct_all_files()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
